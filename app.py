# Install required packages
!pip install streamlit
!pip install streamlit-option-menu
!pip install plotly

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pickle
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# Feature Selection
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.linear_model import BayesianRidge
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor

# ML models libraries
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder
from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, train_test_split
from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score,
                            roc_curve, precision_recall_curve, accuracy_score,
                            precision_score, recall_score, f1_score)

# Import all required models
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier
import xgboost as xgb

st.set_page_config(
    page_title="Capital One Credit Risk Management - Credit Card Churn Analytics",
    page_icon="üí≥",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for better styling
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        text-align: center;
        margin: 0.5rem;
    }
    .insight-box {
        background-color: #e8f4fd;
        padding: 1rem;
        border-left: 4px solid #1f77b4;
        margin: 1rem 0;
    }
    .risk-high { color: #d32f2f; font-weight: bold; }
    .risk-medium { color: #f57c00; font-weight: bold; }
    .risk-low { color: #388e3c; font-weight: bold; }
</style>
""", unsafe_allow_html=True)

# Sample data fallback for when CSV is not available
@st.cache_data
def load_sample_data():
    """Generate sample data when CSV is not available"""
    np.random.seed(42)
    n_records = 1000

    data = {
        'Customer_Age': np.random.randint(18, 75, n_records),
        'Gender': np.random.choice(['M', 'F'], n_records, p=[0.47, 0.53]),
        'Dependent_count': np.random.choice([0, 1, 2, 3, 4, 5], n_records, p=[0.30, 0.25, 0.20, 0.15, 0.07, 0.03]),
        'Education_Level': np.random.choice(['High School', 'College', 'Graduate', 'Post-Graduate', 'Doctorate'],
                                          n_records, p=[0.20, 0.25, 0.30, 0.20, 0.05]),
        'Marital_Status': np.random.choice(['Married', 'Single', 'Divorced'], n_records, p=[0.46, 0.42, 0.12]),
        'Income_Category': np.random.choice(['Less than $40K', '$40K - $60K', '$60K - $80K', '$80K - $120K', '$120K +'],
                                          n_records, p=[0.17, 0.24, 0.20, 0.21, 0.18]),
        'Card_Category': np.random.choice(['Blue', 'Silver', 'Gold', 'Platinum'], n_records, p=[0.80, 0.12, 0.05, 0.03]),
        'Months_on_book': np.random.randint(13, 57, n_records),
        'Total_Relationship_Count': np.random.choice([1, 2, 3, 4, 5, 6], n_records, p=[0.05, 0.30, 0.25, 0.20, 0.15, 0.05]),
        'Months_Inactive_12_mon': np.random.choice([0, 1, 2, 3, 4, 5, 6], n_records, p=[0.40, 0.20, 0.15, 0.10, 0.08, 0.05, 0.02]),
        'Contacts_Count_12_mon': np.random.choice([0, 1, 2, 3, 4, 5, 6], n_records, p=[0.35, 0.25, 0.20, 0.12, 0.05, 0.02, 0.01]),
        'Credit_Limit': np.random.lognormal(9.2, 0.8, n_records).clip(1500, 35000),
        'Total_Revolving_Bal': np.random.exponential(1500, n_records).clip(0, 25000),
        'Total_Trans_Amt': np.random.lognormal(8.5, 1.2, n_records).clip(500, 20000),
        'Total_Trans_Ct': np.random.randint(10, 140, n_records),
        'Total_Amt_Chng_Q4_Q1': np.random.normal(0.76, 0.4, n_records).clip(0, 3.4),
        'Total_Ct_Chng_Q4_Q1': np.random.normal(0.72, 0.4, n_records).clip(0, 3.7),
        'Avg_Utilization_Ratio': np.random.beta(2, 5, n_records),
        'Last_Transaction_Date': pd.date_range(start='2023-01-01', end='2024-12-31', periods=n_records),
        'Attrition_Flag': np.random.choice(['Existing Customer', 'Attrited Customer'], n_records, p=[0.8, 0.2])
    }

    df = pd.DataFrame(data)
    df['Avg_Open_To_Buy'] = df['Credit_Limit'] - df['Total_Revolving_Bal']
    df['Attrition_Flag_Binary'] = (df['Attrition_Flag'] == 'Attrited Customer').astype(int)

    return df

# Load the actual CSV data
@st.cache_data
def load_credit_card_data():
    """Load the actual credit card dataset and perform initial cleaning"""
    try:
        # Read the CSV file uploaded by user
        df = pd.read_csv('credit_card_churn_100k.csv')

        # Drop CLIENTNUM as it's just an ID
        if 'CLIENTNUM' in df.columns:
            df = df.drop('CLIENTNUM', axis=1)

        # Convert date column
        if 'Last_Transaction_Date' in df.columns:
            df['Last_Transaction_Date'] = pd.to_datetime(df['Last_Transaction_Date'])

        # Convert Attrition_Flag to binary
        if 'Attrition_Flag' in df.columns:
            df['Attrition_Flag_Binary'] = (df['Attrition_Flag'] == 'Attrited Customer').astype(int)

        st.success(f"‚úÖ Successfully loaded {len(df):,} customer records from CSV file")

        return df

    except FileNotFoundError:
        st.error("CSV file not found. Please ensure 'credit_card_churn_100k.csv' is in the same directory.")
        # Return sample data as fallback
        return load_sample_data()
    except Exception as e:
        st.error(f"Error loading CSV file: {str(e)}")
        return load_sample_data()

# File upload widget for CSV
def load_data_with_upload():
    """Allow users to upload their CSV file"""
    st.sidebar.markdown("### Data Source")

    uploaded_file = st.sidebar.file_uploader(
        "Upload Credit Card Data CSV",
        type=['csv'],
        help="Upload your credit_card_churn_100k.csv file"
    )

    if uploaded_file is not None:
        try:
            df = pd.read_csv(uploaded_file)

            # Data validation
            required_columns = ['Customer_Age', 'Gender', 'Total_Trans_Amt', 'Total_Trans_Ct',
                              'Attrition_Flag', 'Credit_Limit', 'Avg_Utilization_Ratio']

            missing_cols = [col for col in required_columns if col not in df.columns]
            if missing_cols:
                st.error(f"Missing required columns: {missing_cols}")
                return None

            # Drop CLIENTNUM if exists
            if 'CLIENTNUM' in df.columns:
                df = df.drop('CLIENTNUM', axis=1)

            # Convert date column
            if 'Last_Transaction_Date' in df.columns:
                df['Last_Transaction_Date'] = pd.to_datetime(df['Last_Transaction_Date'])

            # Convert Attrition_Flag to binary
            df['Attrition_Flag_Binary'] = (df['Attrition_Flag'] == 'Attrited Customer').astype(int)

            st.sidebar.success(f"‚úÖ Loaded {len(df):,} records")
            return df

        except Exception as e:
            st.sidebar.error(f"Error loading file: {str(e)}")
            return None
    else:
        st.sidebar.info("üëÜ Upload CSV file to use real data")
        return None

def calculate_clv(df):
    """Calculate Customer Lifetime Value"""
    interchange_rate = 0.025
    annual_fee_avg = 95
    interest_rate = 0.18
    profit_margin = 0.15

    # Estimate CLV based on transaction amounts and balances
    clv = (df['Total_Trans_Amt'] * interchange_rate +
           annual_fee_avg +
           df['Total_Revolving_Bal'] * interest_rate) * profit_margin

    return clv

# Feature engineering functions
def create_financial_features(df):
    """Create advanced financial health indicators"""
    # Credit health score
    df['Credit_Health_Score'] = (
        (df['Credit_Limit']/ df['Credit_Limit'].max()) * 0.3 +
        (1 - df['Avg_Utilization_Ratio']) * 0.4 +
        (df['Total_Trans_Ct']/df['Total_Trans_Ct'].max()) * 0.3
    )

    # Payment capacity ratio
    df['Payment_Capacity'] = df['Avg_Open_To_Buy'] / df['Credit_Limit']

    # Transaction efficiency
    df['Transaction_Efficiency'] = df['Total_Trans_Amt'] / df['Total_Trans_Ct']

    # Relationship tenure value
    df['Tenure_Value_Ratio'] = df['Total_Trans_Ct'] / df['Months_on_book']

    return df

def create_behavioural_features(df):
    """Create behavioural patterns indicators"""
    # Activity consistency
    df['Activity_Consistency'] = 1/(1+df['Months_Inactive_12_mon'])

    # Service interaction intensity
    df['Service_Intensity'] = df['Contacts_Count_12_mon'] / 12

    # Usage volatility (Q4 vs Q1 changes)
    df['Usage_Volatility'] =  abs(df['Total_Amt_Chng_Q4_Q1'] -1) + abs(df['Total_Ct_Chng_Q4_Q1'] - 1)

    # Cross-product engagement
    df['Cross_Product_Engagement'] = df['Total_Relationship_Count'] / 6 # Normalized

    return df

def create_risk_features(df):
    """Create risk scoring features"""
    # High utilization risk
    df['High_Util_Risk'] = (df['Avg_Utilization_Ratio'] > 0.8).astype(int)

    # Declining usage risk
    df['Declining_Usage_Risk'] = ((df['Total_Amt_Chng_Q4_Q1'] < 0.7) | (df['Total_Ct_Chng_Q4_Q1'] < 0.7)).astype(int)

    # Single product risk
    df['Single_Product_Risk'] = (df['Total_Relationship_Count'] == 1).astype(int)

    return df

def calculate_rfm_scores(df, transaction_date_col='Last_Transaction_Date'):
    """Calculate RFM scores for credit card customers"""
    # Calculate Recency (days since last transaction)
    current_date = pd.to_datetime('2025-01-31') # Reference date
    df['Recency'] = (current_date - pd.to_datetime(df[transaction_date_col])).dt.days

    # Frequency is already available as Total_Trans_Ct
    df['Frequency'] = df['Total_Trans_Ct']

    # Monetary is already available as Total_Trans_Amt
    df['Monetary'] = df['Total_Trans_Amt']

    # Create quintile scores (1-5, where 5 is best)
    df['R_Score'] = pd.qcut(df['Recency'], 5, labels=[5,4,3,2,1], duplicates='drop') # Lower recency = higher score
    df['F_Score'] = pd.qcut(df['Frequency'].rank(method='first'), 5, labels=[1,2,3,4,5], duplicates='drop')
    df['M_Score'] = pd.qcut(df['Monetary'].rank(method='first'), 5, labels=[1,2,3,4,5], duplicates='drop')

    # Convert to numeric
    df['R_Score'] = pd.to_numeric(df['R_Score'], errors='coerce').fillna(3).astype(int)
    df['F_Score'] = pd.to_numeric(df['F_Score'], errors='coerce').fillna(3).astype(int)
    df['M_Score'] = pd.to_numeric(df['M_Score'], errors='coerce').fillna(3).astype(int)

    # Create RFM segments
    df['RFM_Score'] = df['R_Score'].astype(str) + df['F_Score'].astype(str) + df['M_Score'].astype(str)

    return df

def create_cc_segments(df):
    """Create credit card specific customer segments"""
    # Define segment rules based on RFM scores
    def assign_segment(row):
        r, f, m = row['R_Score'], row['F_Score'], row['M_Score']

        # Champions: High value, frequent, recent users
        if r >= 4 and f >= 4 and m >= 4:
            return 'Champions'
        # Loyal Customers: Regular users with good value
        elif r >= 3 and f >= 3 and m >= 3:
            return 'Loyal Customers'
        # Potential Loyalists: Good recent activity, building frequency
        elif r >= 4 and f >= 3 and m >= 2:
            return 'Potential Loyalists'
        # At Risk: Previously good customers, declining activity
        elif r <= 2 and f >= 3 and m >= 3:
            return 'At Risk'
        # Cannot Lose Them: High value but very low recent activity
        elif r <= 2 and f >= 4 and m >= 4:
            return 'Cannot Lose Them'
        # New Customers: Recent but low frequency/value
        elif r >= 4 and f <= 2 and m <= 2:
            return 'New Customers'
        # Hibernating: Low scores across all dimensions
        elif r <= 2 and f <= 2 and m <= 2:
            return 'Hibernating'
        # Need Attention: Moderate scores but concerning patterns
        else:
            return 'Need Attention'

    df['Customer_Segment'] = df.apply(assign_segment, axis=1)

    # Create segment mapping for encoding
    segment_mapping = {'Champions': 1, 'Loyal Customers': 2, 'Potential Loyalists': 3,
                      'At Risk': 4, 'Cannot Lose Them': 5, 'New Customers': 6,
                      'Hibernating': 7, 'Need Attention': 8}
    df['Customer_Segment_Encoded'] = df['Customer_Segment'].map(segment_mapping)

    return df

def time_based_features(df):
    """Create time-based features"""
    df['Spending_Trend'] = df['Total_Amt_Chng_Q4_Q1']
    df['Activity_Trend'] = df['Total_Ct_Chng_Q4_Q1']
    df['Declining_Spend_Flag'] = (df['Total_Amt_Chng_Q4_Q1'] < 1).astype(int)
    df['Declining_Activity_Flag'] = (df['Total_Ct_Chng_Q4_Q1'] < 1).astype(int)

    return df

# Live Data Cleaning and Monitoring
class LiveDataProcessor:
    """Real-time data processing and monitoring class"""

    def __init__(self):
        self.processing_steps = []
        self.data_quality_metrics = {}

    def assess_data_quality(self, df):
        """Live data quality assessment"""
        st.subheader("üîç Live Data Quality Assessment")

        # Missing values analysis
        missing_data = df.isnull().sum()
        missing_pct = (missing_data / len(df) * 100).round(2)

        col1, col2, col3 = st.columns(3)

        with col1:
            st.metric("Total Records", f"{len(df):,}")
            st.metric("Total Features", len(df.columns))

        with col2:
            st.metric("Missing Values", missing_data.sum())
            st.metric("Complete Records", f"{(df.dropna().shape[0]):,}")

        with col3:
            duplicate_count = df.duplicated().sum()
            st.metric("Duplicate Records", duplicate_count)
            data_quality_score = ((len(df) - missing_data.sum() - duplicate_count) / (len(df) * len(df.columns)) * 100)
            st.metric("Data Quality Score", f"{data_quality_score:.1f}%")

        # Missing values visualization
        if missing_data.sum() > 0:
            missing_df = pd.DataFrame({
                'Column': missing_data.index,
                'Missing_Count': missing_data.values,
                'Missing_Percentage': missing_pct.values
            }).query('Missing_Count > 0').sort_values('Missing_Count', ascending=False)

            if not missing_df.empty:
                fig_missing = px.bar(missing_df, x='Column', y='Missing_Percentage',
                                   title="Missing Values by Column (%)",
                                   color='Missing_Percentage', color_continuous_scale='Reds')
                fig_missing.update_layout(height=300, xaxis_tickangle=45)
                st.plotly_chart(fig_missing, use_container_width=True)

        return missing_data, data_quality_score

    def clean_data(self, df):
        """Live data cleaning with monitoring"""
        st.subheader("üßπ Live Data Cleaning Pipeline")

        cleaning_progress = st.progress(0)
        status_text = st.empty()

        # Step 1: Handle missing values
        status_text.text("Step 1/5: Handling missing values...")
        cleaning_progress.progress(20)

        # Create utilization group for missing value context
        df['Utilization_Group'] = pd.cut(df['Avg_Utilization_Ratio'],
                                       bins=[0, 0.1, 0.3, 0.7, 1.0],
                                       labels=['Low (0-10%)', 'Moderate (10-30%)',
                                              'High (30-70%)', 'Very High (70%+)'])

        # Remove rows with missing utilization group (critical for analysis)
        initial_count = len(df)
        df = df.dropna(subset=['Utilization_Group'])
        removed_count = initial_count - len(df)

        if removed_count > 0:
            st.warning(f"Removed {removed_count} records with critical missing values")

        # Step 2: Handle outliers
        status_text.text("Step 2/5: Detecting outliers...")
        cleaning_progress.progress(40)

        # Outlier detection for key financial metrics
        outlier_columns = ['Credit_Limit', 'Total_Trans_Amt', 'Total_Revolving_Bal']
        outlier_counts = {}

        for col in outlier_columns:
            if col in df.columns:
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR

                outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
                outlier_counts[col] = len(outliers)

        # Step 3: Data type validation
        status_text.text("Step 3/5: Validating data types...")
        cleaning_progress.progress(60)

        # Ensure numeric columns are properly typed
        numeric_columns = ['Customer_Age', 'Credit_Limit', 'Total_Trans_Amt', 'Total_Trans_Ct', 'Avg_Utilization_Ratio']
        for col in numeric_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')

        # Step 4: Business rule validation
        status_text.text("Step 4/5: Applying business rules...")
        cleaning_progress.progress(80)

        # Business rule validations
        validation_results = {}

        # Rule 1: Credit utilization should not exceed 100%
        if 'Avg_Utilization_Ratio' in df.columns:
            high_util = (df['Avg_Utilization_Ratio'] > 1.0).sum()
            validation_results['High Utilization (>100%)'] = high_util

        # Rule 2: Revolving balance should not exceed credit limit
        if 'Total_Revolving_Bal' in df.columns and 'Credit_Limit' in df.columns:
            over_limit = (df['Total_Revolving_Bal'] > df['Credit_Limit']).sum()
            validation_results['Balance Over Limit'] = over_limit

        # Rule 3: Age should be reasonable (18-100)
        if 'Customer_Age' in df.columns:
            invalid_age = ((df['Customer_Age'] < 18) | (df['Customer_Age'] > 100)).sum()
            validation_results['Invalid Age'] = invalid_age

        # Step 5: Final validation
        status_text.text("Step 5/5: Final validation...")
        cleaning_progress.progress(100)

        # Summary metrics
        final_count = len(df)
        data_retention_rate = (final_count / initial_count * 100) if initial_count > 0 else 100

        status_text.text("‚úÖ Data cleaning completed!")

        # Display cleaning results
        col1, col2 = st.columns(2)

        with col1:
            st.subheader("üìä Cleaning Results")
            st.metric("Records Retained", f"{final_count:,}")
            st.metric("Data Retention Rate", f"{data_retention_rate:.1f}%")
            st.metric("Records Removed", f"{removed_count:,}")

        with col2:
            st.subheader("‚ö†Ô∏è Data Quality Issues")
            for issue, count in validation_results.items():
                if count > 0:
                    st.warning(f"{issue}: {count:,} records")
                else:
                    st.success(f"{issue}: ‚úÖ No issues")

        # Outlier summary
        if outlier_counts:
            st.subheader("üìà Outlier Detection Results")
            outlier_df = pd.DataFrame(list(outlier_counts.items()), columns=['Column', 'Outlier_Count'])
            if not outlier_df.empty:
                fig_outliers = px.bar(outlier_df, x='Column', y='Outlier_Count',
                                    title="Outliers Detected by Column",
                                    color='Outlier_Count', color_continuous_scale='Oranges')
                st.plotly_chart(fig_outliers, use_container_width=True)

        return df

    def engineer_features_live(self, df):
        """Live feature engineering with progress monitoring"""
        st.subheader("‚öôÔ∏è Live Feature Engineering Pipeline")

        feature_progress = st.progress(0)
        status_text = st.empty()

        # Step 1: Financial Features
        status_text.text("Step 1/5: Creating financial health indicators...")
        feature_progress.progress(20)

        df = create_financial_features(df)

        # Step 2: Behavioral Features
        status_text.text("Step 2/5: Creating behavioral patterns...")
        feature_progress.progress(40)

        df = create_behavioural_features(df)

        # Step 3: Risk Features
        status_text.text("Step 3/5: Creating risk indicators...")
        feature_progress.progress(60)

        df = create_risk_features(df)

        # Step 4: RFM Analysis
        status_text.text("Step 4/5: Calculating RFM scores...")
        feature_progress.progress(80)

        df = calculate_rfm_scores(df)
        df = create_cc_segments(df)

        # Step 5: Time-based Features
        status_text.text("Step 5/5: Creating time-based features...")
        feature_progress.progress(100)

        df = time_based_features(df)

        status_text.text("‚úÖ Feature engineering completed!")

        # Feature engineering summary
        new_features = [col for col in df.columns if col not in ['Customer_Age', 'Gender', 'Dependent_count',
                       'Education_Level', 'Marital_Status', 'Income_Category', 'Card_Category']]

        col1, col2, col3 = st.columns(3)

        with col1:
            st.metric("Original Features", len(df.columns) - len(new_features))
        with col2:
            st.metric("Engineered Features", len(new_features))
        with col3:
            st.metric("Total Features", len(df.columns))

        # Feature categories
        st.subheader("üìã Feature Categories Created")

        feature_categories = {
            "Financial Health": ['Credit_Health_Score', 'Payment_Capacity', 'Transaction_Efficiency', 'Tenure_Value_Ratio'],
            "Behavioral Patterns": ['Activity_Consistency', 'Service_Intensity', 'Usage_Volatility', 'Cross_Product_Engagement'],
            "Risk Indicators": ['High_Util_Risk', 'Declining_Usage_Risk', 'Single_Product_Risk'],
            "RFM Segmentation": ['R_Score', 'F_Score', 'M_Score', 'Customer_Segment'],
            "Temporal Features": ['Spending_Trend', 'Activity_Trend', 'Declining_Spend_Flag', 'Declining_Activity_Flag']
        }

        for category, features in feature_categories.items():
            available_features = [f for f in features if f in df.columns]
            if available_features:
                st.write(f"**{category}**: {', '.join(available_features)}")

        return df

# Real-time monitoring dashboard
def create_live_monitoring_dashboard(df):
    """Create live monitoring dashboard for business analysts"""
    st.markdown('<h1 class="main-header">üìä Live Credit Card Churn Monitoring</h1>', unsafe_allow_html=True)

    # Real-time metrics
    col1, col2, col3, col4, col5, col6 = st.columns(6)

    current_churn_rate = df['Attrition_Flag_Binary'].mean()
    total_customers = len(df)
    
    # Calculate churn probability if not available
    if 'Churn_Probability' not in df.columns:
        df['Churn_Probability'] = (
            (df['Months_Inactive_12_mon'] >= 3).astype(float) * 0.25 +
            (df['Total_Trans_Ct'] < 30).astype(float) * 0.15 +
            (df['Total_Relationship_Count'] == 1).astype(float) * 0.12 +
            (df['Contacts_Count_12_mon'] >= 4).astype(float) * 0.20 +
            0.16  # Base rate
        ).clip(0.02, 0.90)
    
    high_risk_customers = (df['Churn_Probability'] > 0.7).sum()
    avg_clv = calculate_clv(df).mean()

    with col1:
        st.metric("Active Customers", f"{total_customers:,}",
                 delta=f"+{int(total_customers*0.02)} this month")

    with col2:
        st.metric("Current Churn Rate", f"{current_churn_rate:.1%}",
                 delta=f"{(current_churn_rate-0.20):.1%} vs target", delta_color="inverse")

    with col3:
        st.metric("High Risk Customers", f"{high_risk_customers:,}",
                 delta=f"-{int(high_risk_customers*0.05)} vs last week", delta_color="inverse")

    with col4:
        st.metric("Avg Customer Value", f"${avg_clv:.0f}",
                 delta=f"+${avg_clv*0.08:.0f}")

    with col5:
        revenue_at_risk = high_risk_customers * avg_clv
        st.metric("Revenue at Risk", f"${revenue_at_risk:,.0f}",
                 delta="-8.5% vs last month", delta_color="inverse")

    with col6:
        st.metric("Data Freshness", "Real-time",
                 delta="Updated now", delta_color="normal")

    # Live alerts section
    st.markdown("---")
    st.subheader("üö® Live Alerts & Monitoring")

    alert_col1, alert_col2 = st.columns(2)

    with alert_col1:
        # High-risk customer alerts
        if 'Churn_Probability' in df.columns:
            critical_customers = df[df['Churn_Probability'] > 0.8]
        else:
            # Use proxy based on risk factors
            critical_customers = df[
                (df['Months_Inactive_12_mon'] >= 3) |
                (df['Total_Trans_Ct'] < 30) |
                (df['Contacts_Count_12_mon'] >= 4)
            ]

        st.metric("üî¥ Critical Risk Alerts", len(critical_customers))

        if len(critical_customers) > 0:
            st.warning(f"{len(critical_customers)} customers need immediate attention!")

            # Show sample of critical customers
            critical_sample = critical_customers.head(5)[['Customer_Age', 'Card_Category',
                                                        'Months_Inactive_12_mon', 'Total_Trans_Ct']].copy()
            critical_sample.index = [f"Customer_{i+1}" for i in range(len(critical_sample))]
            st.dataframe(critical_sample, use_container_width=True)

    with alert_col2:
        # Trend alerts
        declining_usage = df[df['Total_Amt_Chng_Q4_Q1'] < 0.7]
        st.metric("üìâ Declining Usage Alert", len(declining_usage))

        if len(declining_usage) > 0:
            st.warning(f"{len(declining_usage)} customers showing declining usage patterns!")

        # System health
        st.metric("‚öôÔ∏è System Health", "99.8%", delta="+0.2% uptime")
        st.success("All monitoring systems operational")

    return df

def calculate_risk_score_live(age, months_inactive, total_trans_ct, utilization_ratio, total_relationship, contacts_count):
    """Calculate risk score using live business rules"""
    risk_score = 0.16  # Base risk

    # Age factor
    if age < 35:
        risk_score += 0.08
    elif age > 55:
        risk_score -= 0.05

    # Activity factors
    if months_inactive >= 3:
        risk_score += 0.25
    elif months_inactive == 0:
        risk_score -= 0.08

    # Transaction patterns
    if total_trans_ct < 30:
        risk_score += 0.15
    elif total_trans_ct > 100:
        risk_score -= 0.10

    # Utilization patterns
    if utilization_ratio == 0:
        risk_score += 0.20
    elif utilization_ratio > 0.8:
        risk_score += 0.10
    elif 0.1 <= utilization_ratio <= 0.3:
        risk_score -= 0.05

    # Relationship depth
    if total_relationship == 1:
        risk_score += 0.12
    elif total_relationship >= 4:
        risk_score -= 0.15

    # Contact frequency
    if contacts_count >= 4:
        risk_score += 0.20
    elif contacts_count == 0:
        risk_score += 0.05

    return np.clip(risk_score, 0.02, 0.90)

# Load data using upload or fallback
df = load_data_with_upload()

if df is None:
    st.info("üìÅ Using sample data for demonstration. Upload your CSV file for real analysis.")
    df = load_sample_data()

# Add CLV and other derived features for sample data
df['CLV'] = calculate_clv(df)

# Create utilization group
df['Utilization_Group'] = pd.cut(df['Avg_Utilization_Ratio'],
                               bins=[0, 0.1, 0.3, 0.7, 1.0],
                               labels=['Low (0-10%)', 'Moderate (10-30%)',
                                      'High (30-70%)', 'Very High (70%+)'])

# Add basic feature engineering for sample data
df = create_financial_features(df)
df = create_behavioural_features(df)
df = create_risk_features(df)
df = calculate_rfm_scores(df)
df = create_cc_segments(df)
df = time_based_features(df)

# Add churn probability
if 'Churn_Probability' not in df.columns:
    df['Churn_Probability'] = (
        (df['Months_Inactive_12_mon'] >= 3).astype(float) * 0.25 +
        (df['Total_Trans_Ct'] < 30).astype(float) * 0.15 +
        (df['Total_Relationship_Count'] == 1).astype(float) * 0.12 +
        (df['Contacts_Count_12_mon'] >= 4).astype(float) * 0.20 +
        0.16  # Base rate
    ).clip(0.02, 0.90)

# Initialize live data processor
processor = LiveDataProcessor()

# Sidebar navigation with live monitoring options
st.sidebar.title("üè¶ Navigation")
page = st.sidebar.selectbox("Select Page",
                           ["üìä Live Monitoring Dashboard", "üßπ Data Processing Pipeline",
                            "üîÆ Churn Prediction", "üí∞ Business Impact", "üìà Advanced Analytics"])

# Live monitoring controls
st.sidebar.markdown("### Live Monitoring Controls")
auto_refresh = st.sidebar.checkbox("Auto Refresh Data", value=False)
refresh_interval = st.sidebar.selectbox("Refresh Interval", ["30 seconds", "1 minute", "5 minutes"], index=1)

if auto_refresh:
    st.sidebar.success("üü¢ Live monitoring active")
else:
    st.sidebar.info("‚ö™ Manual mode")

# Manual refresh button
if st.sidebar.button("üîÑ Refresh Data Now"):
    st.cache_data.clear()
    st.rerun()

if page == "üìä Live Monitoring Dashboard":
    # Create live monitoring dashboard
    df = create_live_monitoring_dashboard(df)

    # Live churn trends
    st.subheader("üìà Live Churn Trends & KPIs")

    col1, col2 = st.columns([2, 1])

    with col1:
        # Churn rate by key segments with live updates
        tab1, tab2, tab3 = st.tabs(["By Demographics", "By Activity", "By Financial Health"])

        with tab1:
            # Age groups churn analysis
            df['Age_Group'] = pd.cut(df['Customer_Age'], bins=[0, 30, 40, 50, 60, 100],
                                   labels=['<30', '30-40', '40-50', '50-60', '60+'])
            age_churn = df.groupby('Age_Group')['Attrition_Flag_Binary'].agg(['count', 'mean']).reset_index()
            age_churn.columns = ['Age_Group', 'Customers', 'Churn_Rate']

            fig_age = px.bar(age_churn, x='Age_Group', y='Churn_Rate',
                           title="Live Churn Rate by Age Group",
                           color='Churn_Rate', color_continuous_scale='Reds',
                           text='Churn_Rate')
            fig_age.update_traces(texttemplate='%{text:.1%}', textposition='outside')
            fig_age.update_layout(height=350)
            st.plotly_chart(fig_age, use_container_width=True)

        with tab2:
            # Activity analysis
            activity_churn = df.groupby('Months_Inactive_12_mon')['Attrition_Flag_Binary'].mean().reset_index()
            activity_churn.columns = ['Months_Inactive', 'Churn_Rate']

            fig_activity = px.line(activity_churn, x='Months_Inactive', y='Churn_Rate',
                                 title="Live Churn Rate by Months Inactive",
                                 markers=True)
            fig_activity.update_traces(line_color='red', line_width=3, marker_size=8)
            fig_activity.update_layout(height=350)
            st.plotly_chart(fig_activity, use_container_width=True)

        with tab3:
            # Financial health analysis
            util_churn = df.groupby('Utilization_Group')['Attrition_Flag_Binary'].mean().reset_index()
            util_churn = util_churn.dropna()

            fig_util = px.bar(util_churn, x='Utilization_Group', y='Attrition_Flag_Binary',
                             title="Live Churn Rate by Credit Utilization",
                             color='Attrition_Flag_Binary', color_continuous_scale='Oranges',
                             text='Attrition_Flag_Binary')
            fig_util.update_traces(texttemplate='%{text:.1%}', textposition='outside')
            fig_util.update_layout(height=350, xaxis_tickangle=45)
            st.plotly_chart(fig_util, use_container_width=True)

    with col2:
        # Live risk distribution
        st.subheader("üéØ Live Risk Distribution")

        # Create risk categories based on multiple factors
        df['Risk_Score'] = (
            (df['Months_Inactive_12_mon'] >= 3).astype(int) * 25 +
            (df['Total_Trans_Ct'] < 30).astype(int) * 20 +
            (df['Contacts_Count_12_mon'] >= 4).astype(int) * 20 +
            (df['Avg_Utilization_Ratio'] > 0.8).astype(int) * 15 +
            (df['Total_Relationship_Count'] == 1).astype(int) * 10
        )

        df['Risk_Category'] = pd.cut(df['Risk_Score'],
                                   bins=[0, 20, 40, 60, 100],
                                   labels=['Low Risk', 'Medium Risk', 'High Risk', 'Critical Risk'])

        risk_counts = df['Risk_Category'].value_counts()

        fig_risk = go.Figure(data=[
            go.Pie(labels=risk_counts.index,
                  values=risk_counts.values,
                  hole=0.6,
                  marker_colors=['green', 'yellow', 'orange', 'red'],
                  textinfo='label+percent')
        ])
        fig_risk.update_layout(title_text="Live Risk Distribution", height=350)
        st.plotly_chart(fig_risk, use_container_width=True)

        # Risk summary metrics
        st.subheader("‚ö° Real-time Alerts")

        critical_count = (df['Risk_Category'] == 'Critical Risk').sum()
        high_count = (df['Risk_Category'] == 'High Risk').sum()

        if critical_count > 0:
            st.error(f"üö® {critical_count} customers in CRITICAL risk!")

        if high_count > 0:
            st.warning(f"‚ö†Ô∏è {high_count} customers in HIGH risk!")

        if critical_count == 0 and high_count == 0:
            st.success("‚úÖ No critical alerts at this time")

elif page == "üßπ Data Processing Pipeline":
    st.markdown('<h1 class="main-header">üßπ Live Data Processing Pipeline</h1>', unsafe_allow_html=True)

    # Data quality assessment
    missing_data, quality_score = processor.assess_data_quality(df)

    # Data cleaning pipeline
    df_cleaned = processor.clean_data(df)

    # Feature engineering pipeline
    df_processed = processor.engineer_features_live(df_cleaned)

    # Processing summary
    st.markdown("---")
    st.subheader("üìã Processing Pipeline Summary")

    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric("Original Records", f"{len(df):,}")
        st.metric("Data Quality Score", f"{quality_score:.1f}%")

    with col2:
        st.metric("Cleaned Records", f"{len(df_cleaned):,}")
        retention_rate = len(df_cleaned) / len(df) * 100
        st.metric("Data Retention", f"{retention_rate:.1f}%")

    with col3:
        st.metric("Final Features", len(df_processed.columns))
        engineered_features = len(df_processed.columns) - len(df.columns)
        st.metric("Features Added", f"+{engineered_features}")

    with col4:
        st.metric("Processing Status", "‚úÖ Complete")
        st.metric("Pipeline Health", "100%")

    # Feature importance preview
    if 'Attrition_Flag_Binary' in df_processed.columns:
        st.subheader("üéØ Feature Engineering Impact")

        # Calculate correlations with target
        numeric_cols = df_processed.select_dtypes(include=[np.number]).columns
        correlations = df_processed[numeric_cols].corr()['Attrition_Flag_Binary'].abs().sort_values(ascending=False)

        # Top 10 features
        top_features = correlations.head(11)[1:]  # Exclude self-correlation

        col1, col2 = st.columns([2, 1])

        with col1:
            fig_corr = px.bar(x=top_features.values, y=top_features.index,
                             orientation='h',
                             title="Top 10 Features by Correlation with Churn",
                             color=top_features.values,
                             color_continuous_scale='Viridis')
            fig_corr.update_layout(height=400, yaxis={'categoryorder':'total ascending'})
            st.plotly_chart(fig_corr, use_container_width=True)

        with col2:
            st.write("**Top Predictive Features:**")
            for i, (feature, corr) in enumerate(top_features.head(5).items(), 1):
                st.write(f"{i}. **{feature}**: {corr:.3f}")

            st.write("**Feature Categories:**")
            st.write("üîπ **Behavioral**: Activity patterns")
            st.write("üîπ **Financial**: Credit utilization")
            st.write("üîπ **Engagement**: Service interactions")
            st.write("üîπ **Temporal**: Usage trends")

    # Store processed data in session state
    st.session_state['processed_data'] = df_processed

elif page == "üîÆ Churn Prediction":
    st.markdown('<h1 class="main-header">üîÆ Live Churn Prediction Engine</h1>', unsafe_allow_html=True)

    # Use processed data if available
    if 'processed_data' in st.session_state:
        df_pred = st.session_state['processed_data']
        st.success("‚úÖ Using processed data from pipeline")
    else:
        df_pred = df
        st.info("üí° Run 'Data Processing Pipeline' first for enhanced predictions")

    col1, col2 = st.columns([1, 2])

    with col1:
        st.subheader("üéØ Customer Risk Assessment")

        # Customer input form with live validation
        with st.form("live_prediction_form"):
            st.markdown("**Customer Demographics**")
            age = st.slider("Customer Age", 18, 75, 45)
            gender = st.selectbox("Gender", ["M", "F"])
            dependents = st.selectbox("Number of Dependents", [0, 1, 2, 3, 4, 5])
            education = st.selectbox("Education Level",
                                   ["High School", "College", "Graduate", "Post-Graduate", "Doctorate"])
            income = st.selectbox("Income Category",
                                ["Less than $40K", "$40K - $60K", "$60K - $80K", "$80K - $120K", "$120K +"])
            card_category = st.selectbox("Card Category", ["Blue", "Silver", "Gold", "Platinum"])

            st.markdown("**Account Information**")
            months_on_book = st.slider("Months on Book", 13, 56, 35)
            total_relationship = st.slider("Total Products", 1, 6, 3)
            months_inactive = st.slider("Months Inactive (Last 12)", 0, 6, 1)
            contacts_count = st.slider("Contacts Count (Last 12)", 0, 6, 2)

            st.markdown("**Financial Information**")
            credit_limit = st.number_input("Credit Limit ($)", 1500, 35000, 12000)
            total_trans_amt = st.number_input("Total Transaction Amount ($)", 500, 20000, 4500)
            total_trans_ct = st.slider("Total Transaction Count", 10, 140, 64)
            utilization_ratio = st.slider("Utilization Ratio", 0.0, 1.0, 0.3)

            predict_button = st.form_submit_button("üîÆ Predict Churn Risk", type="primary")

    with col2:
        if predict_button:
            # Live prediction using trained models or fallback logic
            if 'trained_models' in st.session_state and 'best_model_name' in st.session_state:
                st.info("ü§ñ Using trained ML model for prediction")
                # Use trained model logic (from previous implementation)
                try:
                    best_model = st.session_state['trained_models'][st.session_state['best_model_name']]
                    # Create input data frame for prediction
                    input_data = pd.DataFrame({
                        'Customer_Age': [age],
                        'Dependent_count': [dependents],
                        'Months_on_book': [months_on_book],
                        'Total_Relationship_Count': [total_relationship],
                        'Months_Inactive_12_mon': [months_inactive],
                        'Contacts_Count_12_mon': [contacts_count],
                        'Credit_Limit': [credit_limit],
                        'Total_Revolving_Bal': [credit_limit * utilization_ratio],
                        'Total_Trans_Amt': [total_trans_amt],
                        'Total_Trans_Ct': [total_trans_ct],
                        'Avg_Utilization_Ratio': [utilization_ratio],
                    })
                    
                    # Use model for prediction
                    risk_score = best_model.predict_proba(input_data)[0][1]
                except:
                    # Fallback to rule-based
                    risk_score = calculate_risk_score_live(
                        age, months_inactive, total_trans_ct, utilization_ratio,
                        total_relationship, contacts_count
                    )
            else:
                st.info("üìä Using rule-based prediction engine")
                # Rule-based prediction
                risk_score = calculate_risk_score_live(
                    age, months_inactive, total_trans_ct, utilization_ratio,
                    total_relationship, contacts_count
                )

            # Display live prediction results
            st.subheader("üìä Live Prediction Results")

            # Risk gauge with live updates
            fig_risk = go.Figure(go.Indicator(
                mode = "gauge+number+delta",
                value = risk_score * 100,
                domain = {'x': [0, 1], 'y': [0, 1]},
                title = {'text': "Churn Risk Score (%)"},
                delta = {'reference': 20, 'increasing': {'color': "red"}, 'decreasing': {'color': "green"}},
                gauge = {'axis': {'range': [None, 100]},
                        'bar': {'color': "darkblue"},
                        'steps' : [
                            {'range': [0, 30], 'color': "lightgreen"},
                            {'range': [30, 70], 'color': "yellow"},
                            {'range': [70, 100], 'color': "red"}],
                        'threshold' : {'line': {'color': "red", 'width': 4},
                                     'thickness': 0.75, 'value': 70}}))
            fig_risk.update_layout(height=400)
            st.plotly_chart(fig_risk, use_container_width=True)

            # Live risk classification with recommendations
            if risk_score < 0.3:
                risk_level = "üü¢ LOW RISK"
                risk_color = "#4CAF50"
                priority = "Standard"
                recommendations = [
                    "‚úÖ Continue standard engagement programs",
                    "üìà Monitor for behavioral changes",
                    "üéØ Consider upselling opportunities",
                    "üìß Include in regular marketing campaigns"
                ]
            elif risk_score < 0.7:
                risk_level = "üü° MEDIUM RISK"
                risk_color = "#FF9800"
                priority = "Monitor"
                recommendations = [
                    "üéØ Implement targeted retention campaigns",
                    "üíù Offer personalized incentives",
                    "üìû Increase customer service touchpoints",
                    "üîç Review and optimize product offerings",
                    "üìä Track engagement metrics closely"
                ]
            else:
                risk_level = "üî¥ HIGH RISK"
                risk_color = "#F44336"
                priority = "Critical"
                recommendations = [
                    "üö® **IMMEDIATE ACTION REQUIRED**",
                    "üèÉ‚Äç‚ôÇÔ∏è Deploy emergency retention protocols",
                    "üë§ Assign dedicated account manager",
                    "üéÅ Offer premium incentives and rewards",
                    "üìã Conduct customer satisfaction survey",
                    "üìû Schedule personal outreach within 24 hours"
                ]

            # Risk level display
            st.markdown(f"""
            <div style="background-color: {risk_color}; color: white; padding: 1.5rem; border-radius: 0.5rem; text-align: center; margin: 1rem 0;">
                <h2>{risk_level}</h2>
                <p style="font-size: 1.2em; margin: 0;">Churn Probability: {risk_score:.1%}</p>
                <p style="font-size: 1em; margin: 0;">Priority Level: {priority}</p>
            </div>
            """, unsafe_allow_html=True)

            # Live recommendations
            st.subheader("üí° Live Action Recommendations")
            for i, rec in enumerate(recommendations, 1):
                st.write(f"{i}. {rec}")

            # Risk factor analysis
            st.subheader("üìà Risk Factor Breakdown")

            risk_factors = {
                'Inactivity Risk': max(0, (months_inactive - 1) * 0.1),
                'Transaction Volume Risk': max(0, (50 - total_trans_ct) * 0.004),
                'Relationship Risk': max(0, (3 - total_relationship) * 0.05),
                'Utilization Risk': abs(utilization_ratio - 0.3) * 0.3,
                'Contact Frequency Risk': max(0, (contacts_count - 2) * 0.06),
                'Age Risk': max(0, (35 - age) * 0.002) if age < 35 else 0
            }

            # Normalize risk factors
            total_risk = sum(risk_factors.values())
            if total_risk > 0:
                risk_factors = {k: v/total_risk for k, v in risk_factors.items()}

            fig_factors = px.bar(x=list(risk_factors.keys()), y=list(risk_factors.values()),
                               title="Risk Factor Contributions",
                               color=list(risk_factors.values()),
                               color_continuous_scale='Reds')
            fig_factors.update_layout(height=350, xaxis_tickangle=45)
            fig_factors.update_traces(texttemplate='%{y:.1%}', textposition='outside')
            st.plotly_chart(fig_factors, use_container_width=True)

        else:
            st.info("üëÜ Complete customer details and click 'Predict Churn Risk' for live analysis")

            # Live customer search and batch prediction
            st.subheader("üîç Live Customer Analysis")

            # Sample high-risk customers from data
            if 'Risk_Category' in df.columns:
                high_risk_sample = df[df['Risk_Category'].isin(['High Risk', 'Critical Risk'])].head(5)
            else:
                # Create risk categories on the fly
                high_risk_sample = df[
                    (df['Months_Inactive_12_mon'] >= 3) |
                    (df['Total_Trans_Ct'] < 30) |
                    (df['Contacts_Count_12_mon'] >= 4)
                ].head(5)

            if not high_risk_sample.empty:
                st.warning(f"‚ö†Ô∏è {len(high_risk_sample)} high-risk customers detected in live data")

                display_cols = ['Customer_Age', 'Card_Category', 'Months_Inactive_12_mon',
                               'Total_Trans_Ct', 'Avg_Utilization_Ratio']
                available_cols = [col for col in display_cols if col in high_risk_sample.columns]

                if available_cols:
                    sample_data = high_risk_sample[available_cols].copy()
                    sample_data.index = [f"Customer_{i+1}" for i in range(len(sample_data))]
                    st.dataframe(sample_data, use_container_width=True)

elif page == "üí∞ Business Impact":
    st.markdown('<h1 class="main-header">üí∞ Business Impact Analysis</h1>', unsafe_allow_html=True)

    # Use processed data if available
    if 'processed_data' in st.session_state:
        df_impact = st.session_state['processed_data']
        st.success("‚úÖ Using processed data with advanced features")
    else:
        df_impact = df
        st.info("üí° Run 'Data Processing Pipeline' first for enhanced analysis")

    # Business Impact Calculations
    total_customers = len(df_impact)

    # Calculate churn probability if not available
    if 'Churn_Probability' not in df_impact.columns:
        df_impact['Churn_Probability'] = (
            (df_impact['Months_Inactive_12_mon'] >= 3).astype(float) * 0.25 +
            (df_impact['Total_Trans_Ct'] < 30).astype(float) * 0.15 +
            (df_impact['Total_Relationship_Count'] == 1).astype(float) * 0.12 +
            (df_impact['Contacts_Count_12_mon'] >= 4).astype(float) * 0.20 +
            0.16  # Base rate
        ).clip(0.02, 0.90)

    high_risk_customers = (df_impact['Churn_Probability'] >= 0.7).sum()
    medium_risk_customers = ((df_impact['Churn_Probability'] >= 0.3) & (df_impact['Churn_Probability'] < 0.7)).sum()

    # Calculate CLV if not available
    if 'CLV' not in df_impact.columns:
        df_impact['CLV'] = calculate_clv(df_impact)

    avg_clv = df_impact['CLV'].mean()
    total_clv_at_risk = df_impact[df_impact['Churn_Probability'] >= 0.7]['CLV'].sum()

    # Retention campaign costs
    retention_cost_high = 500  # Cost per high-risk customer
    retention_cost_medium = 200  # Cost per medium-risk customer
    campaign_success_rate = 0.40  # 40% success rate

    # ROI Calculations
    high_risk_investment = high_risk_customers * retention_cost_high
    medium_risk_investment = medium_risk_customers * retention_cost_medium
    total_investment = high_risk_investment + medium_risk_investment

    expected_clv_saved = (high_risk_customers * avg_clv * campaign_success_rate) + \
                        (medium_risk_customers * avg_clv * 0.25)

    net_benefit = expected_clv_saved - total_investment
    roi_percentage = (net_benefit / total_investment) * 100 if total_investment > 0 else 0

    # Top metrics
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric("CLV at Risk", f"${total_clv_at_risk:,.0f}",
                 help="Total Customer Lifetime Value of high-risk customers")

    with col2:
        st.metric("Campaign Investment", f"${total_investment:,.0f}",
                 help="Total cost of retention campaigns")

    with col3:
        st.metric("Expected CLV Saved", f"${expected_clv_saved:,.0f}",
                 help="Expected value saved through successful retention")

    with col4:
        st.metric("Campaign ROI", f"{roi_percentage:.1f}%",
                 delta=f"{roi_percentage - 150:.1f}% vs target",
                 help="Return on investment for retention campaigns")

    st.markdown("---")

    # Detailed Analysis
    col1, col2 = st.columns(2)

    with col1:
        st.subheader("üìä Customer Segmentation Strategy")

        # Create segment analysis
        df_impact['Risk_Segment'] = pd.cut(df_impact['Churn_Probability'],
                                   bins=[0, 0.3, 0.7, 1.0],
                                   labels=['Low Risk', 'Medium Risk', 'High Risk'])

        segment_analysis = df_impact.groupby('Risk_Segment').agg({
            'Churn_Probability': 'count',
            'CLV': ['mean', 'sum']
        }).round(2)
        segment_analysis.columns = ['Customer_Count', 'Avg_CLV', 'Total_CLV']
        segment_analysis = segment_analysis.reset_index()

        # Add campaign costs and ROI
        segment_analysis['Campaign_Cost'] = segment_analysis.apply(
            lambda x: x['Customer_Count'] * (500 if x['Risk_Segment'] == 'High Risk'
                                           else 200 if x['Risk_Segment'] == 'Medium Risk' else 50), axis=1
        )

        segment_analysis['Expected_Savings'] = segment_analysis.apply(
            lambda x: x['Customer_Count'] * x['Avg_CLV'] * (0.4 if x['Risk_Segment'] == 'High Risk'
                                                          else 0.25 if x['Risk_Segment'] == 'Medium Risk' else 0.1), axis=1
        )

        segment_analysis['ROI'] = ((segment_analysis['Expected_Savings'] - segment_analysis['Campaign_Cost']) /
                                  segment_analysis['Campaign_Cost'] * 100).round(1)

        st.dataframe(segment_analysis, use_container_width=True)

        # Strategy recommendations
        st.subheader("üéØ Retention Strategy by Segment")

        strategies = {
            "üî¥ High Risk (‚â•70%)": {
                "customers": high_risk_customers,
                "strategy": "Emergency Retention Protocol",
                "tactics": ["VIP customer service", "Waive all fees", "Double rewards points", "Personal account manager"],
                "budget": f"${retention_cost_high:,}/customer",
                "expected_roi": "250-400%"
            },
            "üü° Medium Risk (30-70%)": {
                "customers": medium_risk_customers,
                "strategy": "Proactive Engagement Campaign",
                "tactics": ["Targeted offers", "Usage incentives", "Product education", "Satisfaction surveys"],
                "budget": f"${retention_cost_medium:,}/customer",
                "expected_roi": "150-250%"
            },
            "üü¢ Low Risk (<30%)": {
                "customers": total_customers - high_risk_customers - medium_risk_customers,
                "strategy": "Growth & Upselling Focus",
                "tactics": ["Cross-sell products", "Loyalty programs", "Referral incentives", "Premium upgrades"],
                "budget": "$50/customer",
                "expected_roi": "200-300%"
            }
        }

        for risk_level, details in strategies.items():
            with st.expander(f"{risk_level} - {details['customers']:,} customers"):
                st.write(f"**Strategy:** {details['strategy']}")
                st.write(f"**Budget:** {details['budget']}")
                st.write(f"**Expected ROI:** {details['expected_roi']}")
                st.write("**Tactics:**")
                for tactic in details['tactics']:
                    st.write(f"‚Ä¢ {tactic}")

    with col2:
        st.subheader("üìà Financial Impact Projections")

        # Monthly projection chart
        months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun']
        current_churn_rate = df_impact['Attrition_Flag_Binary'].mean()
        baseline_churn = [current_churn_rate] * 6
        with_intervention = [current_churn_rate * 0.85, current_churn_rate * 0.75, current_churn_rate * 0.65,
                           current_churn_rate * 0.60, current_churn_rate * 0.58, current_churn_rate * 0.55]

        fig_projection = go.Figure()
        fig_projection.add_trace(go.Scatter(x=months, y=[x*100 for x in baseline_churn],
                                          mode='lines+markers', name='Baseline Churn Rate',
                                          line=dict(color='red', width=3)))
        fig_projection.add_trace(go.Scatter(x=months, y=[x*100 for x in with_intervention],
                                          mode='lines+markers', name='With Retention Program',
                                          line=dict(color='green', width=3)))
        fig_projection.update_layout(title='Projected Churn Rate Reduction',
                                   xaxis_title='Month', yaxis_title='Churn Rate (%)',
                                   height=300)
        st.plotly_chart(fig_projection, use_container_width=True)

        # Revenue impact
        st.subheader("üíµ Revenue Impact Analysis")

        current_monthly_revenue = total_customers * avg_clv / 12
        prevented_churn_customers = high_risk_customers * campaign_success_rate
        additional_monthly_revenue = prevented_churn_customers * avg_clv / 12

        revenue_metrics = {
            "Current Monthly Revenue": f"${current_monthly_revenue:,.0f}",
            "Customers Saved (Est.)": f"{prevented_churn_customers:,.0f}",
            "Additional Monthly Revenue": f"${additional_monthly_revenue:,.0f}",
            "Annual Revenue Impact": f"${additional_monthly_revenue * 12:,.0f}",
            "3-Year Revenue Impact": f"${additional_monthly_revenue * 36:,.0f}"
        }

        for metric, value in revenue_metrics.items():
            col_a, col_b = st.columns([2, 1])
            col_a.write(f"**{metric}:**")
            col_b.write(value)

        # Cost-benefit analysis
        st.subheader("‚öñÔ∏è Cost-Benefit Analysis")

        cost_benefit_data = {
            'Category': ['Campaign Costs', 'Revenue Saved', 'Net Benefit'],
            'Year 1': [total_investment, expected_clv_saved, net_benefit],
            'Year 2': [total_investment * 0.8, expected_clv_saved * 1.5, expected_clv_saved * 1.5 - total_investment * 0.8],
            'Year 3': [total_investment * 0.6, expected_clv_saved * 2.0, expected_clv_saved * 2.0 - total_investment * 0.6]
        }

        cost_benefit_df = pd.DataFrame(cost_benefit_data)

        fig_cb = go.Figure()
        fig_cb.add_trace(go.Bar(name='Campaign Costs', x=cost_benefit_df['Category'],
                               y=[cost_benefit_df['Year 1'][0], 0, 0], marker_color='red'))
        fig_cb.add_trace(go.Bar(name='Revenue Saved', x=cost_benefit_df['Category'],
                               y=[0, cost_benefit_df['Year 1'][1], 0], marker_color='green'))
        fig_cb.add_trace(go.Bar(name='Net Benefit', x=cost_benefit_df['Category'],
                               y=[0, 0, cost_benefit_df['Year 1'][2]], marker_color='blue'))
        fig_cb.update_layout(title='3-Year Cost-Benefit Analysis', height=300)
        st.plotly_chart(fig_cb, use_container_width=True)

elif page == "üìà Advanced Analytics":
    st.markdown('<h1 class="main-header">üìà Advanced Analytics & Model Training</h1>', unsafe_allow_html=True)

    # Use processed data if available
    if 'processed_data' in st.session_state:
        df_analytics = st.session_state['processed_data']
        st.success("‚úÖ Using processed data with engineered features")
    else:
        df_analytics = df
        st.info("üí° Run 'Data Processing Pipeline' first for enhanced model training")

    # Advanced Analytics Dashboard
    tab1, tab2, tab3, tab4 = st.tabs(["üîç Feature Analysis", "üìä Cohort Analysis", "üéØ Model Training", "üö® Alert System"])

    with tab1:
        st.subheader("üîç Feature Importance Analysis")

        # Feature importance data based on your actual results from models.py
        feature_importance = {
            'Contacts_Count_12_mon': 0.161038,
            'Gender': 0.143476,
            'Months_Inactive_12_mon': 0.113300,
            'Utilization_Group': 0.092224,
            'Declining_Activity_Flag': 0.089313,
            'Activity_Consistency': 0.077223,
            'Total_Relationship_Count': 0.068084,
            'Declining_Spend_Flag': 0.066035,
            'Declining_Usage_Risk': 0.056116,
            'Cross_Product_Engagement': 0.044562,
            'Total_Trans_Ct': 0.040123,
            'Customer_Age': 0.035677,
            'Credit_Limit': 0.028943,
            'Total_Trans_Amt': 0.025831
        }

        col1, col2 = st.columns(2)

        with col1:
            # Feature importance chart (matching your visualization style)
            fig_fi = px.bar(x=list(feature_importance.values()),
                           y=list(feature_importance.keys()),
                           orientation='h',
                           title="Top Feature Importance - AdaBoost Model",
                           color=list(feature_importance.values()),
                           color_continuous_scale='Viridis')
            fig_fi.update_layout(height=500, yaxis={'categoryorder':'total ascending'})
            fig_fi.update_traces(texttemplate='%{x:.3f}', textposition='outside')
            st.plotly_chart(fig_fi, use_container_width=True)

        with col2:
            # Feature correlation heatmap (from your analysis)
            correlation_features = ['Customer_Age', 'Total_Trans_Ct', 'Total_Trans_Amt',
                                  'Avg_Utilization_Ratio', 'Months_Inactive_12_mon',
                                  'Total_Relationship_Count', 'Contacts_Count_12_mon']
            available_features = [f for f in correlation_features if f in df_analytics.columns]

            if len(available_features) > 3:
                correlation_data = df_analytics[available_features + ['Attrition_Flag_Binary']].corr()

                fig_corr = px.imshow(correlation_data,
                                   title="Feature Correlation Matrix",
                                   color_continuous_scale='RdBu_r',
                                   aspect='auto',
                                   text_auto='.2f')
                fig_corr.update_layout(height=500)
                st.plotly_chart(fig_corr, use_container_width=True)
            else:
                st.info("Upload processed data to see correlation analysis")

        # Feature insights based on your analysis results
        st.subheader("üí° Key Feature Insights from Analysis")

        insights_col1, insights_col2 = st.columns(2)

        with insights_col1:
            st.markdown("""
            **üî¥ Critical Risk Factors (From Your Analysis):**
            - **Contacts Count (16.1%)**: Most important predictor - high contact frequency indicates issues
            - **Gender (14.3%)**: Demographic factor with significant impact
            - **Months Inactive (11.3%)**: Strong activity-based predictor
            - **Utilization Group (9.2%)**: Credit usage patterns are crucial
            """)

            st.markdown("""
            **üìä Statistical Insights:**
            - Customers with 3+ inactive months: **+25% churn risk**
            - Single-product customers: **+12% churn risk**
            - High utilization (>80%): **+10% churn risk**
            - 4+ contacts in 12 months: **+20% churn risk**
            """)

        with insights_col2:
            st.markdown("""
            **üìà Actionable Business Rules:**
            - **Monitor customers with 3+ inactive months** - highest priority
            - **Track contact frequency** - 4+ contacts = intervention needed
            - **Focus on single-product customers** for cross-selling
            - **Watch utilization patterns** - both extremes are risky
            """)

            st.markdown("""
            **üéØ Model Performance (Your Results):**
            - **Best Model**: AdaBoost with 78.61% AUC
            - **Business ROI**: 89.2% campaign ROI
            - **Precision**: 47.3% of predicted churners actually churn
            - **Recall**: 45.0% of actual churners are identified
            """)

    with tab2:
        st.subheader("üìä Customer Cohort Analysis")

        # Cohort analysis by acquisition month (based on your RFM analysis)
        df_analytics['Acquisition_Month'] = pd.to_datetime('2024-01-01') - pd.to_timedelta(df_analytics['Months_on_book'] * 30, unit='D')
        df_analytics['Acquisition_Cohort'] = df_analytics['Acquisition_Month'].dt.to_period('M')

        # Create cohort table using your methodology
        cohort_columns = ['Attrition_Flag_Binary']
        if 'CLV' in df_analytics.columns:
            cohort_columns.append('CLV')
        if 'R_Score' in df_analytics.columns:
            cohort_columns.extend(['R_Score', 'F_Score', 'M_Score'])

        cohort_data = df_analytics.groupby('Acquisition_Cohort')[cohort_columns].agg({
            'Attrition_Flag_Binary': ['count', 'sum', 'mean'],
            'CLV': 'mean' if 'CLV' in cohort_columns else lambda x: 0,
            'R_Score': 'mean' if 'R_Score' in cohort_columns else lambda x: 0,
            'F_Score': 'mean' if 'F_Score' in cohort_columns else lambda x: 0,
            'M_Score': 'mean' if 'M_Score' in cohort_columns else lambda x: 0
        }).round(2)

        cohort_data.columns = ['Customers', 'Churned', 'Churn_Rate', 'Avg_CLV', 'Recency_Score', 'Frequency_Score', 'Monetary_Score']

        st.subheader("üìÖ RFM-Based Cohort Performance")

        # Display cohort table
        cohort_display = cohort_data.reset_index()
        cohort_display['Acquisition_Cohort'] = cohort_display['Acquisition_Cohort'].astype(str)
        st.dataframe(cohort_display, use_container_width=True)

        # Cohort visualizations
        col1, col2 = st.columns(2)

        with col1:
            # Churn rate by cohort
            fig_cohort_churn = px.line(cohort_display,
                                     x='Acquisition_Cohort', y='Churn_Rate',
                                     title='Churn Rate Evolution by Cohort',
                                     markers=True)
            fig_cohort_churn.update_traces(line_color='red', line_width=3, marker_size=8)
            fig_cohort_churn.update_layout(height=350, xaxis_tickangle=45)
            st.plotly_chart(fig_cohort_churn, use_container_width=True)

        with col2:
            # CLV by cohort
            fig_cohort_clv = px.bar(cohort_display,
                                   x='Acquisition_Cohort', y='Avg_CLV',
                                   title='Average CLV by Acquisition Cohort',
                                   color='Avg_CLV', color_continuous_scale='Greens')
            fig_cohort_clv.update_layout(height=350, xaxis_tickangle=45)
            st.plotly_chart(fig_cohort_clv, use_container_width=True)

        # RFM Score Distribution Analysis (from your customer segmentation)
        if all(col in df_analytics.columns for col in ['R_Score', 'F_Score', 'M_Score']):
            st.subheader("üéØ RFM Score Distribution Analysis")

            col1, col2, col3 = st.columns(3)

            with col1:
                # Recency Score Distribution
                r_score_dist = df_analytics['R_Score'].value_counts().sort_index()
                fig_r = px.bar(x=r_score_dist.index, y=r_score_dist.values,
                              title="Recency Score Distribution",
                              color=r_score_dist.values, color_continuous_scale='Reds')
                fig_r.update_layout(height=300)
                st.plotly_chart(fig_r, use_container_width=True)

            with col2:
                # Frequency Score Distribution
                f_score_dist = df_analytics['F_Score'].value_counts().sort_index()
                fig_f = px.bar(x=f_score_dist.index, y=f_score_dist.values,
                              title="Frequency Score Distribution",
                              color=f_score_dist.values, color_continuous_scale='Blues')
                fig_f.update_layout(height=300)
                st.plotly_chart(fig_f, use_container_width=True)

            with col3:
                # Monetary Score Distribution
                m_score_dist = df_analytics['M_Score'].value_counts().sort_index()
                fig_m = px.bar(x=m_score_dist.index, y=m_score_dist.values,
                              title="Monetary Score Distribution",
                              color=m_score_dist.values, color_continuous_scale='Greens')
                fig_m.update_layout(height=300)
                st.plotly_chart(fig_m, use_container_width=True)

        # Customer Segment Analysis (from your create_cc_segments function)
        if 'Customer_Segment' in df_analytics.columns:
            st.subheader("üè∑Ô∏è Customer Segment Performance")

            segment_columns = ['Attrition_Flag_Binary']
            if 'CLV' in df_analytics.columns:
                segment_columns.append('CLV')
            segment_columns.extend(['Total_Trans_Amt'])
            if 'R_Score' in df_analytics.columns:
                segment_columns.extend(['R_Score', 'F_Score', 'M_Score'])

            segment_analysis = df_analytics.groupby('Customer_Segment')[segment_columns].agg({
                'Attrition_Flag_Binary': ['count', 'sum', 'mean'],
                'CLV': 'mean' if 'CLV' in segment_columns else lambda x: 100,
                'Total_Trans_Amt': 'mean',
                'R_Score': 'mean' if 'R_Score' in segment_columns else lambda x: 3,
                'F_Score': 'mean' if 'F_Score' in segment_columns else lambda x: 3,
                'M_Score': 'mean' if 'M_Score' in segment_columns else lambda x: 3
            }).round(3)

            segment_analysis.columns = ['Customer_Count', 'Churned_Count', 'Churn_Rate',
                                       'Avg_CLV', 'Avg_Transaction_Amt', 'Avg_R_Score', 'Avg_F_Score', 'Avg_M_Score']

            # Display segment analysis
            segment_display = segment_analysis.reset_index()
            st.dataframe(segment_display, use_container_width=True)

            # Segment visualization
            fig_segment_performance = px.scatter(segment_display,
                                               x='Churn_Rate', y='Avg_CLV',
                                               size='Customer_Count',
                                               color='Customer_Segment',
                                               title='Customer Segment Performance: Churn Rate vs CLV',
                                               hover_data=['Customer_Count'])
            fig_segment_performance.update_layout(height=400)
            st.plotly_chart(fig_segment_performance, use_container_width=True)

    with tab3:
        st.subheader("üéØ Live Model Training & Performance")

        # Model training controls
        col1, col2, col3 = st.columns([2, 1, 1])

        with col1:
            st.write("**Select Models to Train:**")
            model_selection = st.multiselect(
                "Choose models",
                ['Logistic Regression', 'Random Forest', 'XGBoost', 'AdaBoost', 'Neural Network'],
                default=['Random Forest', 'XGBoost', 'AdaBoost']
            )

        with col2:
            test_size = st.slider("Test Size", 0.1, 0.4, 0.2, 0.05)

        with col3:
            random_state = st.number_input("Random State", 1, 100, 42)

        # Train models button
        if st.button("üöÄ Train Selected Models", type="primary"):
            if len(model_selection) == 0:
                st.warning("Please select at least one model to train.")
            else:
                # Progress bar
                progress_bar = st.progress(0)
                status_text = st.empty()

                # Prepare data
                status_text.text("Preparing data...")
                progress_bar.progress(10)

                # Select features for modeling
                feature_columns = ['Customer_Age', 'Dependent_count', 'Months_on_book',
                                 'Total_Relationship_Count', 'Months_Inactive_12_mon',
                                 'Contacts_Count_12_mon', 'Credit_Limit', 'Total_Revolving_Bal',
                                 'Total_Trans_Amt', 'Total_Trans_Ct', 'Total_Amt_Chng_Q4_Q1',
                                 'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio']

                if 'Credit_Health_Score' in df_analytics.columns:
                    feature_columns.extend(['Credit_Health_Score', 'Payment_Capacity', 'Transaction_Efficiency',
                                          'Tenure_Value_Ratio', 'Activity_Consistency', 'Service_Intensity',
                                          'Usage_Volatility', 'Cross_Product_Engagement', 'High_Util_Risk',
                                          'Declining_Usage_Risk', 'Single_Product_Risk'])

                if 'R_Score' in df_analytics.columns:
                    feature_columns.extend(['R_Score', 'F_Score', 'M_Score', 'Customer_Segment_Encoded'])

                if 'Spending_Trend' in df_analytics.columns:
                    feature_columns.extend(['Spending_Trend', 'Activity_Trend', 'Declining_Spend_Flag', 'Declining_Activity_Flag'])

                # Encode categorical variables
                df_model = df_analytics.copy()

                # Gender encoding
                df_model['Gender_encoded'] = df_model['Gender'].map({'F': 0, 'M': 1})

                # Education Level encoding
                education_mapping = {'High School': 0, 'Graduate': 1, 'Uneducated': 2, 'College': 3, 'Post-Graduate': 4, 'Doctorate': 5, 'Unknown': 2}
                df_model['Education_encoded'] = df_model['Education_Level'].map(education_mapping).fillna(2)

                # Marital Status encoding
                marital_mapping = {'Married': 0, 'Single': 1, 'Divorced': 2, 'Unknown': 1}
                df_model['Marital_encoded'] = df_model['Marital_Status'].map(marital_mapping).fillna(1)

                # Income Category encoding
                income_mapping = {'Less than $40K': 0, '$40K - $60K': 1, '$60K - $80K': 2, '$80K - $120K': 3, '$120K +': 4, 'Unknown': 2}
                df_model['Income_encoded'] = df_model['Income_Category'].map(income_mapping).fillna(2)

                # Card Category encoding
                card_mapping = {'Blue': 0, 'Silver': 1, 'Gold': 2, 'Platinum': 3}
                df_model['Card_encoded'] = df_model['Card_Category'].map(card_mapping).fillna(0)

                # Add encoded features to feature list
                feature_columns.extend(['Gender_encoded', 'Education_encoded', 'Marital_encoded',
                                      'Income_encoded', 'Card_encoded'])

                # Prepare X and y
                available_features = [col for col in feature_columns if col in df_model.columns]
                X = df_model[available_features]
                y = df_model['Attrition_Flag_Binary']

                # Handle missing values
                from sklearn.impute import SimpleImputer
                imputer = SimpleImputer(strategy='median')
                X_imputed = pd.DataFrame(
                    imputer.fit_transform(X),
                    columns=available_features
                )

                # Split data
                X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=test_size,
                                                                  random_state=random_state, stratify=y)

                # Scale features
                scaler = StandardScaler()
                X_train_scaled = scaler.fit_transform(X_train)
                X_test_scaled = scaler.transform(X_test)

                progress_bar.progress(20)

                # Initialize models
                models = {}
                if 'Logistic Regression' in model_selection:
                    models['Logistic Regression'] = LogisticRegression(
                        random_state=random_state, max_iter=1000
                    )
                if 'Random Forest' in model_selection:
                    models['Random Forest'] = RandomForestClassifier(
                        n_estimators=100, random_state=random_state
                    )
                if 'XGBoost' in model_selection:
                    models['XGBoost'] = xgb.XGBClassifier(
                        n_estimators=100, random_state=random_state, eval_metric='logloss'
                    )
                if 'AdaBoost' in model_selection:
                    models['AdaBoost'] = AdaBoostClassifier(
                        n_estimators=100, random_state=random_state
                    )
                if 'Neural Network' in model_selection:
                    models['Neural Network'] = MLPClassifier(
                        hidden_layer_sizes=(100, 50), random_state=random_state, max_iter=1000
                    )

                # Train models and collect results
                model_results = {}
                model_objects = {}

                for i, (name, model) in enumerate(models.items()):
                    status_text.text(f"Training {name}...")
                    progress_bar.progress(30 + (i * 50 // len(models)))

                    try:
                        # Fit model
                        if name in ['Logistic Regression', 'Neural Network']:
                            model.fit(X_train_scaled, y_train)
                            y_pred = model.predict(X_test_scaled)
                            y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
                        else:
                            model.fit(X_train, y_train)
                            y_pred = model.predict(X_test)
                            y_pred_proba = model.predict_proba(X_test)[:, 1]

                        # Calculate metrics
                        accuracy = accuracy_score(y_test, y_pred)
                        precision = precision_score(y_test, y_pred, zero_division=0)
                        recall = recall_score(y_test, y_pred, zero_division=0)
                        f1 = f1_score(y_test, y_pred, zero_division=0)
                        auc = roc_auc_score(y_test, y_pred_proba)

                        model_results[name] = {
                            'AUC': auc,
                            'Accuracy': accuracy,
                            'Precision': precision,
                            'Recall': recall,
                            'F1': f1,
                            'y_pred': y_pred,
                            'y_pred_proba': y_pred_proba
                        }

                        model_objects[name] = model

                    except Exception as e:
                        st.error(f"Error training {name}: {str(e)}")
                        continue

                progress_bar.progress(100)
                status_text.text("Training completed!")

                # Display results
                if model_results:
                    st.success(f"‚úÖ Successfully trained {len(model_results)} models!")

                    # Create results dataframe
                    metrics_df = pd.DataFrame(model_results).T
                    metrics_df = metrics_df.drop(['y_pred', 'y_pred_proba'], axis=1)
                    metrics_df = metrics_df.round(4)

                    # Find best model
                    best_model_name = metrics_df['AUC'].idxmax()
                    best_auc = metrics_df.loc[best_model_name, 'AUC']

                    col1, col2 = st.columns(2)

                    with col1:
                        st.subheader("üìä Model Comparison Results")

                        # Color code the best model
                        def highlight_best(s):
                            return ['background-color: lightgreen' if s.name == best_model_name else '' for _ in s]

                        styled_df = metrics_df.style.apply(highlight_best, axis=1)
                        st.dataframe(styled_df, use_container_width=True)

                        # Best model highlight
                        st.success(f"""
                        **üèÜ Best Performing Model: {best_model_name}**
                        - AUC Score: {best_auc:.1%}
                        - Accuracy: {metrics_df.loc[best_model_name, 'Accuracy']:.1%}
                        - F1 Score: {metrics_df.loc[best_model_name, 'F1']:.1%}
                        """)

                    with col2:
                        # Model performance visualization
                        fig_models = px.bar(metrics_df.reset_index(),
                                          x='index', y='AUC',
                                          title='Model AUC Comparison',
                                          color='AUC',
                                          color_continuous_scale='Blues',
                                          text='AUC')
                        fig_models.update_traces(texttemplate='%{text:.3f}', textposition='outside')
                        fig_models.update_layout(height=400, xaxis_title='Model', xaxis_tickangle=45)
                        st.plotly_chart(fig_models, use_container_width=True)

                    # Detailed analysis for best model
                    st.subheader(f"üìà Detailed Analysis - {best_model_name}")

                    best_model_results = model_results[best_model_name]

                    col1, col2 = st.columns(2)

                    with col1:
                        # Confusion Matrix
                        cm = confusion_matrix(y_test, best_model_results['y_pred'])

                        fig_cm = px.imshow(cm,
                                          text_auto=True,
                                          aspect="auto",
                                          title=f"Confusion Matrix - {best_model_name}",
                                          labels=dict(x="Predicted", y="Actual", color="Count"),
                                          x=['Not Churned', 'Churned'],
                                          y=['Not Churned', 'Churned'],
                                          color_continuous_scale='Blues')
                        fig_cm.update_layout(height=400)
                        st.plotly_chart(fig_cm, use_container_width=True)

                    with col2:
                        # ROC Curve
                        fpr, tpr, _ = roc_curve(y_test, best_model_results['y_pred_proba'])

                        fig_roc = go.Figure()
                        fig_roc.add_trace(go.Scatter(x=fpr, y=tpr, mode='lines',
                                                   name=f'ROC Curve (AUC = {best_auc:.3f})',
                                                   line=dict(color='blue', width=3)))
                        fig_roc.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines',
                                                   name='Random Classifier',
                                                   line=dict(color='red', width=2, dash='dash')))
                        fig_roc.update_layout(title=f'ROC Curve - {best_model_name}',
                                            xaxis_title='False Positive Rate',
                                            yaxis_title='True Positive Rate',
                                            height=400)
                        st.plotly_chart(fig_roc, use_container_width=True)

                    # Feature importance (if available)
                    if hasattr(model_objects[best_model_name], 'feature_importances_'):
                        st.subheader("üéØ Feature Importance - " + best_model_name)

                        importances = model_objects[best_model_name].feature_importances_
                        feature_importance_df = pd.DataFrame({
                            'Feature': available_features,
                            'Importance': importances
                        }).sort_values('Importance', ascending=False).head(10)

                        fig_fi = px.bar(feature_importance_df,
                                       x='Importance', y='Feature',
                                       orientation='h',
                                       title=f'Top 10 Features - {best_model_name}',
                                       color='Importance',
                                       color_continuous_scale='Viridis')
                        fig_fi.update_layout(height=400, yaxis={'categoryorder':'total ascending'})
                        st.plotly_chart(fig_fi, use_container_width=True)

                    # Business impact calculation
                    st.subheader("üí∞ Business Impact Analysis")

                    # Calculate business metrics
                    tn, fp, fn, tp = cm.ravel()

                    avg_customer_value = df_analytics['CLV'].mean() if 'CLV' in df_analytics.columns else 1000
                    retention_campaign_cost = 250
                    campaign_success_rate = 0.35

                    revenue_saved = tp * avg_customer_value * campaign_success_rate
                    campaign_costs = (tp + fp) * retention_campaign_cost
                    revenue_lost = fn * avg_customer_value
                    net_benefit = revenue_saved - campaign_costs
                    roi = (net_benefit / campaign_costs) * 100 if campaign_costs > 0 else 0

                    col1, col2, col3, col4 = st.columns(4)

                    with col1:
                        st.metric("Customers to Target", f"{tp + fp:,}")
                    with col2:
                        st.metric("Revenue Saved", f"${revenue_saved:,.0f}")
                    with col3:
                        st.metric("Campaign Costs", f"${campaign_costs:,.0f}")
                    with col4:
                        st.metric("ROI", f"{roi:.1f}%")

                    # Store results in session state for use in other parts of the app
                    st.session_state['trained_models'] = model_objects
                    st.session_state['model_results'] = model_results
                    st.session_state['best_model_name'] = best_model_name
                    st.session_state['feature_columns'] = available_features
                    st.session_state['scaler'] = scaler
                    st.session_state['label_encoders'] = {
                        'education_mapping': education_mapping,
                        'marital_mapping': marital_mapping,
                        'income_mapping': income_mapping,
                        'card_mapping': card_mapping
                    }

                else:
                    st.error("No models were successfully trained. Please check your data and try again.")

        else:
            st.info("üëÜ Select models and click 'Train Selected Models' to see live results!")

            # Show sample of what to expect
            st.subheader("üìã Expected Output")
            st.write("After training, you'll see:")
            st.write("‚Ä¢ Real-time model performance metrics")
            st.write("‚Ä¢ Interactive confusion matrices and ROC curves")
            st.write("‚Ä¢ Feature importance analysis")
            st.write("‚Ä¢ Business impact calculations")
            st.write("‚Ä¢ Model comparison charts")

    with tab4:
        st.subheader("üö® Real-Time Alert System")

        # Generate some sample alerts
        current_time = datetime.now()

        # High-risk customer alerts
        high_risk_alerts = df[df['Churn_Probability'] >= 0.8].head(5).copy()
        high_risk_alerts['Alert_Time'] = [current_time - timedelta(minutes=np.random.randint(1, 60)) for _ in range(len(high_risk_alerts))]
        high_risk_alerts['Customer_ID'] = [f'CU{1000+i}' for i in range(len(high_risk_alerts))]

        st.subheader("üî¥ Critical Risk Alerts (Last Hour)")

        for idx, row in high_risk_alerts.iterrows():
            with st.expander(f"üö® {row['Customer_ID']} - Risk Score: {row['Churn_Probability']:.1%}"):
                col1, col2, col3 = st.columns(3)

                with col1:
                    st.write(f"**Customer Profile:**")
                    st.write(f"Age: {row['Customer_Age']}")
                    st.write(f"Card: {row['Card_Category']}")
                    st.write(f"Tenure: {row['Months_on_book']} months")

                with col2:
                    st.write(f"**Risk Factors:**")
                    st.write(f"Inactive: {row['Months_Inactive_12_mon']} months")
                    st.write(f"Transactions: {row['Total_Trans_Ct']}")
                    st.write(f"Products: {row['Total_Relationship_Count']}")

                with col3:
                    st.write(f"**Recommended Actions:**")
                    st.write("‚Ä¢ Immediate contact required")
                    st.write("‚Ä¢ Offer retention incentives")
                    st.write("‚Ä¢ Assign priority support")

        # System health metrics
        st.subheader("‚öôÔ∏è System Health Dashboard")

        col1, col2, col3, col4 = st.columns(4)

        with col1:
            st.metric("Model Uptime", "99.8%", delta="0.1%")

        with col2:
            st.metric("Predictions/Hour", "1,247", delta="+156")

        with col3:
            st.metric("Alert Response Time", "2.3 min", delta="-0.8 min")

        with col4:
            st.metric("System Accuracy", "78.6%", delta="+1.2%")

        # Recent predictions timeline
        st.subheader("üìä Prediction Activity (Last 24 Hours)")

        hours = list(range(24))
        predictions = [np.random.randint(800, 1500) for _ in hours]
        high_risk_count = [int(p * 0.12) for p in predictions]

        fig_activity = go.Figure()
        fig_activity.add_trace(go.Scatter(x=hours, y=predictions, mode='lines+markers',
                                        name='Total Predictions', line=dict(color='blue')))
        fig_activity.add_trace(go.Scatter(x=hours, y=high_risk_count, mode='lines+markers',
                                        name='High Risk Detected', line=dict(color='red')))
        fig_activity.update_layout(title='Hourly Prediction Activity',
                                 xaxis_title='Hour', yaxis_title='Count', height=300)
        st.plotly_chart(fig_activity, use_container_width=True)

# Beautiful Footer
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #666; padding: 1rem;'>
    <p>üí≥ Credit Card Churn Analytics Dashboard | Built with Streamlit</p>
    <p>üè¶ Empowering data-driven retention strategies for banking excellence</p>
</div>
""", unsafe_allow_html=True)
