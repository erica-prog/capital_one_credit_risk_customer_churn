{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPG+BU5exLkI6F38tpxLDAF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nX9S7Tfujhet","executionInfo":{"status":"ok","timestamp":1754522749488,"user_tz":240,"elapsed":17413,"user":{"displayName":"Oghenemaro Atoma","userId":"15325721832201980316"}},"outputId":"4282f1be-ae8b-4c16-99e9-80d29dbd8aa3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting streamlit\n","  Downloading streamlit-1.48.0-py3-none-any.whl.metadata (9.5 kB)\n","Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n","Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n","Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n","Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n","Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n","Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (25.0)\n","Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n","Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.3.0)\n","Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n","Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n","Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n","Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n","Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n","Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n","Collecting watchdog<7,>=2.1.5 (from streamlit)\n","  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.45)\n","Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n","  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n","Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.0)\n","Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.0.1)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.7.14)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.4.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.26.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n","Downloading streamlit-1.48.0-py3-none-any.whl (9.9 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n","Successfully installed pydeck-0.9.1 streamlit-1.48.0 watchdog-6.0.0\n","Collecting streamlit-option-menu\n","  Downloading streamlit_option_menu-0.4.0-py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: streamlit>=1.36 in /usr/local/lib/python3.11/dist-packages (from streamlit-option-menu) (1.48.0)\n","Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.36->streamlit-option-menu) (5.5.0)\n","Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.36->streamlit-option-menu) (1.9.0)\n","Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.36->streamlit-option-menu) (5.5.2)\n","Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.36->streamlit-option-menu) (8.2.1)\n","Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.36->streamlit-option-menu) (2.0.2)\n","Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.36->streamlit-option-menu) (25.0)\n","Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.36->streamlit-option-menu) (2.2.2)\n","Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.36->streamlit-option-menu) (11.3.0)\n","Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.36->streamlit-option-menu) (5.29.5)\n","Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.36->streamlit-option-menu) (18.1.0)\n","Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.36->streamlit-option-menu) (2.32.3)\n","Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.36->streamlit-option-menu) (8.5.0)\n","Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.36->streamlit-option-menu) (0.10.2)\n","Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.36->streamlit-option-menu) (4.14.1)\n","Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.36->streamlit-option-menu) (6.0.0)\n","Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.36->streamlit-option-menu) (3.1.45)\n","Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.36->streamlit-option-menu) (0.9.1)\n","Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.36->streamlit-option-menu) (6.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit>=1.36->streamlit-option-menu) (3.1.6)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit>=1.36->streamlit-option-menu) (4.25.0)\n","Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit>=1.36->streamlit-option-menu) (2.0.1)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.36->streamlit-option-menu) (4.0.12)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit>=1.36->streamlit-option-menu) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit>=1.36->streamlit-option-menu) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit>=1.36->streamlit-option-menu) (2025.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit>=1.36->streamlit-option-menu) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit>=1.36->streamlit-option-menu) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit>=1.36->streamlit-option-menu) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit>=1.36->streamlit-option-menu) (2025.7.14)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.36->streamlit-option-menu) (5.0.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit>=1.36->streamlit-option-menu) (3.0.2)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit>=1.36->streamlit-option-menu) (25.3.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit>=1.36->streamlit-option-menu) (2025.4.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit>=1.36->streamlit-option-menu) (0.36.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit>=1.36->streamlit-option-menu) (0.26.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit>=1.36->streamlit-option-menu) (1.17.0)\n","Downloading streamlit_option_menu-0.4.0-py3-none-any.whl (829 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m829.3/829.3 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: streamlit-option-menu\n","Successfully installed streamlit-option-menu-0.4.0\n","Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (8.5.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly) (25.0)\n"]},{"output_type":"stream","name":"stderr","text":["2025-08-06 23:25:48.295 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"]}],"source":["!pip install streamlit\n","!pip install streamlit-option-menu\n","!pip install plotly\n","import streamlit as st\n","from streamlit_option_menu import option_menu\n","import pandas as pd\n","import numpy as np\n","import plotly.express as px\n","import plotly.graph_objects as go\n","from plotly.subplots import make_subplots\n","import pickle\n","from datetime import datetime, timedelta\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","source":["# Feature Selection\n","from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import IterativeImputer\n","from sklearn.linear_model import BayesianRidge\n","from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n","\n","# ML models libraries\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n","from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold\n","from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score,\n","                            roc_curve, precision_recall_curve, accuracy_score,\n","                            precision_score, recall_score, f1_score)\n","\n","# Import all required models\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.neural_network import MLPClassifier\n","import xgboost as xgb"],"metadata":{"id":"6U_um0HWjm1u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["st.set_page_config(\n","    page_title=\"Capital One Credit Risk Management - Credit Card Churn Analytics\",\n","    page_icon=\"üí≥\",\n","    layout=\"wide\",\n","    initial_sidebar_state=\"expanded\"\n",")"],"metadata":{"id":"4AqVQ36sjpPs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Custom CSS for better styling\n","st.markdown(\"\"\"\n","<style>\n","    .main-header {\n","        font-size: 2.5rem;\n","        font-weight: bold;\n","        color: #1f77b4;\n","        text-align: center;\n","        margin-bottom: 2rem;\n","    }\n","    .metric-card {\n","        background-color: #f0f2f6;\n","        padding: 1rem;\n","        border-radius: 0.5rem;\n","        text-align: center;\n","        margin: 0.5rem;\n","    }\n","    .insight-box {\n","        background-color: #e8f4fd;\n","        padding: 1rem;\n","        border-left: 4px solid #1f77b4;\n","        margin: 1rem 0;\n","    }\n","    .risk-high { color: #d32f2f; font-weight: bold; }\n","    .risk-medium { color: #f57c00; font-weight: bold; }\n","    .risk-low { color: #388e3c; font-weight: bold; }\n","</style>\n","\"\"\", unsafe_allow_html=True)"],"metadata":{"id":"6me3_c9VjrS-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the actual CSV data\n","@st.cache_data\n","def load_credit_card_data():\n","    \"\"\"Load the actual credit card dataset and perform initial cleaning\"\"\"\n","    try:\n","        # Read the CSV file uploaded by user\n","        df = pd.read_csv('credit_card_churn_100k.csv')\n","\n","        # Drop CLIENTNUM as it's just an ID\n","        if 'CLIENTNUM' in df.columns:\n","            df = df.drop('CLIENTNUM', axis=1)\n","\n","        # Convert date column\n","        if 'Last_Transaction_Date' in df.columns:\n","            df['Last_Transaction_Date'] = pd.to_datetime(df['Last_Transaction_Date'])\n","\n","        # Convert Attrition_Flag to binary\n","        if 'Attrition_Flag' in df.columns:\n","            df['Attrition_Flag_Binary'] = (df['Attrition_Flag'] == 'Attrited Customer').astype(int)\n","\n","        st.success(f\"‚úÖ Successfully loaded {len(df):,} customer records from CSV file\")\n","\n","        return df\n","\n","    except FileNotFoundError:\n","        st.error(\"CSV file not found. Please ensure 'credit_card_churn_100k.csv' is in the same directory.\")\n","        # Return sample data as fallback\n","        return load_sample_data()\n","    except Exception as e:\n","        st.error(f\"Error loading CSV file: {str(e)}\")\n","        return load_sample_data()"],"metadata":{"id":"kMaUoHk_juRU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# File upload widget for CSV\n","def load_data_with_upload():\n","    \"\"\"Allow users to upload their CSV file\"\"\"\n","    st.sidebar.markdown(\"### Data Source\")\n","\n","    uploaded_file = st.sidebar.file_uploader(\n","        \"Upload Credit Card Data CSV\",\n","        type=['csv'],\n","        help=\"Upload your credit_card_churn_100k.csv file\"\n","    )\n","\n","    if uploaded_file is not None:\n","        try:\n","            df = pd.read_csv(uploaded_file)\n","\n","            # Data validation\n","            required_columns = ['Customer_Age', 'Gender', 'Total_Trans_Amt', 'Total_Trans_Ct',\n","                              'Attrition_Flag', 'Credit_Limit', 'Avg_Utilization_Ratio']\n","\n","            missing_cols = [col for col in required_columns if col not in df.columns]\n","            if missing_cols:\n","                st.error(f\"Missing required columns: {missing_cols}\")\n","                return None\n","\n","            # Drop CLIENTNUM if exists\n","            if 'CLIENTNUM' in df.columns:\n","                df = df.drop('CLIENTNUM', axis=1)\n","\n","            # Convert date column\n","            if 'Last_Transaction_Date' in df.columns:\n","                df['Last_Transaction_Date'] = pd.to_datetime(df['Last_Transaction_Date'])\n","\n","            # Convert Attrition_Flag to binary\n","            df['Attrition_Flag_Binary'] = (df['Attrition_Flag'] == 'Attrited Customer').astype(int)\n","\n","            st.sidebar.success(f\"‚úÖ Loaded {len(df):,} records\")\n","            return df\n","\n","        except Exception as e:\n","            st.sidebar.error(f\"Error loading file: {str(e)}\")\n","            return None\n","    else:\n","        st.sidebar.info(\"üëÜ Upload CSV file to use real data\")\n","        return None"],"metadata":{"id":"kYmLFROsl6e9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Live Data Cleaning and Monitoring\n","class LiveDataProcessor:\n","    \"\"\"Real-time data processing and monitoring class\"\"\"\n","\n","    def __init__(self):\n","        self.processing_steps = []\n","        self.data_quality_metrics = {}\n","\n","    def assess_data_quality(self, df):\n","        \"\"\"Live data quality assessment\"\"\"\n","\n","        st.subheader(\"üîç Live Data Quality Assessment\")\n","\n","        # Missing values analysis\n","        missing_data = df.isnull().sum()\n","        missing_pct = (missing_data / len(df) * 100).round(2)\n","\n","        col1, col2, col3 = st.columns(3)\n","\n","        with col1:\n","            st.metric(\"Total Records\", f\"{len(df):,}\")\n","            st.metric(\"Total Features\", len(df.columns))\n","\n","        with col2:\n","            st.metric(\"Missing Values\", missing_data.sum())\n","            st.metric(\"Complete Records\", f\"{(df.dropna().shape[0]):,}\")\n","\n","        with col3:\n","            duplicate_count = df.duplicated().sum()\n","            st.metric(\"Duplicate Records\", duplicate_count)\n","            data_quality_score = ((len(df) - missing_data.sum() - duplicate_count) / (len(df) * len(df.columns)) * 100)\n","            st.metric(\"Data Quality Score\", f\"{data_quality_score:.1f}%\")\n","\n","        # Missing values visualization\n","        if missing_data.sum() > 0:\n","            missing_df = pd.DataFrame({\n","                'Column': missing_data.index,\n","                'Missing_Count': missing_data.values,\n","                'Missing_Percentage': missing_pct.values\n","            }).query('Missing_Count > 0').sort_values('Missing_Count', ascending=False)\n","\n","            if not missing_df.empty:\n","                fig_missing = px.bar(missing_df, x='Column', y='Missing_Percentage',\n","                                   title=\"Missing Values by Column (%)\",\n","                                   color='Missing_Percentage', color_continuous_scale='Reds')\n","                fig_missing.update_layout(height=300, xaxis_tickangle=45)\n","                st.plotly_chart(fig_missing, use_container_width=True)\n","\n","        return missing_data, data_quality_score\n","\n","    def clean_data(self, df):\n","        \"\"\"Live data cleaning with monitoring\"\"\"\n","\n","        st.subheader(\"üßπ Live Data Cleaning Pipeline\")\n","\n","        cleaning_progress = st.progress(0)\n","        status_text = st.empty()\n","\n","        # Step 1: Handle missing values\n","        status_text.text(\"Step 1/5: Handling missing values...\")\n","        cleaning_progress.progress(20)\n","\n","        # Create utilization group for missing value context\n","        df['Utilization_Group'] = pd.cut(df['Avg_Utilization_Ratio'],\n","                                       bins=[0, 0.1, 0.3, 0.7, 1.0],\n","                                       labels=['Low (0-10%)', 'Moderate (10-30%)',\n","                                              'High (30-70%)', 'Very High (70%+)'])\n","\n","        # Remove rows with missing utilization group (critical for analysis)\n","        initial_count = len(df)\n","        df = df.dropna(subset=['Utilization_Group'])\n","        removed_count = initial_count - len(df)\n","\n","        if removed_count > 0:\n","            st.warning(f\"Removed {removed_count} records with critical missing values\")\n","\n","        # Step 2: Handle outliers\n","        status_text.text(\"Step 2/5: Detecting outliers...\")\n","        cleaning_progress.progress(40)\n","\n","        # Outlier detection for key financial metrics\n","        outlier_columns = ['Credit_Limit', 'Total_Trans_Amt', 'Total_Revolving_Bal']\n","        outlier_counts = {}\n","\n","        for col in outlier_columns:\n","            if col in df.columns:\n","                Q1 = df[col].quantile(0.25)\n","                Q3 = df[col].quantile(0.75)\n","                IQR = Q3 - Q1\n","                lower_bound = Q1 - 1.5 * IQR\n","                upper_bound = Q3 + 1.5 * IQR\n","\n","                outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n","                outlier_counts[col] = len(outliers)\n","\n","        # Step 3: Data type validation\n","        status_text.text(\"Step 3/5: Validating data types...\")\n","        cleaning_progress.progress(60)\n","\n","        # Ensure numeric columns are properly typed\n","        numeric_columns = ['Customer_Age', 'Credit_Limit', 'Total_Trans_Amt', 'Total_Trans_Ct', 'Avg_Utilization_Ratio']\n","        for col in numeric_columns:\n","            if col in df.columns:\n","                df[col] = pd.to_numeric(df[col], errors='coerce')\n","\n","        # Step 4: Business rule validation\n","        status_text.text(\"Step 4/5: Applying business rules...\")\n","        cleaning_progress.progress(80)\n","\n","        # Business rule validations\n","        validation_results = {}\n","\n","        # Rule 1: Credit utilization should not exceed 100%\n","        if 'Avg_Utilization_Ratio' in df.columns:\n","            high_util = (df['Avg_Utilization_Ratio'] > 1.0).sum()\n","            validation_results['High Utilization (>100%)'] = high_util\n","\n","        # Rule 2: Revolving balance should not exceed credit limit\n","        if 'Total_Revolving_Bal' in df.columns and 'Credit_Limit' in df.columns:\n","            over_limit = (df['Total_Revolving_Bal'] > df['Credit_Limit']).sum()\n","            validation_results['Balance Over Limit'] = over_limit\n","\n","        # Rule 3: Age should be reasonable (18-100)\n","        if 'Customer_Age' in df.columns:\n","            invalid_age = ((df['Customer_Age'] < 18) | (df['Customer_Age'] > 100)).sum()\n","            validation_results['Invalid Age'] = invalid_age\n","\n","        # Step 5: Final validation\n","        status_text.text(\"Step 5/5: Final validation...\")\n","        cleaning_progress.progress(100)\n","\n","        # Summary metrics\n","        final_count = len(df)\n","        data_retention_rate = (final_count / initial_count * 100) if initial_count > 0 else 100\n","\n","        status_text.text(\"‚úÖ Data cleaning completed!\")\n","\n","        # Display cleaning results\n","        col1, col2 = st.columns(2)\n","\n","        with col1:\n","            st.subheader(\"üìä Cleaning Results\")\n","            st.metric(\"Records Retained\", f\"{final_count:,}\")\n","            st.metric(\"Data Retention Rate\", f\"{data_retention_rate:.1f}%\")\n","            st.metric(\"Records Removed\", f\"{removed_count:,}\")\n","\n","        with col2:\n","            st.subheader(\"‚ö†Ô∏è Data Quality Issues\")\n","            for issue, count in validation_results.items():\n","                if count > 0:\n","                    st.warning(f\"{issue}: {count:,} records\")\n","                else:\n","                    st.success(f\"{issue}: ‚úÖ No issues\")\n","\n","        # Outlier summary\n","        if outlier_counts:\n","            st.subheader(\"üìà Outlier Detection Results\")\n","            outlier_df = pd.DataFrame(list(outlier_counts.items()), columns=['Column', 'Outlier_Count'])\n","            if not outlier_df.empty:\n","                fig_outliers = px.bar(outlier_df, x='Column', y='Outlier_Count',\n","                                    title=\"Outliers Detected by Column\",\n","                                    color='Outlier_Count', color_continuous_scale='Oranges')\n","                st.plotly_chart(fig_outliers, use_container_width=True)\n","\n","        return df\n","\n","    def engineer_features_live(self, df):\n","        \"\"\"Live feature engineering with progress monitoring\"\"\"\n","\n","        st.subheader(\"‚öôÔ∏è Live Feature Engineering Pipeline\")\n","\n","        feature_progress = st.progress(0)\n","        status_text = st.empty()\n","\n","        # Step 1: Financial Features\n","        status_text.text(\"Step 1/5: Creating financial health indicators...\")\n","        feature_progress.progress(20)\n","\n","        df = create_financial_features(df)\n","\n","        # Step 2: Behavioral Features\n","        status_text.text(\"Step 2/5: Creating behavioral patterns...\")\n","        feature_progress.progress(40)\n","\n","        df = create_behavioural_features(df)\n","\n","        # Step 3: Risk Features\n","        status_text.text(\"Step 3/5: Creating risk indicators...\")\n","        feature_progress.progress(60)\n","\n","        df = create_risk_features(df)\n","\n","        # Step 4: RFM Analysis\n","        status_text.text(\"Step 4/5: Calculating RFM scores...\")\n","        feature_progress.progress(80)\n","\n","        df = calculate_rfm_scores(df)\n","        df = create_cc_segments(df)\n","\n","        # Step 5: Time-based Features\n","        status_text.text(\"Step 5/5: Creating time-based features...\")\n","        feature_progress.progress(100)\n","\n","        df = time_based_features(df)\n","\n","        status_text.text(\"‚úÖ Feature engineering completed!\")\n","\n","        # Feature engineering summary\n","        new_features = [col for col in df.columns if col not in ['Customer_Age', 'Gender', 'Dependent_count',\n","                       'Education_Level', 'Marital_Status', 'Income_Category', 'Card_Category']]\n","\n","        col1, col2, col3 = st.columns(3)\n","\n","        with col1:\n","            st.metric(\"Original Features\", len(df.columns) - len(new_features))\n","        with col2:\n","            st.metric(\"Engineered Features\", len(new_features))\n","        with col3:\n","            st.metric(\"Total Features\", len(df.columns))\n","\n","        # Feature categories\n","        st.subheader(\"üìã Feature Categories Created\")\n","\n","        feature_categories = {\n","            \"Financial Health\": ['Credit_Health_Score', 'Payment_Capacity', 'Transaction_Efficiency', 'Tenure_Value_Ratio'],\n","            \"Behavioral Patterns\": ['Activity_Consistency', 'Service_Intensity', 'Usage_Volatility', 'Cross_Product_Engagement'],\n","            \"Risk Indicators\": ['High_Util_Risk', 'Declining_Usage_Risk', 'Single_Product_Risk'],\n","            \"RFM Segmentation\": ['R_Score', 'F_Score', 'M_Score', 'Customer_Segment'],\n","            \"Temporal Features\": ['Spending_Trend', 'Activity_Trend', 'Declining_Spend_Flag', 'Declining_Activity_Flag']\n","        }\n","\n","        for category, features in feature_categories.items():\n","            available_features = [f for f in features if f in df.columns]\n","            if available_features:\n","                st.write(f\"**{category}**: {', '.join(available_features)}\")\n","\n","        return df"],"metadata":{"id":"Fm71_W_0l95G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Real-time monitoring dashboard\n","def create_live_monitoring_dashboard(df):\n","    \"\"\"Create live monitoring dashboard for business analysts\"\"\"\n","\n","    st.markdown('<h1 class=\"main-header\">üìä Live Credit Card Churn Monitoring</h1>', unsafe_allow_html=True)\n","\n","    # Real-time metrics\n","    col1, col2, col3, col4, col5, col6 = st.columns(6)\n","\n","    current_churn_rate = df['Attrition_Flag_Binary'].mean()\n","    total_customers = len(df)\n","    high_risk_customers = (df.get('Churn_Probability', df['Attrition_Flag_Binary']) > 0.7).sum()\n","    avg_clv = calculate_clv(df).mean()\n","\n","    with col1:\n","        st.metric(\"Active Customers\", f\"{total_customers:,}\",\n","                 delta=f\"+{int(total_customers*0.02)} this month\")\n","\n","    with col2:\n","        st.metric(\"Current Churn Rate\", f\"{current_churn_rate:.1%}\",\n","                 delta=f\"{(current_churn_rate-0.20):.1%} vs target\", delta_color=\"inverse\")\n","\n","    with col3:\n","        st.metric(\"High Risk Customers\", f\"{high_risk_customers:,}\",\n","                 delta=f\"-{int(high_risk_customers*0.05)} vs last week\", delta_color=\"inverse\")\n","\n","    with col4:\n","        st.metric(\"Avg Customer Value\", f\"${avg_clv:.0f}\",\n","                 delta=f\"+${avg_clv*0.08:.0f}\")\n","\n","    with col5:\n","        revenue_at_risk = high_risk_customers * avg_clv\n","        st.metric(\"Revenue at Risk\", f\"${revenue_at_risk:,.0f}\",\n","                 delta=\"-8.5% vs last month\", delta_color=\"inverse\")\n","\n","    with col6:\n","        st.metric(\"Data Freshness\", \"Real-time\",\n","                 delta=\"Updated now\", delta_color=\"normal\")\n","\n","    # Live alerts section\n","    st.markdown(\"---\")\n","    st.subheader(\"üö® Live Alerts & Monitoring\")\n","\n","    alert_col1, alert_col2 = st.columns(2)\n","\n","    with alert_col1:\n","        # High-risk customer alerts\n","        if 'Churn_Probability' in df.columns:\n","            critical_customers = df[df['Churn_Probability'] > 0.8]\n","        else:\n","            # Use proxy based on risk factors\n","            critical_customers = df[\n","                (df['Months_Inactive_12_mon'] >= 3) |\n","                (df['Total_Trans_Ct'] < 30) |\n","                (df['Contacts_Count_12_mon'] >= 4)\n","            ]\n","\n","        st.metric(\"üî¥ Critical Risk Alerts\", len(critical_customers))\n","\n","        if len(critical_customers) > 0:\n","            st.warning(f\"{len(critical_customers)} customers need immediate attention!\")\n","\n","            # Show sample of critical customers\n","            critical_sample = critical_customers.head(5)[['Customer_Age', 'Card_Category',\n","                                                        'Months_Inactive_12_mon', 'Total_Trans_Ct']].copy()\n","            critical_sample.index = [f\"Customer_{i+1}\" for i in range(len(critical_sample))]\n","            st.dataframe(critical_sample, use_container_width=True)\n","\n","    with alert_col2:\n","        # Trend alerts\n","        declining_usage = df[df['Total_Amt_Chng_Q4_Q1'] < 0.7]\n","        st.metric(\"üìâ Declining Usage Alert\", len(declining_usage))\n","\n","        if len(declining_usage) > 0:\n","            st.warning(f\"{len(declining_usage)} customers showing declining usage patterns!\")\n","\n","        # System health\n","        st.metric(\"‚öôÔ∏è System Health\", \"99.8%\", delta=\"+0.2% uptime\")\n","        st.success(\"All monitoring systems operational\")\n","\n","    return df"],"metadata":{"id":"A6CkL7comCvL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_clv(df):\n","    \"\"\"Calculate Customer Lifetime Value\"\"\"\n","    interchange_rate = 0.025\n","    annual_fee_avg = 95\n","    interest_rate = 0.18\n","    profit_margin = 0.15\n","\n","    # Estimate CLV based on transaction amounts and balances\n","    clv = (df['Total_Trans_Amt'] * interchange_rate +\n","           annual_fee_avg +\n","           df['Total_Revolving_Bal'] * interest_rate) * profit_margin\n","\n","    return clv# Load data using upload or fallback\n","df = load_data_with_upload()\n","\n","if df is None:\n","    st.info(\"üìÅ Using sample data for demonstration. Upload your CSV file for real analysis.\")\n","    df = load_sample_data()\n","\n","    # Add CLV and other derived features for sample data\n","    df['CLV'] = calculate_clv(df)\n","\n","    # Create utilization group\n","    df['Utilization_Group'] = pd.cut(df['Avg_Utilization_Ratio'],\n","                                   bins=[0, 0.1, 0.3, 0.7, 1.0],\n","                                   labels=['Low (0-10%)', 'Moderate (10-30%)',\n","                                          'High (30-70%)', 'Very High (70%+)'])\n","\n","    # Add basic feature engineering for sample data\n","    df = create_financial_features(df)\n","    df = create_behavioural_features(df)\n","    df = create_risk_features(df)\n","    df = calculate_rfm_scores(df)\n","    df = create_cc_segments(df)\n","    df = time_based_features(df)\n","\n","# Initialize live data processor\n","processor = LiveDataProcessor()\n","\n","# Sidebar navigation with live monitoring options\n","st.sidebar.title(\"üè¶ Navigation\")\n","page = st.sidebar.selectbox(\"Select Page\",\n","                           [\"üìä Live Monitoring Dashboard\", \"üßπ Data Processing Pipeline\",\n","                            \"üîÆ Churn Prediction\", \"üí∞ Business Impact\", \"üìà Advanced Analytics\"])\n","\n","# Live monitoring controls\n","st.sidebar.markdown(\"### Live Monitoring Controls\")\n","auto_refresh = st.sidebar.checkbox(\"Auto Refresh Data\", value=False)\n","refresh_interval = st.sidebar.selectbox(\"Refresh Interval\", [\"30 seconds\", \"1 minute\", \"5 minutes\"], index=1)\n","\n","if auto_refresh:\n","    st.sidebar.success(\"üü¢ Live monitoring active\")\n","else:\n","    st.sidebar.info(\"‚ö™ Manual mode\")\n","\n","# Manual refresh button\n","if st.sidebar.button(\"üîÑ Refresh Data Now\"):\n","    st.cache_data.clear()\n","    st.rerun()\n","\n","if page == \"üìä Live Monitoring Dashboard\":\n","    # Create live monitoring dashboard\n","    df = create_live_monitoring_dashboard(df)\n","\n","    # Live churn trends\n","    st.subheader(\"üìà Live Churn Trends & KPIs\")\n","\n","    col1, col2 = st.columns([2, 1])\n","\n","    with col1:\n","        # Churn rate by key segments with live updates\n","        tab1, tab2, tab3 = st.tabs([\"By Demographics\", \"By Activity\", \"By Financial Health\"])\n","\n","        with tab1:\n","            # Age groups churn analysis\n","            df['Age_Group'] = pd.cut(df['Customer_Age'], bins=[0, 30, 40, 50, 60, 100],\n","                                   labels=['<30', '30-40', '40-50', '50-60', '60+'])\n","            age_churn = df.groupby('Age_Group')['Attrition_Flag_Binary'].agg(['count', 'mean']).reset_index()\n","            age_churn.columns = ['Age_Group', 'Customers', 'Churn_Rate']\n","\n","            fig_age = px.bar(age_churn, x='Age_Group', y='Churn_Rate',\n","                           title=\"Live Churn Rate by Age Group\",\n","                           color='Churn_Rate', color_continuous_scale='Reds',\n","                           text='Churn_Rate')\n","            fig_age.update_traces(texttemplate='%{text:.1%}', textposition='outside')\n","            fig_age.update_layout(height=350)\n","            st.plotly_chart(fig_age, use_container_width=True)\n","\n","        with tab2:\n","            # Activity analysis\n","            activity_churn = df.groupby('Months_Inactive_12_mon')['Attrition_Flag_Binary'].mean().reset_index()\n","            activity_churn.columns = ['Months_Inactive', 'Churn_Rate']\n","\n","            fig_activity = px.line(activity_churn, x='Months_Inactive', y='Churn_Rate',\n","                                 title=\"Live Churn Rate by Months Inactive\",\n","                                 markers=True)\n","            fig_activity.update_traces(line_color='red', line_width=3, marker_size=8)\n","            fig_activity.update_layout(height=350)\n","            st.plotly_chart(fig_activity, use_container_width=True)\n","\n","        with tab3:\n","            # Financial health analysis\n","            util_churn = df.groupby('Utilization_Group')['Attrition_Flag_Binary'].mean().reset_index()\n","            util_churn = util_churn.dropna()\n","\n","            fig_util = px.bar(util_churn, x='Utilization_Group', y='Attrition_Flag_Binary',\n","                             title=\"Live Churn Rate by Credit Utilization\",\n","                             color='Attrition_Flag_Binary', color_continuous_scale='Oranges',\n","                             text='Attrition_Flag_Binary')\n","            fig_util.update_traces(texttemplate='%{text:.1%}', textposition='outside')\n","            fig_util.update_layout(height=350, xaxis_tickangle=45)\n","            st.plotly_chart(fig_util, use_container_width=True)\n","\n","    with col2:\n","        # Live risk distribution\n","        st.subheader(\"üéØ Live Risk Distribution\")\n","\n","        # Create risk categories based on multiple factors\n","        df['Risk_Score'] = (\n","            (df['Months_Inactive_12_mon'] >= 3).astype(int) * 25 +\n","            (df['Total_Trans_Ct'] < 30).astype(int) * 20 +\n","            (df['Contacts_Count_12_mon'] >= 4).astype(int) * 20 +\n","            (df['Avg_Utilization_Ratio'] > 0.8).astype(int) * 15 +\n","            (df['Total_Relationship_Count'] == 1).astype(int) * 10\n","        )\n","\n","        df['Risk_Category'] = pd.cut(df['Risk_Score'],\n","                                   bins=[0, 20, 40, 60, 100],\n","                                   labels=['Low Risk', 'Medium Risk', 'High Risk', 'Critical Risk'])\n","\n","        risk_counts = df['Risk_Category'].value_counts()\n","\n","        fig_risk = go.Figure(data=[\n","            go.Pie(labels=risk_counts.index,\n","                  values=risk_counts.values,\n","                  hole=0.6,\n","                  marker_colors=['green', 'yellow', 'orange', 'red'],\n","                  textinfo='label+percent')\n","        ])\n","        fig_risk.update_layout(title_text=\"Live Risk Distribution\", height=350)\n","        st.plotly_chart(fig_risk, use_container_width=True)\n","\n","        # Risk summary metrics\n","        st.subheader(\"‚ö° Real-time Alerts\")\n","\n","        critical_count = (df['Risk_Category'] == 'Critical Risk').sum()\n","        high_count = (df['Risk_Category'] == 'High Risk').sum()\n","\n","        if critical_count > 0:\n","            st.error(f\"üö® {critical_count} customers in CRITICAL risk!\")\n","\n","        if high_count > 0:\n","            st.warning(f\"‚ö†Ô∏è {high_count} customers in HIGH risk!\")\n","\n","        if critical_count == 0 and high_count == 0:\n","            st.success(\"‚úÖ No critical alerts at this time\")\n","\n","elif page == \"üßπ Data Processing Pipeline\":\n","    st.markdown('<h1 class=\"main-header\">üßπ Live Data Processing Pipeline</h1>', unsafe_allow_html=True)\n","\n","    # Data quality assessment\n","    missing_data, quality_score = processor.assess_data_quality(df)\n","\n","    # Data cleaning pipeline\n","    df_cleaned = processor.clean_data(df)\n","\n","    # Feature engineering pipeline\n","    df_processed = processor.engineer_features_live(df_cleaned)\n","\n","    # Processing summary\n","    st.markdown(\"---\")\n","    st.subheader(\"üìã Processing Pipeline Summary\")\n","\n","    col1, col2, col3, col4 = st.columns(4)\n","\n","    with col1:\n","        st.metric(\"Original Records\", f\"{len(df):,}\")\n","        st.metric(\"Data Quality Score\", f\"{quality_score:.1f}%\")\n","\n","    with col2:\n","        st.metric(\"Cleaned Records\", f\"{len(df_cleaned):,}\")\n","        retention_rate = len(df_cleaned) / len(df) * 100\n","        st.metric(\"Data Retention\", f\"{retention_rate:.1f}%\")\n","\n","    with col3:\n","        st.metric(\"Final Features\", len(df_processed.columns))\n","        engineered_features = len(df_processed.columns) - len(df.columns)\n","        st.metric(\"Features Added\", f\"+{engineered_features}\")\n","\n","    with col4:\n","        st.metric(\"Processing Status\", \"‚úÖ Complete\")\n","        st.metric(\"Pipeline Health\", \"100%\")\n","\n","    # Feature importance preview\n","    if 'Attrition_Flag_Binary' in df_processed.columns:\n","        st.subheader(\"üéØ Feature Engineering Impact\")\n","\n","        # Calculate correlations with target\n","        numeric_cols = df_processed.select_dtypes(include=[np.number]).columns\n","        correlations = df_processed[numeric_cols].corr()['Attrition_Flag_Binary'].abs().sort_values(ascending=False)\n","\n","        # Top 10 features\n","        top_features = correlations.head(11)[1:]  # Exclude self-correlation\n","\n","        col1, col2 = st.columns([2, 1])\n","\n","        with col1:\n","            fig_corr = px.bar(x=top_features.values, y=top_features.index,\n","                             orientation='h',\n","                             title=\"Top 10 Features by Correlation with Churn\",\n","                             color=top_features.values,\n","                             color_continuous_scale='Viridis')\n","            fig_corr.update_layout(height=400, yaxis={'categoryorder':'total ascending'})\n","            st.plotly_chart(fig_corr, use_container_width=True)\n","\n","        with col2:\n","            st.write(\"**Top Predictive Features:**\")\n","            for i, (feature, corr) in enumerate(top_features.head(5).items(), 1):\n","                st.write(f\"{i}. **{feature}**: {corr:.3f}\")\n","\n","            st.write(\"**Feature Categories:**\")\n","            st.write(\"üîπ **Behavioral**: Activity patterns\")\n","            st.write(\"üîπ **Financial**: Credit utilization\")\n","            st.write(\"üîπ **Engagement**: Service interactions\")\n","            st.write(\"üîπ **Temporal**: Usage trends\")\n","\n","    # Store processed data in session state\n","    st.session_state['processed_data'] = df_processed\n","\n","elif page == \"üîÆ Churn Prediction\":\n","    st.markdown('<h1 class=\"main-header\">üîÆ Live Churn Prediction Engine</h1>', unsafe_allow_html=True)\n","\n","    # Use processed data if available\n","    if 'processed_data' in st.session_state:\n","        df_pred = st.session_state['processed_data']\n","        st.success(\"‚úÖ Using processed data from pipeline\")\n","    else:\n","        df_pred = df\n","        st.info(\"üí° Run 'Data Processing Pipeline' first for enhanced predictions\")\n","\n","    col1, col2 = st.columns([1, 2])\n","\n","    with col1:\n","        st.subheader(\"üéØ Customer Risk Assessment\")\n","\n","        # Customer input form with live validation\n","        with st.form(\"live_prediction_form\"):\n","            st.markdown(\"**Customer Demographics**\")\n","            age = st.slider(\"Customer Age\", 18, 75, 45)\n","            gender = st.selectbox(\"Gender\", [\"M\", \"F\"])\n","            dependents = st.selectbox(\"Number of Dependents\", [0, 1, 2, 3, 4, 5])\n","            education = st.selectbox(\"Education Level\",\n","                                   [\"High School\", \"College\", \"Graduate\", \"Post-Graduate\", \"Doctorate\"])\n","            income = st.selectbox(\"Income Category\",\n","                                [\"Less than $40K\", \"$40K - $60K\", \"$60K - $80K\", \"$80K - $120K\", \"$120K +\"])\n","            card_category = st.selectbox(\"Card Category\", [\"Blue\", \"Silver\", \"Gold\", \"Platinum\"])\n","\n","            st.markdown(\"**Account Information**\")\n","            months_on_book = st.slider(\"Months on Book\", 13, 56, 35)\n","            total_relationship = st.slider(\"Total Products\", 1, 6, 3)\n","            months_inactive = st.slider(\"Months Inactive (Last 12)\", 0, 6, 1)\n","            contacts_count = st.slider(\"Contacts Count (Last 12)\", 0, 6, 2)\n","\n","            st.markdown(\"**Financial Information**\")\n","            credit_limit = st.number_input(\"Credit Limit ($)\", 1500, 35000, 12000)\n","            total_trans_amt = st.number_input(\"Total Transaction Amount ($)\", 500, 20000, 4500)\n","            total_trans_ct = st.slider(\"Total Transaction Count\", 10, 140, 64)\n","            utilization_ratio = st.slider(\"Utilization Ratio\", 0.0, 1.0, 0.3)\n","\n","            predict_button = st.form_submit_button(\"üîÆ Predict Churn Risk\", type=\"primary\")\n","\n","    with col2:\n","        if predict_button:\n","            # Live prediction using trained models or fallback logic\n","            if 'trained_models' in st.session_state and 'best_model_name' in st.session_state:\n","                st.info(\"ü§ñ Using trained ML model for prediction\")\n","                # Use trained model logic (from previous implementation)\n","                risk_score = calculate_risk_with_model(\n","                    st.session_state['trained_models'][st.session_state['best_model_name']],\n","                    age, gender, dependents, education, income, card_category,\n","                    months_on_book, total_relationship, months_inactive, contacts_count,\n","                    credit_limit, total_trans_amt, total_trans_ct, utilization_ratio\n","                )\n","            else:\n","                st.info(\"üìä Using rule-based prediction engine\")\n","                # Rule-based prediction\n","                risk_score = calculate_risk_score_live(\n","                    age, months_inactive, total_trans_ct, utilization_ratio,\n","                    total_relationship, contacts_count\n","                )\n","\n","            # Display live prediction results\n","            st.subheader(\"üìä Live Prediction Results\")\n","\n","            # Risk gauge with live updates\n","            fig_risk = go.Figure(go.Indicator(\n","                mode = \"gauge+number+delta\",\n","                value = risk_score * 100,\n","                domain = {'x': [0, 1], 'y': [0, 1]},\n","                title = {'text': \"Churn Risk Score (%)\"},\n","                delta = {'reference': 20, 'increasing': {'color': \"red\"}, 'decreasing': {'color': \"green\"}},\n","                gauge = {'axis': {'range': [None, 100]},\n","                        'bar': {'color': \"darkblue\"},\n","                        'steps' : [\n","                            {'range': [0, 30], 'color': \"lightgreen\"},\n","                            {'range': [30, 70], 'color': \"yellow\"},\n","                            {'range': [70, 100], 'color': \"red\"}],\n","                        'threshold' : {'line': {'color': \"red\", 'width': 4},\n","                                     'thickness': 0.75, 'value': 70}}))\n","            fig_risk.update_layout(height=400)\n","            st.plotly_chart(fig_risk, use_container_width=True)\n","\n","            # Live risk classification with recommendations\n","            if risk_score < 0.3:\n","                risk_level = \"üü¢ LOW RISK\"\n","                risk_color = \"#4CAF50\"\n","                priority = \"Standard\"\n","                recommendations = [\n","                    \"‚úÖ Continue standard engagement programs\",\n","                    \"üìà Monitor for behavioral changes\",\n","                    \"üéØ Consider upselling opportunities\",\n","                    \"üìß Include in regular marketing campaigns\"\n","                ]\n","            elif risk_score < 0.7:\n","                risk_level = \"üü° MEDIUM RISK\"\n","                risk_color = \"#FF9800\"\n","                priority = \"Monitor\"\n","                recommendations = [\n","                    \"üéØ Implement targeted retention campaigns\",\n","                    \"üíù Offer personalized incentives\",\n","                    \"üìû Increase customer service touchpoints\",\n","                    \"üîç Review and optimize product offerings\",\n","                    \"üìä Track engagement metrics closely\"\n","                ]\n","            else:\n","                risk_level = \"üî¥ HIGH RISK\"\n","                risk_color = \"#F44336\"\n","                priority = \"Critical\"\n","                recommendations = [\n","                    \"üö® **IMMEDIATE ACTION REQUIRED**\",\n","                    \"üèÉ‚Äç‚ôÇÔ∏è Deploy emergency retention protocols\",\n","                    \"üë§ Assign dedicated account manager\",\n","                    \"üéÅ Offer premium incentives and rewards\",\n","                    \"üìã Conduct customer satisfaction survey\",\n","                    \"üìû Schedule personal outreach within 24 hours\"\n","                ]\n","\n","            # Risk level display\n","            st.markdown(f\"\"\"\n","            <div style=\"background-color: {risk_color}; color: white; padding: 1.5rem; border-radius: 0.5rem; text-align: center; margin: 1rem 0;\">\n","                <h2>{risk_level}</h2>\n","                <p style=\"font-size: 1.2em; margin: 0;\">Churn Probability: {risk_score:.1%}</p>\n","                <p style=\"font-size: 1em; margin: 0;\">Priority Level: {priority}</p>\n","            </div>\n","            \"\"\", unsafe_allow_html=True)\n","\n","            # Live recommendations\n","            st.subheader(\"üí° Live Action Recommendations\")\n","            for i, rec in enumerate(recommendations, 1):\n","                st.write(f\"{i}. {rec}\")\n","\n","            # Risk factor analysis\n","            st.subheader(\"üìà Risk Factor Breakdown\")\n","\n","            risk_factors = {\n","                'Inactivity Risk': max(0, (months_inactive - 1) * 0.1),\n","                'Transaction Volume Risk': max(0, (50 - total_trans_ct) * 0.004),\n","                'Relationship Risk': max(0, (3 - total_relationship) * 0.05),\n","                'Utilization Risk': abs(utilization_ratio - 0.3) * 0.3,\n","                'Contact Frequency Risk': max(0, (contacts_count - 2) * 0.06),\n","                'Age Risk': max(0, (35 - age) * 0.002) if age < 35 else 0\n","            }\n","\n","            # Normalize risk factors\n","            total_risk = sum(risk_factors.values())\n","            if total_risk > 0:\n","                risk_factors = {k: v/total_risk for k, v in risk_factors.items()}\n","\n","            fig_factors = px.bar(x=list(risk_factors.keys()), y=list(risk_factors.values()),\n","                               title=\"Risk Factor Contributions\",\n","                               color=list(risk_factors.values()),\n","                               color_continuous_scale='Reds')\n","            fig_factors.update_layout(height=350, xaxis_tickangle=45)\n","            fig_factors.update_traces(texttemplate='%{y:.1%}', textposition='outside')\n","            st.plotly_chart(fig_factors, use_container_width=True)\n","\n","        else:\n","            st.info(\"üëÜ Complete customer details and click 'Predict Churn Risk' for live analysis\")\n","\n","            # Live customer search and batch prediction\n","            st.subheader(\"üîç Live Customer Analysis\")\n","\n","            # Sample high-risk customers from data\n","            if 'Risk_Category' in df.columns:\n","                high_risk_sample = df[df['Risk_Category'].isin(['High Risk', 'Critical Risk'])].head(5)\n","            else:\n","                # Create risk categories on the fly\n","                high_risk_sample = df[\n","                    (df['Months_Inactive_12_mon'] >= 3) |\n","                    (df['Total_Trans_Ct'] < 30) |\n","                    (df['Contacts_Count_12_mon'] >= 4)\n","                ].head(5)\n","\n","            if not high_risk_sample.empty:\n","                st.warning(f\"‚ö†Ô∏è {len(high_risk_sample)} high-risk customers detected in live data\")\n","\n","                display_cols = ['Customer_Age', 'Card_Category', 'Months_Inactive_12_mon',\n","                               'Total_Trans_Ct', 'Avg_Utilization_Ratio']\n","                available_cols = [col for col in display_cols if col in high_risk_sample.columns]\n","\n","                if available_cols:\n","                    sample_data = high_risk_sample[available_cols].copy()\n","                    sample_data.index = [f\"Customer_{i+1}\" for i in range(len(sample_data))]\n","                    st.dataframe(sample_data, use_container_width=True)"],"metadata":{"id":"DrxaPlm1wLIm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_clv(df):\n","    \"\"\"Calculate Customer Lifetime Value\"\"\"\n","    interchange_rate = 0.025\n","    annual_fee_avg = 95\n","    interest_rate = 0.18\n","    profit_margin = 0.15\n","\n","    # Estimate CLV based on transaction amounts and balances\n","    clv = (df['Total_Trans_Amt'] * interchange_rate +\n","           annual_fee_avg +\n","           df['Total_Revolving_Bal'] * interest_rate) * profit_margin\n","\n","    return clv# Load data using upload or fallback\n","df = load_data_with_upload()\n","\n","if df is None:\n","    st.info(\"üìÅ Using sample data for demonstration. Upload your CSV file for real analysis.\")\n","    df = load_sample_data()\n","\n","    # Add CLV and other derived features for sample data\n","    df['CLV'] = calculate_clv(df)\n","\n","    # Create utilization group\n","    df['Utilization_Group'] = pd.cut(df['Avg_Utilization_Ratio'],\n","                                   bins=[0, 0.1, 0.3, 0.7, 1.0],\n","                                   labels=['Low (0-10%)', 'Moderate (10-30%)',\n","                                          'High (30-70%)', 'Very High (70%+)'])\n","\n","    # Add basic feature engineering for sample data\n","    df = create_financial_features(df)\n","    df = create_behavioural_features(df)\n","    df = create_risk_features(df)\n","    df = calculate_rfm_scores(df)\n","    df = create_cc_segments(df)\n","    df = time_based_features(df)\n","\n","# Initialize live data processor\n","processor = LiveDataProcessor()\n","\n","# Sidebar navigation with live monitoring options\n","st.sidebar.title(\"üè¶ Navigation\")\n","page = st.sidebar.selectbox(\"Select Page\",\n","                           [\"üìä Live Monitoring Dashboard\", \"üßπ Data Processing Pipeline\",\n","                            \"üîÆ Churn Prediction\", \"üí∞ Business Impact\", \"üìà Advanced Analytics\"])\n","\n","# Live monitoring controls\n","st.sidebar.markdown(\"### Live Monitoring Controls\")\n","auto_refresh = st.sidebar.checkbox(\"Auto Refresh Data\", value=False)\n","refresh_interval = st.sidebar.selectbox(\"Refresh Interval\", [\"30 seconds\", \"1 minute\", \"5 minutes\"], index=1)\n","\n","if auto_refresh:\n","    st.sidebar.success(\"üü¢ Live monitoring active\")\n","else:\n","    st.sidebar.info(\"‚ö™ Manual mode\")\n","\n","# Manual refresh button\n","if st.sidebar.button(\"üîÑ Refresh Data Now\"):\n","    st.cache_data.clear()\n","    st.rerun()\n","\n","if page == \"üìä Live Monitoring Dashboard\":\n","    # Create live monitoring dashboard\n","    df = create_live_monitoring_dashboard(df)\n","\n","    # Live churn trends\n","    st.subheader(\"üìà Live Churn Trends & KPIs\")\n","\n","    col1, col2 = st.columns([2, 1])\n","\n","    with col1:\n","        # Churn rate by key segments with live updates\n","        tab1, tab2, tab3 = st.tabs([\"By Demographics\", \"By Activity\", \"By Financial Health\"])\n","\n","        with tab1:\n","            # Age groups churn analysis\n","            df['Age_Group'] = pd.cut(df['Customer_Age'], bins=[0, 30, 40, 50, 60, 100],\n","                                   labels=['<30', '30-40', '40-50', '50-60', '60+'])\n","            age_churn = df.groupby('Age_Group')['Attrition_Flag_Binary'].agg(['count', 'mean']).reset_index()\n","            age_churn.columns = ['Age_Group', 'Customers', 'Churn_Rate']\n","\n","            fig_age = px.bar(age_churn, x='Age_Group', y='Churn_Rate',\n","                           title=\"Live Churn Rate by Age Group\",\n","                           color='Churn_Rate', color_continuous_scale='Reds',\n","                           text='Churn_Rate')\n","            fig_age.update_traces(texttemplate='%{text:.1%}', textposition='outside')\n","            fig_age.update_layout(height=350)\n","            st.plotly_chart(fig_age, use_container_width=True)\n","\n","        with tab2:\n","            # Activity analysis\n","            activity_churn = df.groupby('Months_Inactive_12_mon')['Attrition_Flag_Binary'].mean().reset_index()\n","            activity_churn.columns = ['Months_Inactive', 'Churn_Rate']\n","\n","            fig_activity = px.line(activity_churn, x='Months_Inactive', y='Churn_Rate',\n","                                 title=\"Live Churn Rate by Months Inactive\",\n","                                 markers=True)\n","            fig_activity.update_traces(line_color='red', line_width=3, marker_size=8)\n","            fig_activity.update_layout(height=350)\n","            st.plotly_chart(fig_activity, use_container_width=True)\n","\n","        with tab3:\n","            # Financial health analysis\n","            util_churn = df.groupby('Utilization_Group')['Attrition_Flag_Binary'].mean().reset_index()\n","            util_churn = util_churn.dropna()\n","\n","            fig_util = px.bar(util_churn, x='Utilization_Group', y='Attrition_Flag_Binary',\n","                             title=\"Live Churn Rate by Credit Utilization\",\n","                             color='Attrition_Flag_Binary', color_continuous_scale='Oranges',\n","                             text='Attrition_Flag_Binary')\n","            fig_util.update_traces(texttemplate='%{text:.1%}', textposition='outside')\n","            fig_util.update_layout(height=350, xaxis_tickangle=45)\n","            st.plotly_chart(fig_util, use_container_width=True)\n","\n","    with col2:\n","        # Live risk distribution\n","        st.subheader(\"üéØ Live Risk Distribution\")\n","\n","        # Create risk categories based on multiple factors\n","        df['Risk_Score'] = (\n","            (df['Months_Inactive_12_mon'] >= 3).astype(int) * 25 +\n","            (df['Total_Trans_Ct'] < 30).astype(int) * 20 +\n","            (df['Contacts_Count_12_mon'] >= 4).astype(int) * 20 +\n","            (df['Avg_Utilization_Ratio'] > 0.8).astype(int) * 15 +\n","            (df['Total_Relationship_Count'] == 1).astype(int) * 10\n","        )\n","\n","        df['Risk_Category'] = pd.cut(df['Risk_Score'],\n","                                   bins=[0, 20, 40, 60, 100],\n","                                   labels=['Low Risk', 'Medium Risk', 'High Risk', 'Critical Risk'])\n","\n","        risk_counts = df['Risk_Category'].value_counts()\n","\n","        fig_risk = go.Figure(data=[\n","            go.Pie(labels=risk_counts.index,\n","                  values=risk_counts.values,\n","                  hole=0.6,\n","                  marker_colors=['green', 'yellow', 'orange', 'red'],\n","                  textinfo='label+percent')\n","        ])\n","        fig_risk.update_layout(title_text=\"Live Risk Distribution\", height=350)\n","        st.plotly_chart(fig_risk, use_container_width=True)\n","\n","        # Risk summary metrics\n","        st.subheader(\"‚ö° Real-time Alerts\")\n","\n","        critical_count = (df['Risk_Category'] == 'Critical Risk').sum()\n","        high_count = (df['Risk_Category'] == 'High Risk').sum()\n","\n","        if critical_count > 0:\n","            st.error(f\"üö® {critical_count} customers in CRITICAL risk!\")\n","\n","        if high_count > 0:\n","            st.warning(f\"‚ö†Ô∏è {high_count} customers in HIGH risk!\")\n","\n","        if critical_count == 0 and high_count == 0:\n","            st.success(\"‚úÖ No critical alerts at this time\")\n","\n","elif page == \"üßπ Data Processing Pipeline\":\n","    st.markdown('<h1 class=\"main-header\">üßπ Live Data Processing Pipeline</h1>', unsafe_allow_html=True)\n","\n","    # Data quality assessment\n","    missing_data, quality_score = processor.assess_data_quality(df)\n","\n","    # Data cleaning pipeline\n","    df_cleaned = processor.clean_data(df)\n","\n","    # Feature engineering pipeline\n","    df_processed = processor.engineer_features_live(df_cleaned)\n","\n","    # Processing summary\n","    st.markdown(\"---\")\n","    st.subheader(\"üìã Processing Pipeline Summary\")\n","\n","    col1, col2, col3, col4 = st.columns(4)\n","\n","    with col1:\n","        st.metric(\"Original Records\", f\"{len(df):,}\")\n","        st.metric(\"Data Quality Score\", f\"{quality_score:.1f}%\")\n","\n","    with col2:\n","        st.metric(\"Cleaned Records\", f\"{len(df_cleaned):,}\")\n","        retention_rate = len(df_cleaned) / len(df) * 100\n","        st.metric(\"Data Retention\", f\"{retention_rate:.1f}%\")\n","\n","    with col3:\n","        st.metric(\"Final Features\", len(df_processed.columns))\n","        engineered_features = len(df_processed.columns) - len(df.columns)\n","        st.metric(\"Features Added\", f\"+{engineered_features}\")\n","\n","    with col4:\n","        st.metric(\"Processing Status\", \"‚úÖ Complete\")\n","        st.metric(\"Pipeline Health\", \"100%\")\n","\n","    # Feature importance preview\n","    if 'Attrition_Flag_Binary' in df_processed.columns:\n","        st.subheader(\"üéØ Feature Engineering Impact\")\n","\n","        # Calculate correlations with target\n","        numeric_cols = df_processed.select_dtypes(include=[np.number]).columns\n","        correlations = df_processed[numeric_cols].corr()['Attrition_Flag_Binary'].abs().sort_values(ascending=False)\n","\n","        # Top 10 features\n","        top_features = correlations.head(11)[1:]  # Exclude self-correlation\n","\n","        col1, col2 = st.columns([2, 1])\n","\n","        with col1:\n","            fig_corr = px.bar(x=top_features.values, y=top_features.index,\n","                             orientation='h',\n","                             title=\"Top 10 Features by Correlation with Churn\",\n","                             color=top_features.values,\n","                             color_continuous_scale='Viridis')\n","            fig_corr.update_layout(height=400, yaxis={'categoryorder':'total ascending'})\n","            st.plotly_chart(fig_corr, use_container_width=True)\n","\n","        with col2:\n","            st.write(\"**Top Predictive Features:**\")\n","            for i, (feature, corr) in enumerate(top_features.head(5).items(), 1):\n","                st.write(f\"{i}. **{feature}**: {corr:.3f}\")\n","\n","            st.write(\"**Feature Categories:**\")\n","            st.write(\"üîπ **Behavioral**: Activity patterns\")\n","            st.write(\"üîπ **Financial**: Credit utilization\")\n","            st.write(\"üîπ **Engagement**: Service interactions\")\n","            st.write(\"üîπ **Temporal**: Usage trends\")\n","\n","    # Store processed data in session state\n","    st.session_state['processed_data'] = df_processed\n","\n","elif page == \"üîÆ Churn Prediction\":\n","    st.markdown('<h1 class=\"main-header\">üîÆ Live Churn Prediction Engine</h1>', unsafe_allow_html=True)\n","\n","    # Use processed data if available\n","    if 'processed_data' in st.session_state:\n","        df_pred = st.session_state['processed_data']\n","        st.success(\"‚úÖ Using processed data from pipeline\")\n","    else:\n","        df_pred = df\n","        st.info(\"üí° Run 'Data Processing Pipeline' first for enhanced predictions\")\n","\n","    col1, col2 = st.columns([1, 2])\n","\n","    with col1:\n","        st.subheader(\"üéØ Customer Risk Assessment\")\n","\n","        # Customer input form with live validation\n","        with st.form(\"live_prediction_form\"):\n","            st.markdown(\"**Customer Demographics**\")\n","            age = st.slider(\"Customer Age\", 18, 75, 45)\n","            gender = st.selectbox(\"Gender\", [\"M\", \"F\"])\n","            dependents = st.selectbox(\"Number of Dependents\", [0, 1, 2, 3, 4, 5])\n","            education = st.selectbox(\"Education Level\",\n","                                   [\"High School\", \"College\", \"Graduate\", \"Post-Graduate\", \"Doctorate\"])\n","            income = st.selectbox(\"Income Category\",\n","                                [\"Less than $40K\", \"$40K - $60K\", \"$60K - $80K\", \"$80K - $120K\", \"$120K +\"])\n","            card_category = st.selectbox(\"Card Category\", [\"Blue\", \"Silver\", \"Gold\", \"Platinum\"])\n","\n","            st.markdown(\"**Account Information**\")\n","            months_on_book = st.slider(\"Months on Book\", 13, 56, 35)\n","            total_relationship = st.slider(\"Total Products\", 1, 6, 3)\n","            months_inactive = st.slider(\"Months Inactive (Last 12)\", 0, 6, 1)\n","            contacts_count = st.slider(\"Contacts Count (Last 12)\", 0, 6, 2)\n","\n","            st.markdown(\"**Financial Information**\")\n","            credit_limit = st.number_input(\"Credit Limit ($)\", 1500, 35000, 12000)\n","            total_trans_amt = st.number_input(\"Total Transaction Amount ($)\", 500, 20000, 4500)\n","            total_trans_ct = st.slider(\"Total Transaction Count\", 10, 140, 64)\n","            utilization_ratio = st.slider(\"Utilization Ratio\", 0.0, 1.0, 0.3)\n","\n","            predict_button = st.form_submit_button(\"üîÆ Predict Churn Risk\", type=\"primary\")\n","\n","    with col2:\n","        if predict_button:\n","            # Live prediction using trained models or fallback logic\n","            if 'trained_models' in st.session_state and 'best_model_name' in st.session_state:\n","                st.info(\"ü§ñ Using trained ML model for prediction\")\n","                # Use trained model logic (from previous implementation)\n","                risk_score = calculate_risk_with_model(\n","                    st.session_state['trained_models'][st.session_state['best_model_name']],\n","                    age, gender, dependents, education, income, card_category,\n","                    months_on_book, total_relationship, months_inactive, contacts_count,\n","                    credit_limit, total_trans_amt, total_trans_ct, utilization_ratio\n","                )\n","            else:\n","                st.info(\"üìä Using rule-based prediction engine\")\n","                # Rule-based prediction\n","                risk_score = calculate_risk_score_live(\n","                    age, months_inactive, total_trans_ct, utilization_ratio,\n","                    total_relationship, contacts_count\n","                )\n","\n","            # Display live prediction results\n","            st.subheader(\"üìä Live Prediction Results\")\n","\n","            # Risk gauge with live updates\n","            fig_risk = go.Figure(go.Indicator(\n","                mode = \"gauge+number+delta\",\n","                value = risk_score * 100,\n","                domain = {'x': [0, 1], 'y': [0, 1]},\n","                title = {'text': \"Churn Risk Score (%)\"},\n","                delta = {'reference': 20, 'increasing': {'color': \"red\"}, 'decreasing': {'color': \"green\"}},\n","                gauge = {'axis': {'range': [None, 100]},\n","                        'bar': {'color': \"darkblue\"},\n","                        'steps' : [\n","                            {'range': [0, 30], 'color': \"lightgreen\"},\n","                            {'range': [30, 70], 'color': \"yellow\"},\n","                            {'range': [70, 100], 'color': \"red\"}],\n","                        'threshold' : {'line': {'color': \"red\", 'width': 4},\n","                                     'thickness': 0.75, 'value': 70}}))\n","            fig_risk.update_layout(height=400)\n","            st.plotly_chart(fig_risk, use_container_width=True)\n","\n","            # Live risk classification with recommendations\n","            if risk_score < 0.3:\n","                risk_level = \"üü¢ LOW RISK\"\n","                risk_color = \"#4CAF50\"\n","                priority = \"Standard\"\n","                recommendations = [\n","                    \"‚úÖ Continue standard engagement programs\",\n","                    \"üìà Monitor for behavioral changes\",\n","                    \"üéØ Consider upselling opportunities\",\n","                    \"üìß Include in regular marketing campaigns\"\n","                ]\n","            elif risk_score < 0.7:\n","                risk_level = \"üü° MEDIUM RISK\"\n","                risk_color = \"#FF9800\"\n","                priority = \"Monitor\"\n","                recommendations = [\n","                    \"üéØ Implement targeted retention campaigns\",\n","                    \"üíù Offer personalized incentives\",\n","                    \"üìû Increase customer service touchpoints\",\n","                    \"üîç Review and optimize product offerings\",\n","                    \"üìä Track engagement metrics closely\"\n","                ]\n","            else:\n","                risk_level = \"üî¥ HIGH RISK\"\n","                risk_color = \"#F44336\"\n","                priority = \"Critical\"\n","                recommendations = [\n","                    \"üö® **IMMEDIATE ACTION REQUIRED**\",\n","                    \"üèÉ‚Äç‚ôÇÔ∏è Deploy emergency retention protocols\",\n","                    \"üë§ Assign dedicated account manager\",\n","                    \"üéÅ Offer premium incentives and rewards\",\n","                    \"üìã Conduct customer satisfaction survey\",\n","                    \"üìû Schedule personal outreach within 24 hours\"\n","                ]\n","\n","            # Risk level display\n","            st.markdown(f\"\"\"\n","            <div style=\"background-color: {risk_color}; color: white; padding: 1.5rem; border-radius: 0.5rem; text-align: center; margin: 1rem 0;\">\n","                <h2>{risk_level}</h2>\n","                <p style=\"font-size: 1.2em; margin: 0;\">Churn Probability: {risk_score:.1%}</p>\n","                <p style=\"font-size: 1em; margin: 0;\">Priority Level: {priority}</p>\n","            </div>\n","            \"\"\", unsafe_allow_html=True)\n","\n","            # Live recommendations\n","            st.subheader(\"üí° Live Action Recommendations\")\n","            for i, rec in enumerate(recommendations, 1):\n","                st.write(f\"{i}. {rec}\")\n","\n","            # Risk factor analysis\n","            st.subheader(\"üìà Risk Factor Breakdown\")\n","\n","            risk_factors = {\n","                'Inactivity Risk': max(0, (months_inactive - 1) * 0.1),\n","                'Transaction Volume Risk': max(0, (50 - total_trans_ct) * 0.004),\n","                'Relationship Risk': max(0, (3 - total_relationship) * 0.05),\n","                'Utilization Risk': abs(utilization_ratio - 0.3) * 0.3,\n","                'Contact Frequency Risk': max(0, (contacts_count - 2) * 0.06),\n","                'Age Risk': max(0, (35 - age) * 0.002) if age < 35 else 0\n","            }\n","\n","            # Normalize risk factors\n","            total_risk = sum(risk_factors.values())\n","            if total_risk > 0:\n","                risk_factors = {k: v/total_risk for k, v in risk_factors.items()}\n","\n","            fig_factors = px.bar(x=list(risk_factors.keys()), y=list(risk_factors.values()),\n","                               title=\"Risk Factor Contributions\",\n","                               color=list(risk_factors.values()),\n","                               color_continuous_scale='Reds')\n","            fig_factors.update_layout(height=350, xaxis_tickangle=45)\n","            fig_factors.update_traces(texttemplate='%{y:.1%}', textposition='outside')\n","            st.plotly_chart(fig_factors, use_container_width=True)\n","\n","        else:\n","            st.info(\"üëÜ Complete customer details and click 'Predict Churn Risk' for live analysis\")\n","\n","            # Live customer search and batch prediction\n","            st.subheader(\"üîç Live Customer Analysis\")\n","\n","            # Sample high-risk customers from data\n","            if 'Risk_Category' in df.columns:\n","                high_risk_sample = df[df['Risk_Category'].isin(['High Risk', 'Critical Risk'])].head(5)\n","            else:\n","                # Create risk categories on the fly\n","                high_risk_sample = df[\n","                    (df['Months_Inactive_12_mon'] >= 3) |\n","                    (df['Total_Trans_Ct'] < 30) |\n","                    (df['Contacts_Count_12_mon'] >= 4)\n","                ].head(5)\n","\n","            if not high_risk_sample.empty:\n","                st.warning(f\"‚ö†Ô∏è {len(high_risk_sample)} high-risk customers detected in live data\")\n","\n","                display_cols = ['Customer_Age', 'Card_Category', 'Months_Inactive_12_mon',\n","                               'Total_Trans_Ct', 'Avg_Utilization_Ratio']\n","                available_cols = [col for col in display_cols if col in high_risk_sample.columns]\n","\n","                if available_cols:\n","                    sample_data = high_risk_sample[available_cols].copy()\n","                    sample_data.index = [f\"Customer_{i+1}\" for i in range(len(sample_data))]\n","                    st.dataframe(sample_data, use_container_width=True)"],"metadata":{"id":"eiR1bV8LmJ4H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_risk_score_live(age, months_inactive, total_trans_ct, utilization_ratio, total_relationship, contacts_count):\n","    \"\"\"Calculate risk score using live business rules\"\"\"\n","\n","    risk_score = 0.16  # Base risk\n","\n","    # Age factor\n","    if age < 35:\n","        risk_score += 0.08\n","    elif age > 55:\n","        risk_score -= 0.05\n","\n","    # Activity factors\n","    if months_inactive >= 3:\n","        risk_score += 0.25\n","    elif months_inactive == 0:\n","        risk_score -= 0.08\n","\n","    # Transaction patterns\n","    if total_trans_ct < 30:\n","        risk_score += 0.15\n","    elif total_trans_ct > 100:\n","        risk_score -= 0.10\n","\n","    # Utilization patterns\n","    if utilization_ratio == 0:\n","        risk_score += 0.20\n","    elif utilization_ratio > 0.8:\n","        risk_score += 0.10\n","    elif 0.1 <= utilization_ratio <= 0.3:\n","        risk_score -= 0.05\n","\n","    # Relationship depth\n","    if total_relationship == 1:\n","        risk_score += 0.12\n","    elif total_relationship >= 4:\n","        risk_score -= 0.15\n","\n","    # Contact frequency\n","    if contacts_count >= 4:\n","        risk_score += 0.20\n","    elif contacts_count == 0:\n","        risk_score += 0.05\n","\n","    return np.clip(risk_score, 0.02, 0.90)"],"metadata":{"id":"50qV3ZOtmOqL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Sample data fallback for when CSV is not available\n","def load_sample_data():\n","    \"\"\"Generate sample data when CSV is not available\"\"\"\n","    np.random.seed(42)\n","    n_records = 1000\n","\n","    data = {\n","        'Customer_Age': np.random.randint(18, 75, n_records),\n","        'Gender': np.random.choice(['M', 'F'], n_records, p=[0.47, 0.53]),\n","        'Dependent_count': np.random.choice([0, 1, 2, 3, 4, 5], n_records, p=[0.30, 0.25, 0.20, 0.15, 0.07, 0.03]),\n","        'Education_Level': np.random.choice(['High School', 'College', 'Graduate', 'Post-Graduate', 'Doctorate'],\n","                                          n_records, p=[0.20, 0.25, 0.30, 0.20, 0.05]),\n","        'Marital_Status': np.random.choice(['Married', 'Single', 'Divorced'], n_records, p=[0.46, 0.42, 0.12]),\n","        'Income_Category': np.random.choice(['Less than $40K', '$40K - $60K', '$60K - $80K', '$80K - $120K', '$120K +'],\n","                                          n_records, p=[0.17, 0.24, 0.20, 0.21, 0.18]),\n","        'Card_Category': np.random.choice(['Blue', 'Silver', 'Gold', 'Platinum'], n_records, p=[0.80, 0.12, 0.05, 0.03]),\n","        'Months_on_book': np.random.randint(13, 57, n_records),\n","        'Total_Relationship_Count': np.random.choice([1, 2, 3, 4, 5, 6], n_records, p=[0.05, 0.30, 0.25, 0.20, 0.15, 0.05]),\n","        'Months_Inactive_12_mon': np.random.choice([0, 1, 2, 3, 4, 5, 6], n_records, p=[0.40, 0.20, 0.15, 0.10, 0.08, 0.05, 0.02]),\n","        'Contacts_Count_12_mon': np.random.choice([0, 1, 2, 3, 4, 5, 6], n_records, p=[0.35, 0.25, 0.20, 0.12, 0.05, 0.02, 0.01]),\n","        'Credit_Limit': np.random.lognormal(9.2, 0.8, n_records).clip(1500, 35000),\n","        'Total_Revolving_Bal': np.random.exponential(1500, n_records).clip(0, 25000),\n","        'Total_Trans_Amt': np.random.lognormal(8.5, 1.2, n_records).clip(500, 20000),\n","        'Total_Trans_Ct': np.random.randint(10, 140, n_records),\n","        'Total_Amt_Chng_Q4_Q1': np.random.normal(0.76, 0.4, n_records).clip(0, 3.4),\n","        'Total_Ct_Chng_Q4_Q1': np.random.normal(0.72, 0.4, n_records).clip(0, 3.7),\n","        'Avg_Utilization_Ratio': np.random.beta(2, 5, n_records),\n","        'Last_Transaction_Date': pd.date_range(start='2023-01-01', end='2024-12-31', periods=n_records),\n","        'Attrition_Flag': np.random.choice(['Existing Customer', 'Attrited Customer'], n_records, p=[0.8, 0.2])\n","    }\n","\n","    df = pd.DataFrame(data)\n","    df['Avg_Open_To_Buy'] = df['Credit_Limit'] - df['Total_Revolving_Bal']\n","    df['Attrition_Flag_Binary'] = (df['Attrition_Flag'] == 'Attrited Customer').astype(int)\n","\n","    return df"],"metadata":{"id":"5-RHTdsUmVBC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Feature engineering functions from your code\n","def create_financial_features(df):\n","    \"\"\"Create advanced financial health indicators from your advanced_feature_engineering.py\"\"\"\n","\n","    # Credit health score\n","    df['Credit_Health_Score'] = (\n","        (df['Credit_Limit']/ df['Credit_Limit'].max()) * 0.3 +\n","        (1 - df['Avg_Utilization_Ratio']) * 0.4 +\n","        (df['Total_Trans_Ct']/df['Total_Trans_Ct'].max()) * 0.3\n","    )\n","\n","    # Payment capacity ratio\n","    df['Payment_Capacity'] = df['Avg_Open_To_Buy'] / df['Credit_Limit']\n","\n","    # Transaction efficiency\n","    df['Transaction_Efficiency'] = df['Total_Trans_Amt'] / df['Total_Trans_Ct']\n","\n","    # Relationship tenure value\n","    df['Tenure_Value_Ratio'] = df['Total_Trans_Ct'] / df['Months_on_book']\n","\n","    return df\n","\n","def create_behavioural_features(df):\n","    \"\"\"Create behavioural patterns indicators from your code\"\"\"\n","\n","    # Activity consistency\n","    df['Activity_Consistency'] = 1/(1+df['Months_Inactive_12_mon'])\n","\n","    # Service interaction intensity\n","    df['Service_Intensity'] = df['Contacts_Count_12_mon'] / 12\n","\n","    # Usage volatility (Q4 vs Q1 changes)\n","    df['Usage_Volatility'] =  abs(df['Total_Amt_Chng_Q4_Q1'] -1) + abs(df['Total_Ct_Chng_Q4_Q1'] - 1)\n","\n","    # Cross-product engagement\n","    df['Cross_Product_Engagement'] = df['Total_Relationship_Count'] / 6 # Normalized\n","\n","    return df\n","\n","def create_risk_features(df):\n","    \"\"\"Create risk scoring features from your code\"\"\"\n","\n","    # High utilization risk\n","    df['High_Util_Risk'] = (df['Avg_Utilization_Ratio'] > 0.8).astype(int)\n","\n","    # Declining usage risk\n","    df['Declining_Usage_Risk'] = ((df['Total_Amt_Chng_Q4_Q1'] < 0.7) | (df['Total_Ct_Chng_Q4_Q1'] < 0.7)).astype(int)\n","\n","    # Single product risk\n","    df['Single_Product_Risk'] = (df['Total_Relationship_Count'] == 1).astype(int)\n","\n","    return df\n","\n","def calculate_rfm_scores(df, transaction_date_col='Last_Transaction_Date'):\n","    \"\"\"Calculate RFM scores for credit card customers from your code\"\"\"\n","\n","    # Calculate Recency (days since last transaction)\n","    current_date = pd.to_datetime('2025-01-31') # Reference date\n","    df['Recency'] = (current_date - pd.to_datetime(df[transaction_date_col])).dt.days\n","\n","    # Frequency is already available as Total_Trans_Ct\n","    df['Frequency'] = df['Total_Trans_Ct']\n","\n","    # Monetary is already available as Total_Trans_Amt\n","    df['Monetary'] = df['Total_Trans_Amt']\n","\n","    # Create quintile scores (1-5, where 5 is best)\n","    df['R_Score'] = pd.qcut(df['Recency'], 5, labels=[5,4,3,2,1], duplicates='drop') # Lower recency = higher score\n","    df['F_Score'] = pd.qcut(df['Frequency'].rank(method='first'), 5, labels=[1,2,3,4,5], duplicates='drop')\n","    df['M_Score'] = pd.qcut(df['Monetary'].rank(method='first'), 5, labels=[1,2,3,4,5], duplicates='drop')\n","\n","    # Convert to numeric\n","    df['R_Score'] = pd.to_numeric(df['R_Score'], errors='coerce').fillna(3).astype(int)\n","    df['F_Score'] = pd.to_numeric(df['F_Score'], errors='coerce').fillna(3).astype(int)\n","    df['M_Score'] = pd.to_numeric(df['M_Score'], errors='coerce').fillna(3).astype(int)\n","\n","    # Create RFM segments\n","    df['RFM_Score'] = df['R_Score'].astype(str) + df['F_Score'].astype(str) + df['M_Score'].astype(str)\n","\n","    return df\n","\n","def create_cc_segments(df):\n","    \"\"\"Create credit card specific customer segments from your code\"\"\"\n","\n","    # Define segment rules based on RFM scores\n","    def assign_segment(row):\n","        r, f, m = row['R_Score'], row['F_Score'], row['M_Score']\n","\n","        # Champions: High value, frequent, recent users\n","        if r >= 4 and f >= 4 and m >= 4:\n","            return 'Champions'\n","        # Loyal Customers: Regular users with good value\n","        elif r >= 3 and f >= 3 and m >= 3:\n","            return 'Loyal Customers'\n","        # Potential Loyalists: Good recent activity, building frequency\n","        elif r >= 4 and f >= 3 and m >= 2:\n","            return 'Potential Loyalists'\n","        # At Risk: Previously good customers, declining activity\n","        elif r <= 2 and f >= 3 and m >= 3:\n","            return 'At Risk'\n","        # Cannot Lose Them: High value but very low recent activity\n","        elif r <= 2 and f >= 4 and m >= 4:\n","            return 'Cannot Lose Them'\n","        # New Customers: Recent but low frequency/value\n","        elif r >= 4 and f <= 2 and m <= 2:\n","            return 'New Customers'\n","        # Hibernating: Low scores across all dimensions\n","        elif r <= 2 and f <= 2 and m <= 2:\n","            return 'Hibernating'\n","        # Need Attention: Moderate scores but concerning patterns\n","        else:\n","            return 'Need Attention'\n","\n","    df['Customer_Segment'] = df.apply(assign_segment, axis=1)\n","\n","    # Create segment mapping for encoding\n","    segment_mapping = {'Champions': 1, 'Loyal Customers': 2, 'Potential Loyalists': 3,\n","                      'At Risk': 4, 'Cannot Lose Them': 5, 'New Customers': 6,\n","                      'Hibernating': 7, 'Need Attention': 8}\n","    df['Customer_Segment_Encoded'] = df['Customer_Segment'].map(segment_mapping)\n","\n","    return df\n","\n","def time_based_features(df):\n","    \"\"\"Create time-based features from your code\"\"\"\n","\n","    df['Spending_Trend'] = df['Total_Amt_Chng_Q4_Q1']\n","    df['Activity_Trend'] = df['Total_Ct_Chng_Q4_Q1']\n","    df['Declining_Spend_Flag'] = (df['Total_Amt_Chng_Q4_Q1'] < 1).astype(int)\n","    df['Declining_Activity_Flag'] = (df['Total_Ct_Chng_Q4_Q1'] < 1).astype(int)\n","\n","    return df"],"metadata":{"id":"8UjDiFnDmX9D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load data\n","df = load_sample_data()\n","\n","# Sidebar navigation\n","st.sidebar.title(\"üè¶ Navigation\")\n","page = st.sidebar.selectbox(\"Select Page\",\n","                           [\"üìä Executive Dashboard\", \"üîÆ Churn Prediction\", \"üí∞ Business Impact\", \"üìà Advanced Analytics\"])\n","\n","if page == \"üìä Executive Dashboard\":\n","    st.markdown('<h1 class=\"main-header\">üí≥ Credit Card Churn Analytics Dashboard</h1>', unsafe_allow_html=True)\n","\n","    # Key Metrics Row\n","    col1, col2, col3, col4, col5, col6 = st.columns(6)\n","\n","    total_customers = len(df)\n","    churned_customers = df['Attrition_Flag'].sum()\n","    churn_rate = churned_customers / total_customers\n","    avg_credit_limit = df['Credit_Limit'].mean()\n","    avg_clv = df['CLV'].mean()\n","    high_risk_customers = (df['Churn_Probability'] > 0.7).sum()\n","\n","    with col1:\n","        st.metric(\"Total Customers\", f\"{total_customers:,}\",\n","                 delta=f\"+{int(total_customers*0.05)} vs last month\")\n","\n","    with col2:\n","        st.metric(\"Churn Rate\", f\"{churn_rate:.1%}\",\n","                 delta=f\"{churn_rate-0.18:.1%} vs target\", delta_color=\"inverse\")\n","\n","    with col3:\n","        st.metric(\"Avg Credit Limit\", f\"${avg_credit_limit:,.0f}\",\n","                 delta=f\"+${avg_credit_limit*0.03:.0f}\")\n","\n","    with col4:\n","        st.metric(\"Avg CLV\", f\"${avg_clv:.0f}\",\n","                 delta=f\"+${avg_clv*0.12:.0f}\")\n","\n","    with col5:\n","        st.metric(\"High Risk Customers\", f\"{high_risk_customers:,}\",\n","                 delta=f\"-{int(high_risk_customers*0.08)} vs last month\", delta_color=\"inverse\")\n","\n","    with col6:\n","        st.metric(\"Revenue at Risk\", f\"${(high_risk_customers * avg_clv):,.0f}\",\n","                 delta=f\"-${int(high_risk_customers * avg_clv * 0.15):,}\", delta_color=\"inverse\")\n","\n","    st.markdown(\"---\")\n","\n","    # Main Dashboard Content\n","    col1, col2 = st.columns([2, 1])\n","\n","    with col1:\n","        # Churn Rate by Key Segments\n","        st.subheader(\"üìà Churn Rate Analysis\")\n","\n","        tab1, tab2, tab3 = st.tabs([\"By Demographics\", \"By Activity\", \"By Products\"])\n","\n","        with tab1:\n","            # Age groups churn analysis\n","            df['Age_Group'] = pd.cut(df['Customer_Age'], bins=[0, 30, 40, 50, 60, 100],\n","                                   labels=['<30', '30-40', '40-50', '50-60', '60+'])\n","            age_churn = df.groupby('Age_Group')['Attrition_Flag'].agg(['count', 'mean']).reset_index()\n","            age_churn.columns = ['Age_Group', 'Customers', 'Churn_Rate']\n","\n","            fig_age = px.bar(age_churn, x='Age_Group', y='Churn_Rate',\n","                           title=\"Churn Rate by Age Group\",\n","                           color='Churn_Rate', color_continuous_scale='Reds')\n","            fig_age.update_layout(height=300)\n","            st.plotly_chart(fig_age, use_container_width=True)\n","\n","        with tab2:\n","            # Activity analysis\n","            activity_churn = df.groupby('Months_Inactive_12_mon')['Attrition_Flag'].mean().reset_index()\n","            activity_churn.columns = ['Months_Inactive', 'Churn_Rate']\n","\n","            fig_activity = px.line(activity_churn, x='Months_Inactive', y='Churn_Rate',\n","                                 title=\"Churn Rate by Months Inactive\",\n","                                 markers=True)\n","            fig_activity.update_traces(line_color='red', line_width=3, marker_size=8)\n","            fig_activity.update_layout(height=300)\n","            st.plotly_chart(fig_activity, use_container_width=True)\n","\n","        with tab3:\n","            # Product relationship analysis\n","            product_churn = df.groupby('Total_Relationship_Count')['Attrition_Flag'].agg(['count', 'mean']).reset_index()\n","            product_churn.columns = ['Products', 'Customers', 'Churn_Rate']\n","\n","            fig_product = make_subplots(specs=[[{\"secondary_y\": True}]])\n","            fig_product.add_trace(\n","                go.Bar(x=product_churn['Products'], y=product_churn['Customers'], name=\"Customer Count\"),\n","                secondary_y=False,\n","            )\n","            fig_product.add_trace(\n","                go.Scatter(x=product_churn['Products'], y=product_churn['Churn_Rate'],\n","                          mode='lines+markers', name=\"Churn Rate\", line=dict(color='red', width=3)),\n","                secondary_y=True,\n","            )\n","            fig_product.update_xaxes(title_text=\"Number of Products\")\n","            fig_product.update_yaxes(title_text=\"Customer Count\", secondary_y=False)\n","            fig_product.update_yaxes(title_text=\"Churn Rate\", secondary_y=True)\n","            fig_product.update_layout(title_text=\"Products vs Churn Rate\", height=300)\n","            st.plotly_chart(fig_product, use_container_width=True)\n","\n","    with col2:\n","        # Risk Distribution Gauge\n","        st.subheader(\"üéØ Risk Distribution\")\n","\n","        risk_counts = {\n","            'Low Risk': ((df['Churn_Probability'] < 0.3).sum()),\n","            'Medium Risk': ((df['Churn_Probability'] >= 0.3) & (df['Churn_Probability'] < 0.7)).sum(),\n","            'High Risk': ((df['Churn_Probability'] >= 0.7).sum())\n","        }\n","\n","        fig_gauge = go.Figure(data=[\n","            go.Pie(labels=list(risk_counts.keys()), values=list(risk_counts.values()),\n","                  hole=0.6, marker_colors=['green', 'orange', 'red'])\n","        ])\n","        fig_gauge.update_traces(textposition='inside', textinfo='percent+label')\n","        fig_gauge.update_layout(title_text=\"Customer Risk Distribution\", height=300)\n","        st.plotly_chart(fig_gauge, use_container_width=True)\n","\n","        # Top Risk Factors\n","        st.subheader(\"‚ö†Ô∏è Top Risk Factors\")\n","        st.markdown(\"\"\"\n","        <div class=\"insight-box\">\n","        <strong>Critical Risk Indicators:</strong><br>\n","        ‚Ä¢ Months Inactive ‚â• 3: <span class=\"risk-high\">+25% churn risk</span><br>\n","        ‚Ä¢ Single Product: <span class=\"risk-high\">+12% churn risk</span><br>\n","        ‚Ä¢ Low Transactions: <span class=\"risk-medium\">+15% churn risk</span><br>\n","        ‚Ä¢ High Contacts: <span class=\"risk-medium\">+20% churn risk</span>\n","        </div>\n","        \"\"\", unsafe_allow_html=True)\n","\n","    # Bottom Row - Financial Analysis\n","    st.subheader(\"üí∞ Financial Behavior Analysis\")\n","\n","    col1, col2, col3 = st.columns(3)\n","\n","    with col1:\n","        # Credit Utilization Analysis\n","        util_churn = df.groupby('Utilization_Group')['Attrition_Flag'].mean().reset_index()\n","        fig_util = px.bar(util_churn, x='Utilization_Group', y='Attrition_Flag',\n","                         title=\"Churn Rate by Credit Utilization\",\n","                         color='Attrition_Flag', color_continuous_scale='Reds')\n","        fig_util.update_layout(height=300, xaxis_tickangle=45)\n","        st.plotly_chart(fig_util, use_container_width=True)\n","\n","    with col2:\n","        # Transaction Volume vs Churn\n","        df['Trans_Volume_Group'] = pd.cut(df['Total_Trans_Amt'], bins=4, labels=['Low', 'Medium', 'High', 'Very High'])\n","        trans_churn = df.groupby('Trans_Volume_Group')['Attrition_Flag'].mean().reset_index()\n","        fig_trans = px.bar(trans_churn, x='Trans_Volume_Group', y='Attrition_Flag',\n","                          title=\"Churn Rate by Transaction Volume\",\n","                          color='Attrition_Flag', color_continuous_scale='Blues')\n","        fig_trans.update_layout(height=300)\n","        st.plotly_chart(fig_trans, use_container_width=True)\n","\n","    with col3:\n","        # Card Category Performance\n","        card_churn = df.groupby('Card_Category')['Attrition_Flag'].mean().reset_index()\n","        fig_card = px.bar(card_churn, x='Card_Category', y='Attrition_Flag',\n","                         title=\"Churn Rate by Card Category\",\n","                         color='Attrition_Flag', color_continuous_scale='Greens')\n","        fig_card.update_layout(height=300)\n","        st.plotly_chart(fig_card, use_container_width=True)\n","\n","elif page == \"üîÆ Churn Prediction\":\n","    st.markdown('<h1 class=\"main-header\">üîÆ Customer Churn Prediction</h1>', unsafe_allow_html=True)\n","\n","    col1, col2 = st.columns([1, 2])\n","\n","    with col1:\n","        st.subheader(\"üéØ Customer Profile Input\")\n","\n","        # Customer input form\n","        with st.form(\"prediction_form\"):\n","            age = st.slider(\"Customer Age\", 18, 75, 45)\n","            gender = st.selectbox(\"Gender\", [\"M\", \"F\"])\n","            dependents = st.selectbox(\"Number of Dependents\", [0, 1, 2, 3, 4, 5])\n","            education = st.selectbox(\"Education Level\",\n","                                   [\"High School\", \"College\", \"Graduate\", \"Post-Graduate\", \"Doctorate\"])\n","            income = st.selectbox(\"Income Category\",\n","                                [\"Less than $40K\", \"$40K - $60K\", \"$60K - $80K\", \"$80K - $120K\", \"$120K +\"])\n","            card_category = st.selectbox(\"Card Category\", [\"Blue\", \"Silver\", \"Gold\", \"Platinum\"])\n","\n","            st.markdown(\"**Account Information**\")\n","            months_on_book = st.slider(\"Months on Book\", 13, 56, 35)\n","            total_relationship = st.slider(\"Total Products\", 1, 6, 3)\n","            months_inactive = st.slider(\"Months Inactive (Last 12)\", 0, 6, 1)\n","            contacts_count = st.slider(\"Contacts Count (Last 12)\", 0, 6, 2)\n","\n","            st.markdown(\"**Financial Information**\")\n","            credit_limit = st.number_input(\"Credit Limit ($)\", 1500, 35000, 12000)\n","            total_trans_amt = st.number_input(\"Total Transaction Amount ($)\", 500, 20000, 4500)\n","            total_trans_ct = st.slider(\"Total Transaction Count\", 10, 140, 64)\n","            utilization_ratio = st.slider(\"Utilization Ratio\", 0.0, 1.0, 0.3)\n","\n","            predict_button = st.form_submit_button(\"üîÆ Predict Churn Risk\")\n","\n","    with col2:\n","        if predict_button:\n","            # Check if we have trained models from Advanced Analytics\n","            if 'trained_models' in st.session_state and 'best_model_name' in st.session_state:\n","                # Use the trained model for prediction\n","                best_model = st.session_state['trained_models'][st.session_state['best_model_name']]\n","                feature_columns = st.session_state['feature_columns']\n","                scaler = st.session_state['scaler']\n","                encoders = st.session_state['label_encoders']\n","\n","                # Prepare input data\n","                input_data = pd.DataFrame({\n","                    'Customer_Age': [age],\n","                    'Dependent_count': [dependents],\n","                    'Months_on_book': [months_on_book],\n","                    'Total_Relationship_Count': [total_relationship],\n","                    'Months_Inactive_12_mon': [months_inactive],\n","                    'Contacts_Count_12_mon': [contacts_count],\n","                    'Credit_Limit': [credit_limit],\n","                    'Total_Revolving_Bal': [credit_limit * utilization_ratio],\n","                    'Total_Trans_Amt': [total_trans_amt],\n","                    'Total_Trans_Ct': [total_trans_ct],\n","                    'Total_Amt_Chng_Q4_Q1': [np.random.normal(0.76, 0.2)],  # Estimated\n","                    'Total_Ct_Chng_Q4_Q1': [np.random.normal(0.72, 0.2)],   # Estimated\n","                    'Avg_Utilization_Ratio': [utilization_ratio],\n","                    'Gender_encoded': [1 if gender == 'M' else 0],\n","                    'Education_encoded': [encoders['education_mapping'].get(education, 2)],\n","                    'Marital_encoded': [encoders['marital_mapping'].get('Single', 0)],  # Default\n","                    'Income_encoded': [encoders['income_mapping'].get(income, 2)],\n","                    'Card_encoded': [encoders['card_mapping'].get(card_category, 0)]\n","                })\n","\n","                # Ensure all feature columns are present\n","                for col in feature_columns:\n","                    if col not in input_data.columns:\n","                        input_data[col] = 0\n","\n","                input_data = input_data[feature_columns]\n","\n","                # Make prediction\n","                try:\n","                    if st.session_state['best_model_name'] in ['Logistic Regression', 'Neural Network']:\n","                        input_scaled = scaler.transform(input_data)\n","                        risk_score = best_model.predict_proba(input_scaled)[0][1]\n","                    else:\n","                        risk_score = best_model.predict_proba(input_data)[0][1]\n","\n","                    st.success(f\"‚úÖ Using trained {st.session_state['best_model_name']} model for prediction!\")\n","\n","                except Exception as e:\n","                    st.warning(f\"Error using trained model: {str(e)}. Using fallback method.\")\n","                    # Fallback to rule-based prediction\n","                    risk_score = 0.16  # Base risk\n","\n","                    # Age factor\n","                    if age < 35:\n","                        risk_score += 0.08\n","                    elif age > 55:\n","                        risk_score -= 0.05\n","\n","                    # Activity factors\n","                    if months_inactive >= 3:\n","                        risk_score += 0.25\n","                    elif months_inactive == 0:\n","                        risk_score -= 0.08\n","\n","                    # Transaction patterns\n","                    if total_trans_ct < 30:\n","                        risk_score += 0.15\n","                    elif total_trans_ct > 100:\n","                        risk_score -= 0.10\n","\n","                    # Utilization patterns\n","                    if utilization_ratio == 0:\n","                        risk_score += 0.20\n","                    elif utilization_ratio > 0.8:\n","                        risk_score += 0.10\n","                    elif 0.1 <= utilization_ratio <= 0.3:\n","                        risk_score -= 0.05\n","\n","                    # Relationship depth\n","                    if total_relationship == 1:\n","                        risk_score += 0.12\n","                    elif total_relationship >= 4:\n","                        risk_score -= 0.15\n","\n","                    # Contact frequency\n","                    if contacts_count >= 4:\n","                        risk_score += 0.20\n","                    elif contacts_count == 0:\n","                        risk_score += 0.05\n","\n","                    risk_score = np.clip(risk_score, 0.02, 0.90)\n","\n","            else:\n","                # Use rule-based prediction (fallback)\n","                st.info(\"üí° Using rule-based prediction. Train models in 'Advanced Analytics' for ML predictions!\")\n","\n","                risk_score = 0.16  # Base risk\n","\n","                # Age factor\n","                if age < 35:\n","                    risk_score += 0.08\n","                elif age > 55:\n","                    risk_score -= 0.05\n","\n","                # Activity factors\n","                if months_inactive >= 3:\n","                    risk_score += 0.25\n","                elif months_inactive == 0:\n","                    risk_score -= 0.08\n","\n","                # Transaction patterns\n","                if total_trans_ct < 30:\n","                    risk_score += 0.15\n","                elif total_trans_ct > 100:\n","                    risk_score -= 0.10\n","\n","                # Utilization patterns\n","                if utilization_ratio == 0:\n","                    risk_score += 0.20\n","                elif utilization_ratio > 0.8:\n","                    risk_score += 0.10\n","                elif 0.1 <= utilization_ratio <= 0.3:\n","                    risk_score -= 0.05\n","\n","                # Relationship depth\n","                if total_relationship == 1:\n","                    risk_score += 0.12\n","                elif total_relationship >= 4:\n","                    risk_score -= 0.15\n","\n","                # Contact frequency\n","                if contacts_count >= 4:\n","                    risk_score += 0.20\n","                elif contacts_count == 0:\n","                    risk_score += 0.05\n","\n","                risk_score = np.clip(risk_score, 0.02, 0.90)\n","\n","            # Display prediction results\n","            st.subheader(\"üìä Prediction Results\")\n","\n","            # Risk gauge\n","            fig_risk = go.Figure(go.Indicator(\n","                mode = \"gauge+number+delta\",\n","                value = risk_score * 100,\n","                domain = {'x': [0, 1], 'y': [0, 1]},\n","                title = {'text': \"Churn Risk Score (%)\"},\n","                delta = {'reference': 20},\n","                gauge = {'axis': {'range': [None, 100]},\n","                        'bar': {'color': \"darkblue\"},\n","                        'steps' : [\n","                            {'range': [0, 30], 'color': \"lightgreen\"},\n","                            {'range': [30, 70], 'color': \"yellow\"},\n","                            {'range': [70, 100], 'color': \"red\"}],\n","                        'threshold' : {'line': {'color': \"red\", 'width': 4},\n","                                     'thickness': 0.75, 'value': 70}}))\n","            fig_risk.update_layout(height=300)\n","            st.plotly_chart(fig_risk, use_container_width=True)\n","\n","            # Risk classification\n","            if risk_score < 0.3:\n","                risk_level = \"üü¢ LOW RISK\"\n","                risk_color = \"green\"\n","                recommendations = [\n","                    \"Continue standard engagement programs\",\n","                    \"Monitor for any behavioral changes\",\n","                    \"Consider upselling opportunities\"\n","                ]\n","            elif risk_score < 0.7:\n","                risk_level = \"üü° MEDIUM RISK\"\n","                risk_color = \"orange\"\n","                recommendations = [\n","                    \"Implement targeted retention campaigns\",\n","                    \"Offer personalized incentives\",\n","                    \"Increase customer service touchpoints\",\n","                    \"Review and optimize product offerings\"\n","                ]\n","            else:\n","                risk_level = \"üî¥ HIGH RISK\"\n","                risk_color = \"red\"\n","                recommendations = [\n","                    \"**IMMEDIATE ACTION REQUIRED**\",\n","                    \"Deploy emergency retention protocols\",\n","                    \"Assign dedicated account manager\",\n","                    \"Offer premium incentives and rewards\",\n","                    \"Conduct customer satisfaction survey\"\n","                ]\n","\n","            st.markdown(f\"\"\"\n","            <div style=\"background-color: {risk_color}; color: white; padding: 1rem; border-radius: 0.5rem; text-align: center; margin: 1rem 0;\">\n","                <h3>{risk_level}</h3>\n","                <p>Churn Probability: {risk_score:.1%}</p>\n","            </div>\n","            \"\"\", unsafe_allow_html=True)\n","\n","            # Recommendations\n","            st.subheader(\"üí° Recommended Actions\")\n","            for rec in recommendations:\n","                st.write(f\"‚Ä¢ {rec}\")\n","\n","            # Feature contributions\n","            st.subheader(\"üìà Risk Factor Contributions\")\n","            factors = {\n","                'Months Inactive': max(0, (months_inactive - 1) * 0.08),\n","                'Transaction Volume': max(0, (50 - total_trans_ct) * 0.003),\n","                'Product Relationship': max(0, (3 - total_relationship) * 0.04),\n","                'Utilization Pattern': abs(utilization_ratio - 0.2) * 0.2,\n","                'Contact Frequency': max(0, (contacts_count - 2) * 0.05)\n","            }\n","\n","            fig_factors = px.bar(x=list(factors.keys()), y=list(factors.values()),\n","                               title=\"Risk Factor Contributions\",\n","                               color=list(factors.values()), color_continuous_scale='Reds')\n","            fig_factors.update_layout(height=300, xaxis_tickangle=45)\n","            st.plotly_chart(fig_factors, use_container_width=True)\n","\n","        else:\n","            st.info(\"üëÜ Fill in customer details and click 'Predict Churn Risk' to see results\")\n","\n","            # Sample predictions for demonstration\n","            st.subheader(\"üìã Recent Predictions\")\n","            sample_predictions = pd.DataFrame({\n","                'Customer_ID': ['CU001', 'CU002', 'CU003', 'CU004', 'CU005'],\n","                'Risk_Score': [0.85, 0.23, 0.67, 0.12, 0.91],\n","                'Risk_Level': ['HIGH', 'LOW', 'MEDIUM', 'LOW', 'HIGH'],\n","                'Action_Required': ['Immediate', 'Monitor', 'Targeted Campaign', 'Standard', 'Emergency']\n","            })\n","\n","            st.dataframe(sample_predictions, use_container_width=True)\n","\n","elif page == \"üí∞ Business Impact\":\n","    st.markdown('<h1 class=\"main-header\">üí∞ Business Impact Analysis</h1>', unsafe_allow_html=True)\n","\n","    # Use processed data if available\n","    if 'processed_data' in st.session_state:\n","        df_impact = st.session_state['processed_data']\n","        st.success(\"‚úÖ Using processed data with advanced features\")\n","    else:\n","        df_impact = df\n","        st.info(\"üí° Run 'Data Processing Pipeline' first for enhanced analysis\")\n","\n","    # Business Impact Calculations\n","    total_customers = len(df_impact)\n","\n","    # Calculate churn probability if not available\n","    if 'Churn_Probability' not in df_impact.columns:\n","        df_impact['Churn_Probability'] = (\n","            (df_impact['Months_Inactive_12_mon'] >= 3).astype(float) * 0.25 +\n","            (df_impact['Total_Trans_Ct'] < 30).astype(float) * 0.15 +\n","            (df_impact['Total_Relationship_Count'] == 1).astype(float) * 0.12 +\n","            (df_impact['Contacts_Count_12_mon'] >= 4).astype(float) * 0.20 +\n","            0.16  # Base rate\n","        ).clip(0.02, 0.90)\n","\n","    high_risk_customers = (df_impact['Churn_Probability'] >= 0.7).sum()\n","    medium_risk_customers = ((df_impact['Churn_Probability'] >= 0.3) & (df_impact['Churn_Probability'] < 0.7)).sum()\n","\n","    # Calculate CLV if not available\n","    if 'CLV' not in df_impact.columns:\n","        df_impact['CLV'] = calculate_clv(df_impact)\n","\n","    avg_clv = df_impact['CLV'].mean()\n","    total_clv_at_risk = df_impact[df_impact['Churn_Probability'] >= 0.7]['CLV'].sum()\n","\n","    # Retention campaign costs\n","    retention_cost_high = 500  # Cost per high-risk customer\n","    retention_cost_medium = 200  # Cost per medium-risk customer\n","    campaign_success_rate = 0.40  # 40% success rate\n","\n","    # ROI Calculations\n","    high_risk_investment = high_risk_customers * retention_cost_high\n","    medium_risk_investment = medium_risk_customers * retention_cost_medium\n","    total_investment = high_risk_investment + medium_risk_investment\n","\n","    expected_clv_saved = (high_risk_customers * avg_clv * campaign_success_rate) + \\\n","                        (medium_risk_customers * avg_clv * 0.25)\n","\n","    net_benefit = expected_clv_saved - total_investment\n","    roi_percentage = (net_benefit / total_investment) * 100 if total_investment > 0 else 0\n","\n","    # Top metrics\n","    col1, col2, col3, col4 = st.columns(4)\n","\n","    with col1:\n","        st.metric(\"CLV at Risk\", f\"${total_clv_at_risk:,.0f}\",\n","                 help=\"Total Customer Lifetime Value of high-risk customers\")\n","\n","    with col2:\n","        st.metric(\"Campaign Investment\", f\"${total_investment:,.0f}\",\n","                 help=\"Total cost of retention campaigns\")\n","\n","    with col3:\n","        st.metric(\"Expected CLV Saved\", f\"${expected_clv_saved:,.0f}\",\n","                 help=\"Expected value saved through successful retention\")\n","\n","    with col4:\n","        st.metric(\"Campaign ROI\", f\"{roi_percentage:.1f}%\",\n","                 delta=f\"{roi_percentage - 150:.1f}% vs target\",\n","                 help=\"Return on investment for retention campaigns\")\n","\n","    st.markdown(\"---\")\n","\n","    # Detailed Analysis\n","    col1, col2 = st.columns(2)\n","\n","    with col1:\n","        st.subheader(\"üìä Customer Segmentation Strategy\")\n","\n","        # Create segment analysis\n","        df_impact['Risk_Segment'] = pd.cut(df_impact['Churn_Probability'],\n","                                   bins=[0, 0.3, 0.7, 1.0],\n","                                   labels=['Low Risk', 'Medium Risk', 'High Risk'])\n","\n","        segment_analysis = df_impact.groupby('Risk_Segment').agg({\n","            'Churn_Probability': 'count',\n","            'CLV': ['mean', 'sum']\n","        }).round(2)\n","        segment_analysis.columns = ['Customer_Count', 'Avg_CLV', 'Total_CLV']\n","        segment_analysis = segment_analysis.reset_index()\n","\n","        # Add campaign costs and ROI\n","        segment_analysis['Campaign_Cost'] = segment_analysis.apply(\n","            lambda x: x['Customer_Count'] * (500 if x['Risk_Segment'] == 'High Risk'\n","                                           else 200 if x['Risk_Segment'] == 'Medium Risk' else 50), axis=1\n","        )\n","\n","        segment_analysis['Expected_Savings'] = segment_analysis.apply(\n","            lambda x: x['Customer_Count'] * x['Avg_CLV'] * (0.4 if x['Risk_Segment'] == 'High Risk'\n","                                                          else 0.25 if x['Risk_Segment'] == 'Medium Risk' else 0.1), axis=1\n","        )\n","\n","        segment_analysis['ROI'] = ((segment_analysis['Expected_Savings'] - segment_analysis['Campaign_Cost']) /\n","                                  segment_analysis['Campaign_Cost'] * 100).round(1)\n","\n","        st.dataframe(segment_analysis, use_container_width=True)\n","\n","        # Strategy recommendations\n","        st.subheader(\"üéØ Retention Strategy by Segment\")\n","\n","        strategies = {\n","            \"üî¥ High Risk (‚â•70%)\": {\n","                \"customers\": high_risk_customers,\n","                \"strategy\": \"Emergency Retention Protocol\",\n","                \"tactics\": [\"VIP customer service\", \"Waive all fees\", \"Double rewards points\", \"Personal account manager\"],\n","                \"budget\": f\"${retention_cost_high:,}/customer\",\n","                \"expected_roi\": \"250-400%\"\n","            },\n","            \"üü° Medium Risk (30-70%)\": {\n","                \"customers\": medium_risk_customers,\n","                \"strategy\": \"Proactive Engagement Campaign\",\n","                \"tactics\": [\"Targeted offers\", \"Usage incentives\", \"Product education\", \"Satisfaction surveys\"],\n","                \"budget\": f\"${retention_cost_medium:,}/customer\",\n","                \"expected_roi\": \"150-250%\"\n","            },\n","            \"üü¢ Low Risk (<30%)\": {\n","                \"customers\": total_customers - high_risk_customers - medium_risk_customers,\n","                \"strategy\": \"Growth & Upselling Focus\",\n","                \"tactics\": [\"Cross-sell products\", \"Loyalty programs\", \"Referral incentives\", \"Premium upgrades\"],\n","                \"budget\": \"$50/customer\",\n","                \"expected_roi\": \"200-300%\"\n","            }\n","        }\n","\n","        for risk_level, details in strategies.items():\n","            with st.expander(f\"{risk_level} - {details['customers']:,} customers\"):\n","                st.write(f\"**Strategy:** {details['strategy']}\")\n","                st.write(f\"**Budget:** {details['budget']}\")\n","                st.write(f\"**Expected ROI:** {details['expected_roi']}\")\n","                st.write(\"**Tactics:**\")\n","                for tactic in details['tactics']:\n","                    st.write(f\"‚Ä¢ {tactic}\")\n","\n","    with col2:\n","        st.subheader(\"üìà Financial Impact Projections\")\n","\n","        # Monthly projection chart\n","        months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun']\n","        current_churn_rate = df_impact['Attrition_Flag_Binary'].mean()\n","        baseline_churn = [current_churn_rate] * 6\n","        with_intervention = [current_churn_rate * 0.85, current_churn_rate * 0.75, current_churn_rate * 0.65,\n","                           current_churn_rate * 0.60, current_churn_rate * 0.58, current_churn_rate * 0.55]\n","\n","        fig_projection = go.Figure()\n","        fig_projection.add_trace(go.Scatter(x=months, y=[x*100 for x in baseline_churn],\n","                                          mode='lines+markers', name='Baseline Churn Rate',\n","                                          line=dict(color='red', width=3)))\n","        fig_projection.add_trace(go.Scatter(x=months, y=[x*100 for x in with_intervention],\n","                                          mode='lines+markers', name='With Retention Program',\n","                                          line=dict(color='green', width=3)))\n","        fig_projection.update_layout(title='Projected Churn Rate Reduction',\n","                                   xaxis_title='Month', yaxis_title='Churn Rate (%)',\n","                                   height=300)\n","        st.plotly_chart(fig_projection, use_container_width=True)\n","\n","        # Revenue impact\n","        st.subheader(\"üíµ Revenue Impact Analysis\")\n","\n","        current_monthly_revenue = total_customers * avg_clv / 12\n","        prevented_churn_customers = high_risk_customers * campaign_success_rate\n","        additional_monthly_revenue = prevented_churn_customers * avg_clv / 12\n","\n","        revenue_metrics = {\n","            \"Current Monthly Revenue\": f\"${current_monthly_revenue:,.0f}\",\n","            \"Customers Saved (Est.)\": f\"{prevented_churn_customers:,.0f}\",\n","            \"Additional Monthly Revenue\": f\"${additional_monthly_revenue:,.0f}\",\n","            \"Annual Revenue Impact\": f\"${additional_monthly_revenue * 12:,.0f}\",\n","            \"3-Year Revenue Impact\": f\"${additional_monthly_revenue * 36:,.0f}\"\n","        }\n","\n","        for metric, value in revenue_metrics.items():\n","            col_a, col_b = st.columns([2, 1])\n","            col_a.write(f\"**{metric}:**\")\n","            col_b.write(value)\n","\n","        # Cost-benefit analysis\n","        st.subheader(\"‚öñÔ∏è Cost-Benefit Analysis\")\n","\n","        cost_benefit_data = {\n","            'Category': ['Campaign Costs', 'Revenue Saved', 'Net Benefit'],\n","            'Year 1': [total_investment, expected_clv_saved, net_benefit],\n","            'Year 2': [total_investment * 0.8, expected_clv_saved * 1.5, expected_clv_saved * 1.5 - total_investment * 0.8],\n","            'Year 3': [total_investment * 0.6, expected_clv_saved * 2.0, expected_clv_saved * 2.0 - total_investment * 0.6]\n","        }\n","\n","        cost_benefit_df = pd.DataFrame(cost_benefit_data)\n","\n","        fig_cb = go.Figure()\n","        fig_cb.add_trace(go.Bar(name='Campaign Costs', x=cost_benefit_df['Category'],\n","                               y=[cost_benefit_df['Year 1'][0], 0, 0], marker_color='red'))\n","        fig_cb.add_trace(go.Bar(name='Revenue Saved', x=cost_benefit_df['Category'],\n","                               y=[0, cost_benefit_df['Year 1'][1], 0], marker_color='green'))\n","        fig_cb.add_trace(go.Bar(name='Net Benefit', x=cost_benefit_df['Category'],\n","                               y=[0, 0, cost_benefit_df['Year 1'][2]], marker_color='blue'))\n","        fig_cb.update_layout(title='3-Year Cost-Benefit Analysis', height=300)\n","        st.plotly_chart(fig_cb, use_container_width=True)\n","\n","elif page == \"üìà Advanced Analytics\":\n","    st.markdown('<h1 class=\"main-header\">üìà Advanced Analytics & Model Training</h1>', unsafe_allow_html=True)\n","\n","    # Use processed data if available\n","    if 'processed_data' in st.session_state:\n","        df_analytics = st.session_state['processed_data']\n","        st.success(\"‚úÖ Using processed data with engineered features\")\n","    else:\n","        df_analytics = df\n","        st.info(\"üí° Run 'Data Processing Pipeline' first for enhanced model training\")\n","\n","    # Advanced Analytics Dashboard\n","    tab1, tab2, tab3, tab4 = st.tabs([\"üîç Feature Analysis\", \"üìä Cohort Analysis\", \"üéØ Model Training\", \"üö® Alert System\"])\n","\n","    with tab1:\n","        st.subheader(\"üîç Feature Importance Analysis\")\n","\n","        # Feature importance data based on your actual results from models.py\n","        feature_importance = {\n","            'Contacts_Count_12_mon': 0.161038,\n","            'Gender': 0.143476,\n","            'Months_Inactive_12_mon': 0.113300,\n","            'Utilization_Group': 0.092224,\n","            'Declining_Activity_Flag': 0.089313,\n","            'Activity_Consistency': 0.077223,\n","            'Total_Relationship_Count': 0.068084,\n","            'Declining_Spend_Flag': 0.066035,\n","            'Declining_Usage_Risk': 0.056116,\n","            'Cross_Product_Engagement': 0.044562,\n","            'Total_Trans_Ct': 0.040123,\n","            'Customer_Age': 0.035677,\n","            'Credit_Limit': 0.028943,\n","            'Total_Trans_Amt': 0.025831\n","        }\n","\n","        col1, col2 = st.columns(2)\n","\n","        with col1:\n","            # Feature importance chart (matching your visualization style)\n","            fig_fi = px.bar(x=list(feature_importance.values()),\n","                           y=list(feature_importance.keys()),\n","                           orientation='h',\n","                           title=\"Top Feature Importance - AdaBoost Model\",\n","                           color=list(feature_importance.values()),\n","                           color_continuous_scale='Viridis')\n","            fig_fi.update_layout(height=500, yaxis={'categoryorder':'total ascending'})\n","            fig_fi.update_traces(texttemplate='%{x:.3f}', textposition='outside')\n","            st.plotly_chart(fig_fi, use_container_width=True)\n","\n","        with col2:\n","            # Feature correlation heatmap (from your analysis)\n","            correlation_features = ['Customer_Age', 'Total_Trans_Ct', 'Total_Trans_Amt',\n","                                  'Avg_Utilization_Ratio', 'Months_Inactive_12_mon',\n","                                  'Total_Relationship_Count', 'Contacts_Count_12_mon']\n","            available_features = [f for f in correlation_features if f in df_analytics.columns]\n","\n","            if len(available_features) > 3:\n","                correlation_data = df_analytics[available_features + ['Attrition_Flag_Binary']].corr()\n","\n","                fig_corr = px.imshow(correlation_data,\n","                                   title=\"Feature Correlation Matrix\",\n","                                   color_continuous_scale='RdBu_r',\n","                                   aspect='auto',\n","                                   text_auto='.2f')\n","                fig_corr.update_layout(height=500)\n","                st.plotly_chart(fig_corr, use_container_width=True)\n","            else:\n","                st.info(\"Upload processed data to see correlation analysis\")\n","\n","        # Feature insights based on your analysis results\n","        st.subheader(\"üí° Key Feature Insights from Analysis\")\n","\n","        insights_col1, insights_col2 = st.columns(2)\n","\n","        with insights_col1:\n","            st.markdown(\"\"\"\n","            **üî¥ Critical Risk Factors (From Your Analysis):**\n","            - **Contacts Count (16.1%)**: Most important predictor - high contact frequency indicates issues\n","            - **Gender (14.3%)**: Demographic factor with significant impact\n","            - **Months Inactive (11.3%)**: Strong activity-based predictor\n","            - **Utilization Group (9.2%)**: Credit usage patterns are crucial\n","            \"\"\")\n","\n","            st.markdown(\"\"\"\n","            **üìä Statistical Insights:**\n","            - Customers with 3+ inactive months: **+25% churn risk**\n","            - Single-product customers: **+12% churn risk**\n","            - High utilization (>80%): **+10% churn risk**\n","            - 4+ contacts in 12 months: **+20% churn risk**\n","            \"\"\")\n","\n","        with insights_col2:\n","            st.markdown(\"\"\"\n","            **üìà Actionable Business Rules:**\n","            - **Monitor customers with 3+ inactive months** - highest priority\n","            - **Track contact frequency** - 4+ contacts = intervention needed\n","            - **Focus on single-product customers** for cross-selling\n","            - **Watch utilization patterns** - both extremes are risky\n","            \"\"\")\n","\n","            st.markdown(\"\"\"\n","            **üéØ Model Performance (Your Results):**\n","            - **Best Model**: AdaBoost with 78.61% AUC\n","            - **Business ROI**: 89.2% campaign ROI\n","            - **Precision**: 47.3% of predicted churners actually churn\n","            - **Recall**: 45.0% of actual churners are identified\n","            \"\"\")\n","\n","    with tab2:\n","        st.subheader(\"üìä Customer Cohort Analysis\")\n","\n","        # Cohort analysis by acquisition month (based on your RFM analysis)\n","        df_analytics['Acquisition_Month'] = pd.to_datetime('2024-01-01') - pd.to_timedelta(df_analytics['Months_on_book'] * 30, unit='D')\n","        df_analytics['Acquisition_Cohort'] = df_analytics['Acquisition_Month'].dt.to_period('M')\n","\n","        # Create cohort table using your methodology\n","        cohort_columns = ['Attrition_Flag_Binary']\n","        if 'CLV' in df_analytics.columns:\n","            cohort_columns.append('CLV')\n","        if 'R_Score' in df_analytics.columns:\n","            cohort_columns.extend(['R_Score', 'F_Score', 'M_Score'])\n","\n","        cohort_data = df_analytics.groupby('Acquisition_Cohort')[cohort_columns].agg({\n","            'Attrition_Flag_Binary': ['count', 'sum', 'mean'],\n","            'CLV': 'mean' if 'CLV' in cohort_columns else lambda x: 0,\n","            'R_Score': 'mean' if 'R_Score' in cohort_columns else lambda x: 0,\n","            'F_Score': 'mean' if 'F_Score' in cohort_columns else lambda x: 0,\n","            'M_Score': 'mean' if 'M_Score' in cohort_columns else lambda x: 0\n","        }).round(2)\n","\n","        cohort_data.columns = ['Customers', 'Churned', 'Churn_Rate', 'Avg_CLV', 'Recency_Score', 'Frequency_Score', 'Monetary_Score']\n","\n","        st.subheader(\"üìÖ RFM-Based Cohort Performance\")\n","\n","        # Display cohort table\n","        cohort_display = cohort_data.reset_index()\n","        cohort_display['Acquisition_Cohort'] = cohort_display['Acquisition_Cohort'].astype(str)\n","        st.dataframe(cohort_display, use_container_width=True)\n","\n","        # Cohort visualizations\n","        col1, col2 = st.columns(2)\n","\n","        with col1:\n","            # Churn rate by cohort\n","            fig_cohort_churn = px.line(cohort_display,\n","                                     x='Acquisition_Cohort', y='Churn_Rate',\n","                                     title='Churn Rate Evolution by Cohort',\n","                                     markers=True)\n","            fig_cohort_churn.update_traces(line_color='red', line_width=3, marker_size=8)\n","            fig_cohort_churn.update_layout(height=350, xaxis_tickangle=45)\n","            st.plotly_chart(fig_cohort_churn, use_container_width=True)\n","\n","        with col2:\n","            # CLV by cohort\n","            fig_cohort_clv = px.bar(cohort_display,\n","                                   x='Acquisition_Cohort', y='Avg_CLV',\n","                                   title='Average CLV by Acquisition Cohort',\n","                                   color='Avg_CLV', color_continuous_scale='Greens')\n","            fig_cohort_clv.update_layout(height=350, xaxis_tickangle=45)\n","            st.plotly_chart(fig_cohort_clv, use_container_width=True)\n","\n","        # RFM Score Distribution Analysis (from your customer segmentation)\n","        if all(col in df_analytics.columns for col in ['R_Score', 'F_Score', 'M_Score']):\n","            st.subheader(\"üéØ RFM Score Distribution Analysis\")\n","\n","            col1, col2, col3 = st.columns(3)\n","\n","            with col1:\n","                # Recency Score Distribution\n","                r_score_dist = df_analytics['R_Score'].value_counts().sort_index()\n","                fig_r = px.bar(x=r_score_dist.index, y=r_score_dist.values,\n","                              title=\"Recency Score Distribution\",\n","                              color=r_score_dist.values, color_continuous_scale='Reds')\n","                fig_r.update_layout(height=300)\n","                st.plotly_chart(fig_r, use_container_width=True)\n","\n","            with col2:\n","                # Frequency Score Distribution\n","                f_score_dist = df_analytics['F_Score'].value_counts().sort_index()\n","                fig_f = px.bar(x=f_score_dist.index, y=f_score_dist.values,\n","                              title=\"Frequency Score Distribution\",\n","                              color=f_score_dist.values, color_continuous_scale='Blues')\n","                fig_f.update_layout(height=300)\n","                st.plotly_chart(fig_f, use_container_width=True)\n","\n","            with col3:\n","                # Monetary Score Distribution\n","                m_score_dist = df_analytics['M_Score'].value_counts().sort_index()\n","                fig_m = px.bar(x=m_score_dist.index, y=m_score_dist.values,\n","                              title=\"Monetary Score Distribution\",\n","                              color=m_score_dist.values, color_continuous_scale='Greens')\n","                fig_m.update_layout(height=300)\n","                st.plotly_chart(fig_m, use_container_width=True)\n","\n","        # Customer Segment Analysis (from your create_cc_segments function)\n","        if 'Customer_Segment' in df_analytics.columns:\n","            st.subheader(\"üè∑Ô∏è Customer Segment Performance\")\n","\n","            segment_columns = ['Attrition_Flag_Binary']\n","            if 'CLV' in df_analytics.columns:\n","                segment_columns.append('CLV')\n","            segment_columns.extend(['Total_Trans_Amt'])\n","            if 'R_Score' in df_analytics.columns:\n","                segment_columns.extend(['R_Score', 'F_Score', 'M_Score'])\n","\n","            segment_analysis = df_analytics.groupby('Customer_Segment')[segment_columns].agg({\n","                'Attrition_Flag_Binary': ['count', 'sum', 'mean'],\n","                'CLV': 'mean' if 'CLV' in segment_columns else lambda x: 100,\n","                'Total_Trans_Amt': 'mean',\n","                'R_Score': 'mean' if 'R_Score' in segment_columns else lambda x: 3,\n","                'F_Score': 'mean' if 'F_Score' in segment_columns else lambda x: 3,\n","                'M_Score': 'mean' if 'M_Score' in segment_columns else lambda x: 3\n","            }).round(3)\n","\n","            segment_analysis.columns = ['Customer_Count', 'Churned_Count', 'Churn_Rate',\n","                                       'Avg_CLV', 'Avg_Transaction_Amt', 'Avg_R_Score', 'Avg_F_Score', 'Avg_M_Score']\n","\n","            # Display segment analysis\n","            segment_display = segment_analysis.reset_index()\n","            st.dataframe(segment_display, use_container_width=True)\n","\n","            # Segment visualization\n","            fig_segment_performance = px.scatter(segment_display,\n","                                               x='Churn_Rate', y='Avg_CLV',\n","                                               size='Customer_Count',\n","                                               color='Customer_Segment',\n","                                               title='Customer Segment Performance: Churn Rate vs CLV',\n","                                               hover_data=['Customer_Count'])\n","            fig_segment_performance.update_layout(height=400)\n","            st.plotly_chart(fig_segment_performance, use_container_width=True)\n","\n","    # Business Impact Calculations\n","    total_customers = len(df)\n","    high_risk_customers = (df['Churn_Probability'] >= 0.7).sum()\n","    medium_risk_customers = ((df['Churn_Probability'] >= 0.3) & (df['Churn_Probability'] < 0.7)).sum()\n","\n","    avg_clv = df['CLV'].mean()\n","    total_clv_at_risk = df[df['Churn_Probability'] >= 0.7]['CLV'].sum()\n","\n","    # Retention campaign costs\n","    retention_cost_high = 500  # Cost per high-risk customer\n","    retention_cost_medium = 200  # Cost per medium-risk customer\n","    campaign_success_rate = 0.40  # 40% success rate\n","\n","    # ROI Calculations\n","    high_risk_investment = high_risk_customers * retention_cost_high\n","    medium_risk_investment = medium_risk_customers * retention_cost_medium\n","    total_investment = high_risk_investment + medium_risk_investment\n","\n","    expected_clv_saved = (high_risk_customers * avg_clv * campaign_success_rate) + \\\n","                        (medium_risk_customers * avg_clv * 0.25)\n","\n","    net_benefit = expected_clv_saved - total_investment\n","    roi_percentage = (net_benefit / total_investment) * 100 if total_investment > 0 else 0\n","\n","    # Top metrics\n","    col1, col2, col3, col4 = st.columns(4)\n","\n","    with col1:\n","        st.metric(\"CLV at Risk\", f\"${total_clv_at_risk:,.0f}\",\n","                 help=\"Total Customer Lifetime Value of high-risk customers\")\n","\n","    with col2:\n","        st.metric(\"Campaign Investment\", f\"${total_investment:,.0f}\",\n","                 help=\"Total cost of retention campaigns\")\n","\n","    with col3:\n","        st.metric(\"Expected CLV Saved\", f\"${expected_clv_saved:,.0f}\",\n","                 help=\"Expected value saved through successful retention\")\n","\n","    with col4:\n","        st.metric(\"Campaign ROI\", f\"{roi_percentage:.1f}%\",\n","                 delta=f\"{roi_percentage - 150:.1f}% vs target\",\n","                 help=\"Return on investment for retention campaigns\")\n","\n","    st.markdown(\"---\")\n","\n","    # Detailed Analysis\n","    col1, col2 = st.columns(2)\n","\n","    with col1:\n","        st.subheader(\"üìä Customer Segmentation Strategy\")\n","\n","        # Create segment analysis\n","        df['Risk_Segment'] = pd.cut(df['Churn_Probability'],\n","                                   bins=[0, 0.3, 0.7, 1.0],\n","                                   labels=['Low Risk', 'Medium Risk', 'High Risk'])\n","\n","        segment_analysis = df.groupby('Risk_Segment').agg({\n","            'Churn_Probability': 'count',\n","            'CLV': ['mean', 'sum']\n","        }).round(2)\n","        segment_analysis.columns = ['Customer_Count', 'Avg_CLV', 'Total_CLV']\n","        segment_analysis = segment_analysis.reset_index()\n","\n","        # Add campaign costs and ROI\n","        segment_analysis['Campaign_Cost'] = segment_analysis.apply(\n","            lambda x: x['Customer_Count'] * (500 if x['Risk_Segment'] == 'High Risk'\n","                                           else 200 if x['Risk_Segment'] == 'Medium Risk' else 50), axis=1\n","        )\n","\n","        segment_analysis['Expected_Savings'] = segment_analysis.apply(\n","            lambda x: x['Customer_Count'] * x['Avg_CLV'] * (0.4 if x['Risk_Segment'] == 'High Risk'\n","                                                          else 0.25 if x['Risk_Segment'] == 'Medium Risk' else 0.1), axis=1\n","        )\n","\n","        segment_analysis['ROI'] = ((segment_analysis['Expected_Savings'] - segment_analysis['Campaign_Cost']) /\n","                                  segment_analysis['Campaign_Cost'] * 100).round(1)\n","\n","        st.dataframe(segment_analysis, use_container_width=True)\n","\n","        # Strategy recommendations\n","        st.subheader(\"üéØ Retention Strategy by Segment\")\n","\n","        strategies = {\n","            \"üî¥ High Risk (‚â•70%)\": {\n","                \"customers\": high_risk_customers,\n","                \"strategy\": \"Emergency Retention Protocol\",\n","                \"tactics\": [\"VIP customer service\", \"Waive all fees\", \"Double rewards points\", \"Personal account manager\"],\n","                \"budget\": f\"${retention_cost_high:,}/customer\",\n","                \"expected_roi\": \"250-400%\"\n","            },\n","            \"üü° Medium Risk (30-70%)\": {\n","                \"customers\": medium_risk_customers,\n","                \"strategy\": \"Proactive Engagement Campaign\",\n","                \"tactics\": [\"Targeted offers\", \"Usage incentives\", \"Product education\", \"Satisfaction surveys\"],\n","                \"budget\": f\"${retention_cost_medium:,}/customer\",\n","                \"expected_roi\": \"150-250%\"\n","            },\n","            \"üü¢ Low Risk (<30%)\": {\n","                \"customers\": total_customers - high_risk_customers - medium_risk_customers,\n","                \"strategy\": \"Growth & Upselling Focus\",\n","                \"tactics\": [\"Cross-sell products\", \"Loyalty programs\", \"Referral incentives\", \"Premium upgrades\"],\n","                \"budget\": \"$50/customer\",\n","                \"expected_roi\": \"200-300%\"\n","            }\n","        }\n","\n","        for risk_level, details in strategies.items():\n","            with st.expander(f\"{risk_level} - {details['customers']:,} customers\"):\n","                st.write(f\"**Strategy:** {details['strategy']}\")\n","                st.write(f\"**Budget:** {details['budget']}\")\n","                st.write(f\"**Expected ROI:** {details['expected_roi']}\")\n","                st.write(\"**Tactics:**\")\n","                for tactic in details['tactics']:\n","                    st.write(f\"‚Ä¢ {tactic}\")\n","\n","    with col2:\n","        st.subheader(\"üìà Financial Impact Projections\")\n","\n","        # Monthly projection chart\n","        months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun']\n","        baseline_churn = [churn_rate] * 6\n","        with_intervention = [churn_rate * 0.85, churn_rate * 0.75, churn_rate * 0.65,\n","                           churn_rate * 0.60, churn_rate * 0.58, churn_rate * 0.55]\n","\n","        fig_projection = go.Figure()\n","        fig_projection.add_trace(go.Scatter(x=months, y=[x*100 for x in baseline_churn],\n","                                          mode='lines+markers', name='Baseline Churn Rate',\n","                                          line=dict(color='red', width=3)))\n","        fig_projection.add_trace(go.Scatter(x=months, y=[x*100 for x in with_intervention],\n","                                          mode='lines+markers', name='With Retention Program',\n","                                          line=dict(color='green', width=3)))\n","        fig_projection.update_layout(title='Projected Churn Rate Reduction',\n","                                   xaxis_title='Month', yaxis_title='Churn Rate (%)',\n","                                   height=300)\n","        st.plotly_chart(fig_projection, use_container_width=True)\n","\n","        # Revenue impact\n","        st.subheader(\"üíµ Revenue Impact Analysis\")\n","\n","        current_monthly_revenue = total_customers * avg_clv / 12\n","        prevented_churn_customers = high_risk_customers * campaign_success_rate\n","        additional_monthly_revenue = prevented_churn_customers * avg_clv / 12\n","\n","        revenue_metrics = {\n","            \"Current Monthly Revenue\": f\"${current_monthly_revenue:,.0f}\",\n","            \"Customers Saved (Est.)\": f\"{prevented_churn_customers:,.0f}\",\n","            \"Additional Monthly Revenue\": f\"${additional_monthly_revenue:,.0f}\",\n","            \"Annual Revenue Impact\": f\"${additional_monthly_revenue * 12:,.0f}\",\n","            \"3-Year Revenue Impact\": f\"${additional_monthly_revenue * 36:,.0f}\"\n","        }\n","\n","        for metric, value in revenue_metrics.items():\n","            col_a, col_b = st.columns([2, 1])\n","            col_a.write(f\"**{metric}:**\")\n","            col_b.write(value)\n","\n","        # Cost-benefit analysis\n","        st.subheader(\"‚öñÔ∏è Cost-Benefit Analysis\")\n","\n","        cost_benefit_data = {\n","            'Category': ['Campaign Costs', 'Revenue Saved', 'Net Benefit'],\n","            'Year 1': [total_investment, expected_clv_saved, net_benefit],\n","            'Year 2': [total_investment * 0.8, expected_clv_saved * 1.5, expected_clv_saved * 1.5 - total_investment * 0.8],\n","            'Year 3': [total_investment * 0.6, expected_clv_saved * 2.0, expected_clv_saved * 2.0 - total_investment * 0.6]\n","        }\n","\n","        cost_benefit_df = pd.DataFrame(cost_benefit_data)\n","\n","        fig_cb = go.Figure()\n","        fig_cb.add_trace(go.Bar(name='Campaign Costs', x=cost_benefit_df['Category'],\n","                               y=[cost_benefit_df['Year 1'][0], 0, 0], marker_color='red'))\n","        fig_cb.add_trace(go.Bar(name='Revenue Saved', x=cost_benefit_df['Category'],\n","                               y=[0, cost_benefit_df['Year 1'][1], 0], marker_color='green'))\n","        fig_cb.add_trace(go.Bar(name='Net Benefit', x=cost_benefit_df['Category'],\n","                               y=[0, 0, cost_benefit_df['Year 1'][2]], marker_color='blue'))\n","        fig_cb.update_layout(title='3-Year Cost-Benefit Analysis', height=300)\n","        st.plotly_chart(fig_cb, use_container_width=True)\n","\n","elif page == \"üìà Advanced Analytics\":\n","    st.markdown('<h1 class=\"main-header\">üìà Advanced Analytics & Insights</h1>', unsafe_allow_html=True)\n","\n","    # Advanced Analytics Dashboard\n","    tab1, tab2, tab3, tab4 = st.tabs([\"üîç Feature Analysis\", \"üìä Cohort Analysis\", \"üéØ Model Performance\", \"üö® Alert System\"])\n","\n","    with tab1:\n","        st.subheader(\"üîç Feature Importance Analysis\")\n","\n","        # Feature importance data based on your actual results from models.py\n","        feature_importance = {\n","            'Contacts_Count_12_mon': 0.161038,\n","            'Gender': 0.143476,\n","            'Months_Inactive_12_mon': 0.113300,\n","            'Utilization_Group': 0.092224,\n","            'Declining_Activity_Flag': 0.089313,\n","            'Activity_Consistency': 0.077223,\n","            'Total_Relationship_Count': 0.068084,\n","            'Declining_Spend_Flag': 0.066035,\n","            'Declining_Usage_Risk': 0.056116,\n","            'Cross_Product_Engagement': 0.044562,\n","            'Total_Trans_Ct': 0.040123,\n","            'Customer_Age': 0.035677,\n","            'Credit_Limit': 0.028943,\n","            'Total_Trans_Amt': 0.025831\n","        }\n","\n","        col1, col2 = st.columns(2)\n","\n","        with col1:\n","            # Feature importance chart (matching your visualization style)\n","            fig_fi = px.bar(x=list(feature_importance.values()),\n","                           y=list(feature_importance.keys()),\n","                           orientation='h',\n","                           title=\"Top Feature Importance - AdaBoost Model\",\n","                           color=list(feature_importance.values()),\n","                           color_continuous_scale='Viridis')\n","            fig_fi.update_layout(height=500, yaxis={'categoryorder':'total ascending'})\n","            fig_fi.update_traces(texttemplate='%{x:.3f}', textposition='outside')\n","            st.plotly_chart(fig_fi, use_container_width=True)\n","\n","        with col2:\n","            # Feature correlation heatmap (from your analysis)\n","            correlation_features = ['Customer_Age', 'Total_Trans_Ct', 'Total_Trans_Amt',\n","                                  'Avg_Utilization_Ratio', 'Months_Inactive_12_mon',\n","                                  'Total_Relationship_Count', 'Contacts_Count_12_mon']\n","            correlation_data = df[correlation_features + ['Attrition_Flag']].corr()\n","\n","            fig_corr = px.imshow(correlation_data,\n","                               title=\"Feature Correlation Matrix\",\n","                               color_continuous_scale='RdBu_r',\n","                               aspect='auto',\n","                               text_auto='.2f')\n","            fig_corr.update_layout(height=500)\n","            st.plotly_chart(fig_corr, use_container_width=True)\n","\n","        # Feature insights based on your analysis results\n","        st.subheader(\"üí° Key Feature Insights from Analysis\")\n","\n","        insights_col1, insights_col2 = st.columns(2)\n","\n","        with insights_col1:\n","            st.markdown(\"\"\"\n","            **üî¥ Critical Risk Factors (From Your Analysis):**\n","            - **Contacts Count (16.1%)**: Most important predictor - high contact frequency indicates issues\n","            - **Gender (14.3%)**: Demographic factor with significant impact\n","            - **Months Inactive (11.3%)**: Strong activity-based predictor\n","            - **Utilization Group (9.2%)**: Credit usage patterns are crucial\n","            \"\"\")\n","\n","            st.markdown(\"\"\"\n","            **üìä Statistical Insights:**\n","            - Customers with 3+ inactive months: **+25% churn risk**\n","            - Single-product customers: **+12% churn risk**\n","            - High utilization (>80%): **+10% churn risk**\n","            - 4+ contacts in 12 months: **+20% churn risk**\n","            \"\"\")\n","\n","        with insights_col2:\n","            st.markdown(\"\"\"\n","            **üìà Actionable Business Rules:**\n","            - **Monitor customers with 3+ inactive months** - highest priority\n","            - **Track contact frequency** - 4+ contacts = intervention needed\n","            - **Focus on single-product customers** for cross-selling\n","            - **Watch utilization patterns** - both extremes are risky\n","            \"\"\")\n","\n","            st.markdown(\"\"\"\n","            **üéØ Model Performance (Your Results):**\n","            - **Best Model**: AdaBoost with 78.61% AUC\n","            - **Business ROI**: 89.2% campaign ROI\n","            - **Precision**: 47.3% of predicted churners actually churn\n","            - **Recall**: 45.0% of actual churners are identified\n","            \"\"\")\n","\n","    with tab2:\n","        st.subheader(\"üìä Customer Cohort Analysis\")\n","\n","        # Cohort analysis by acquisition month (based on your RFM analysis)\n","        df['Acquisition_Month'] = pd.to_datetime('2024-01-01') - pd.to_timedelta(df['Months_on_book'] * 30, unit='D')\n","        df['Acquisition_Cohort'] = df['Acquisition_Month'].dt.to_period('M')\n","\n","        # Create cohort table using your methodology\n","        cohort_data = df.groupby('Acquisition_Cohort').agg({\n","            'Attrition_Flag': ['count', 'sum', 'mean'],\n","            'CLV': 'mean',\n","            'Total_Trans_Amt': 'mean',\n","            'R_Score': 'mean',\n","            'F_Score': 'mean',\n","            'M_Score': 'mean'\n","        }).round(2)\n","\n","        cohort_data.columns = ['Customers', 'Churned', 'Churn_Rate', 'Avg_CLV',\n","                              'Avg_Transactions', 'Recency_Score', 'Frequency_Score', 'Monetary_Score']\n","\n","        st.subheader(\"üìÖ RFM-Based Cohort Performance\")\n","\n","        # Display cohort table\n","        cohort_display = cohort_data.reset_index()\n","        cohort_display['Acquisition_Cohort'] = cohort_display['Acquisition_Cohort'].astype(str)\n","        st.dataframe(cohort_display, use_container_width=True)\n","\n","        # Cohort visualizations\n","        col1, col2 = st.columns(2)\n","\n","        with col1:\n","            # Churn rate by cohort\n","            fig_cohort_churn = px.line(cohort_display,\n","                                     x='Acquisition_Cohort', y='Churn_Rate',\n","                                     title='Churn Rate Evolution by Cohort',\n","                                     markers=True)\n","            fig_cohort_churn.update_traces(line_color='red', line_width=3, marker_size=8)\n","            fig_cohort_churn.update_layout(height=350, xaxis_tickangle=45)\n","            st.plotly_chart(fig_cohort_churn, use_container_width=True)\n","\n","        with col2:\n","            # CLV by cohort\n","            fig_cohort_clv = px.bar(cohort_display,\n","                                   x='Acquisition_Cohort', y='Avg_CLV',\n","                                   title='Average CLV by Acquisition Cohort',\n","                                   color='Avg_CLV', color_continuous_scale='Greens')\n","            fig_cohort_clv.update_layout(height=350, xaxis_tickangle=45)\n","            st.plotly_chart(fig_cohort_clv, use_container_width=True)\n","\n","        # RFM Score Distribution Analysis (from your customer segmentation)\n","        st.subheader(\"üéØ RFM Score Distribution Analysis\")\n","\n","        col1, col2, col3 = st.columns(3)\n","\n","        with col1:\n","            # Recency Score Distribution\n","            r_score_dist = df['R_Score'].value_counts().sort_index()\n","            fig_r = px.bar(x=r_score_dist.index, y=r_score_dist.values,\n","                          title=\"Recency Score Distribution\",\n","                          color=r_score_dist.values, color_continuous_scale='Reds')\n","            fig_r.update_layout(height=300)\n","            st.plotly_chart(fig_r, use_container_width=True)\n","\n","        with col2:\n","            # Frequency Score Distribution\n","            f_score_dist = df['F_Score'].value_counts().sort_index()\n","            fig_f = px.bar(x=f_score_dist.index, y=f_score_dist.values,\n","                          title=\"Frequency Score Distribution\",\n","                          color=f_score_dist.values, color_continuous_scale='Blues')\n","            fig_f.update_layout(height=300)\n","            st.plotly_chart(fig_f, use_container_width=True)\n","\n","        with col3:\n","            # Monetary Score Distribution\n","            m_score_dist = df['M_Score'].value_counts().sort_index()\n","            fig_m = px.bar(x=m_score_dist.index, y=m_score_dist.values,\n","                          title=\"Monetary Score Distribution\",\n","                          color=m_score_dist.values, color_continuous_scale='Greens')\n","            fig_m.update_layout(height=300)\n","            st.plotly_chart(fig_m, use_container_width=True)\n","\n","        # Customer Segment Analysis (from your create_cc_segments function)\n","        st.subheader(\"üè∑Ô∏è Customer Segment Performance\")\n","\n","        segment_analysis = df.groupby('Customer_Segment').agg({\n","            'Attrition_Flag': ['count', 'sum', 'mean'],\n","            'CLV': 'mean',\n","            'Total_Trans_Amt': 'mean',\n","            'R_Score': 'mean',\n","            'F_Score': 'mean',\n","            'M_Score': 'mean'\n","        }).round(3)\n","\n","        segment_analysis.columns = ['Customer_Count', 'Churned_Count', 'Churn_Rate',\n","                                   'Avg_CLV', 'Avg_Transaction_Amt', 'Avg_R_Score', 'Avg_F_Score', 'Avg_M_Score']\n","\n","        # Display segment analysis\n","        segment_display = segment_analysis.reset_index()\n","        st.dataframe(segment_display, use_container_width=True)\n","\n","        # Segment visualization\n","        fig_segment_performance = px.scatter(segment_display,\n","                                           x='Churn_Rate', y='Avg_CLV',\n","                                           size='Customer_Count',\n","                                           color='Customer_Segment',\n","                                           title='Customer Segment Performance: Churn Rate vs CLV',\n","                                           hover_data=['Customer_Count'])\n","        fig_segment_performance.update_layout(height=400)\n","        st.plotly_chart(fig_segment_performance, use_container_width=True)\n","\n","    with tab3:\n","        st.subheader(\"üéØ Live Model Training & Performance\")\n","\n","        # Model training controls\n","        col1, col2, col3 = st.columns([2, 1, 1])\n","\n","        with col1:\n","            st.write(\"**Select Models to Train:**\")\n","            model_selection = st.multiselect(\n","                \"Choose models\",\n","                ['Logistic Regression', 'Random Forest', 'XGBoost', 'AdaBoost', 'Neural Network'],\n","                default=['Random Forest', 'XGBoost', 'AdaBoost']\n","            )\n","\n","        with col2:\n","            test_size = st.slider(\"Test Size\", 0.1, 0.4, 0.2, 0.05)\n","\n","        with col3:\n","            random_state = st.number_input(\"Random State\", 1, 100, 42)\n","\n","        # Train models button\n","        if st.button(\"üöÄ Train Selected Models\", type=\"primary\"):\n","            if len(model_selection) == 0:\n","                st.warning(\"Please select at least one model to train.\")\n","            else:\n","                # Import required libraries\n","                from sklearn.model_selection import train_test_split\n","                from sklearn.preprocessing import StandardScaler, LabelEncoder\n","                from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n","                from sklearn.linear_model import LogisticRegression\n","                from sklearn.neural_network import MLPClassifier\n","                from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix\n","                import xgboost as xgb\n","                from sklearn.impute import SimpleImputer\n","\n","                # Progress bar\n","                progress_bar = st.progress(0)\n","                status_text = st.empty()\n","\n","                # Prepare data\n","                status_text.text(\"Preparing data...\")\n","                progress_bar.progress(10)\n","\n","                # Select features for modeling (based on your feature_selection.py)\n","                feature_columns = ['Customer_Age', 'Dependent_count', 'Months_on_book',\n","                                 'Total_Relationship_Count', 'Months_Inactive_12_mon',\n","                                 'Contacts_Count_12_mon', 'Credit_Limit', 'Total_Revolving_Bal',\n","                                 'Total_Trans_Amt', 'Total_Trans_Ct', 'Total_Amt_Chng_Q4_Q1',\n","                                 'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio',\n","                                 'Credit_Health_Score', 'Payment_Capacity', 'Transaction_Efficiency',\n","                                 'Tenure_Value_Ratio', 'Activity_Consistency', 'Service_Intensity',\n","                                 'Usage_Volatility', 'Cross_Product_Engagement', 'High_Util_Risk',\n","                                 'Declining_Usage_Risk', 'Single_Product_Risk', 'R_Score', 'F_Score',\n","                                 'M_Score', 'Customer_Segment_Encoded', 'Spending_Trend', 'Activity_Trend',\n","                                 'Declining_Spend_Flag', 'Declining_Activity_Flag']\n","\n","                # Encode categorical variables (from your encoding function)\n","                df_model = df.copy()\n","\n","                # Gender encoding\n","                df_model['Gender_encoded'] = df_model['Gender'].map({'F': 0, 'M': 1})\n","\n","                # Education Level encoding (from your mapping)\n","                education_mapping = {'High School': 0, 'Graduate': 1, 'Uneducated': 2, 'College': 3, 'Post-Graduate': 4, 'Doctorate': 5, 'Unknown': 2}\n","                df_model['Education_encoded'] = df_model['Education_Level'].map(education_mapping).fillna(2)\n","\n","                # Marital Status encoding\n","                marital_mapping = {'Married': 0, 'Single': 1, 'Divorced': 2, 'Unknown': 1}\n","                df_model['Marital_encoded'] = df_model['Marital_Status'].map(marital_mapping).fillna(1)\n","\n","                # Income Category encoding\n","                income_mapping = {'Less than $40K': 0, '$40K - $60K': 1, '$60K - $80K': 2, '$80K - $120K': 3, '$120K +': 4, 'Unknown': 2}\n","                df_model['Income_encoded'] = df_model['Income_Category'].map(income_mapping).fillna(2)\n","\n","                # Card Category encoding\n","                card_mapping = {'Blue': 0, 'Silver': 1, 'Gold': 2, 'Platinum': 3}\n","                df_model['Card_encoded'] = df_model['Card_Category'].map(card_mapping).fillna(0)\n","\n","                # Add encoded features to feature list\n","                feature_columns.extend(['Gender_encoded', 'Education_encoded', 'Marital_encoded',\n","                                      'Income_encoded', 'Card_encoded'])\n","\n","                # Prepare X and y\n","                available_features = [col for col in feature_columns if col in df_model.columns]\n","                X = df_model[available_features]\n","                y = df_model['Attrition_Flag']\n","\n","                # Handle missing values using your imputation strategy\n","                from sklearn.experimental import enable_iterative_imputer\n","                from sklearn.impute import IterativeImputer\n","                from sklearn.linear_model import BayesianRidge\n","\n","                # Configure IterativeImputer (from your feature_selection.py)\n","                imputer = IterativeImputer(\n","                    estimator=BayesianRidge(),\n","                    max_iter=3,\n","                    tol=1e-2,\n","                    initial_strategy='most_frequent',\n","                    n_nearest_features=5,\n","                    random_state=random_state\n","                )\n","\n","                # Fit and transform\n","                X_imputed = pd.DataFrame(\n","                    imputer.fit_transform(X),\n","                    columns=available_features\n","                )\n","\n","                # Round categorical columns to integers (from your preprocessing)\n","                categorical_cols = ['Education_encoded', 'Marital_encoded', 'Income_encoded', 'Card_encoded']\n","                for col in categorical_cols:\n","                    if col in X_imputed.columns:\n","                        X_imputed[col] = X_imputed[col].round().astype(int)\n","\n","                # Split data with stratification (like in your code)\n","                X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=test_size,\n","                                                                  random_state=random_state, stratify=y)\n","\n","                # Apply SMOTEENN for balancing (from your feature_selection.py approach)\n","                try:\n","                    from imblearn.combine import SMOTEENN\n","                    smoteenn = SMOTEENN(random_state=random_state)\n","                    X_train_balanced, y_train_balanced = smoteenn.fit_resample(X_train, y_train)\n","\n","                    st.info(f\"Applied SMOTEENN: {X_train.shape[0]} ‚Üí {X_train_balanced.shape[0]} samples\")\n","                    X_train = X_train_balanced\n","                    y_train = y_train_balanced\n","                except ImportError:\n","                    st.warning(\"SMOTEENN not available. Using original training data.\")\n","\n","                # Scale features (your approach)\n","                scaler = StandardScaler()\n","                X_train_scaled = scaler.fit_transform(X_train)\n","                X_test_scaled = scaler.transform(X_test)\n","\n","                progress_bar.progress(20)\n","\n","                # Initialize models with your exact configurations\n","                models = {}\n","                if 'Logistic Regression' in model_selection:\n","                    models['Logistic Regression'] = LogisticRegression(\n","                        random_state=random_state, max_iter=1000, C=1.0, penalty='l1', solver='liblinear'\n","                    )\n","                if 'Random Forest' in model_selection:\n","                    models['Random Forest'] = RandomForestClassifier(\n","                        n_estimators=200, max_depth=None, min_samples_split=2,\n","                        min_samples_leaf=1, random_state=random_state\n","                    )\n","                if 'XGBoost' in model_selection:\n","                    models['XGBoost'] = xgb.XGBClassifier(\n","                        n_estimators=200, max_depth=5, learning_rate=0.1, subsample=0.8,\n","                        colsample_bytree=0.8, random_state=random_state, eval_metric='logloss'\n","                    )\n","                if 'AdaBoost' in model_selection:\n","                    models['AdaBoost'] = AdaBoostClassifier(\n","                        n_estimators=200, learning_rate=1.0, algorithm='SAMME', random_state=random_state\n","                    )\n","                if 'Neural Network' in model_selection:\n","                    models['Neural Network'] = MLPClassifier(\n","                        hidden_layer_sizes=(100, 50), activation='tanh', alpha=0.001,\n","                        learning_rate='constant', random_state=random_state, max_iter=1000\n","                    )\n","\n","                # Train models and collect results\n","                model_results = {}\n","                model_objects = {}\n","\n","                for i, (name, model) in enumerate(models.items()):\n","                    status_text.text(f\"Training {name}...\")\n","                    progress_bar.progress(30 + (i * 50 // len(models)))\n","\n","                    try:\n","                        # Fit model\n","                        if name in ['Logistic Regression', 'Neural Network']:\n","                            model.fit(X_train_scaled, y_train)\n","                            y_pred = model.predict(X_test_scaled)\n","                            y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n","                        else:\n","                            model.fit(X_train, y_train)\n","                            y_pred = model.predict(X_test)\n","                            y_pred_proba = model.predict_proba(X_test)[:, 1]\n","\n","                        # Calculate metrics\n","                        accuracy = accuracy_score(y_test, y_pred)\n","                        precision = precision_score(y_test, y_pred, zero_division=0)\n","                        recall = recall_score(y_test, y_pred, zero_division=0)\n","                        f1 = f1_score(y_test, y_pred, zero_division=0)\n","                        auc = roc_auc_score(y_test, y_pred_proba)\n","\n","                        model_results[name] = {\n","                            'AUC': auc,\n","                            'Accuracy': accuracy,\n","                            'Precision': precision,\n","                            'Recall': recall,\n","                            'F1': f1,\n","                            'y_pred': y_pred,\n","                            'y_pred_proba': y_pred_proba\n","                        }\n","\n","                        model_objects[name] = model\n","\n","                    except Exception as e:\n","                        st.error(f\"Error training {name}: {str(e)}\")\n","                        continue\n","\n","                progress_bar.progress(100)\n","                status_text.text(\"Training completed!\")\n","\n","                # Display results\n","                if model_results:\n","                    st.success(f\"‚úÖ Successfully trained {len(model_results)} models!\")\n","\n","                    # Create results dataframe\n","                    metrics_df = pd.DataFrame(model_results).T\n","                    metrics_df = metrics_df.round(4)\n","\n","                    # Find best model\n","                    best_model_name = metrics_df['AUC'].idxmax()\n","                    best_auc = metrics_df.loc[best_model_name, 'AUC']\n","\n","                    col1, col2 = st.columns(2)\n","\n","                    with col1:\n","                        st.subheader(\"üìä Model Comparison Results\")\n","\n","                        # Color code the best model\n","                        def highlight_best(s):\n","                            return ['background-color: lightgreen' if s.name == best_model_name else '' for _ in s]\n","\n","                        styled_df = metrics_df.style.apply(highlight_best, axis=1)\n","                        st.dataframe(styled_df, use_container_width=True)\n","\n","                        # Best model highlight\n","                        st.success(f\"\"\"\n","                        **üèÜ Best Performing Model: {best_model_name}**\n","                        - AUC Score: {best_auc:.1%}\n","                        - Accuracy: {metrics_df.loc[best_model_name, 'Accuracy']:.1%}\n","                        - F1 Score: {metrics_df.loc[best_model_name, 'F1']:.1%}\n","                        \"\"\")\n","\n","                    with col2:\n","                        # Model performance visualization\n","                        fig_models = px.bar(metrics_df.reset_index(),\n","                                          x='index', y='AUC',\n","                                          title='Model AUC Comparison',\n","                                          color='AUC',\n","                                          color_continuous_scale='Blues',\n","                                          text='AUC')\n","                        fig_models.update_traces(texttemplate='%{text:.3f}', textposition='outside')\n","                        fig_models.update_layout(height=400, xaxis_title='Model', xaxis_tickangle=45)\n","                        st.plotly_chart(fig_models, use_container_width=True)\n","\n","                    # Detailed analysis for best model\n","                    st.subheader(f\"üìà Detailed Analysis - {best_model_name}\")\n","\n","                    best_model_results = model_results[best_model_name]\n","\n","                    col1, col2 = st.columns(2)\n","\n","                    with col1:\n","                        # Confusion Matrix\n","                        cm = confusion_matrix(y_test, best_model_results['y_pred'])\n","\n","                        fig_cm = px.imshow(cm,\n","                                          text_auto=True,\n","                                          aspect=\"auto\",\n","                                          title=f\"Confusion Matrix - {best_model_name}\",\n","                                          labels=dict(x=\"Predicted\", y=\"Actual\", color=\"Count\"),\n","                                          x=['Not Churned', 'Churned'],\n","                                          y=['Not Churned', 'Churned'],\n","                                          color_continuous_scale='Blues')\n","                        fig_cm.update_layout(height=400)\n","                        st.plotly_chart(fig_cm, use_container_width=True)\n","\n","                    with col2:\n","                        # ROC Curve\n","                        fpr, tpr, _ = roc_curve(y_test, best_model_results['y_pred_proba'])\n","\n","                        fig_roc = go.Figure()\n","                        fig_roc.add_trace(go.Scatter(x=fpr, y=tpr, mode='lines',\n","                                                   name=f'ROC Curve (AUC = {best_auc:.3f})',\n","                                                   line=dict(color='blue', width=3)))\n","                        fig_roc.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines',\n","                                                   name='Random Classifier',\n","                                                   line=dict(color='red', width=2, dash='dash')))\n","                        fig_roc.update_layout(title=f'ROC Curve - {best_model_name}',\n","                                            xaxis_title='False Positive Rate',\n","                                            yaxis_title='True Positive Rate',\n","                                            height=400)\n","                        st.plotly_chart(fig_roc, use_container_width=True)\n","\n","                    # Feature importance (if available)\n","                    if hasattr(model_objects[best_model_name], 'feature_importances_'):\n","                        st.subheader(\"üéØ Feature Importance - \" + best_model_name)\n","\n","                        importances = model_objects[best_model_name].feature_importances_\n","                        feature_importance_df = pd.DataFrame({\n","                            'Feature': feature_columns,\n","                            'Importance': importances\n","                        }).sort_values('Importance', ascending=False).head(10)\n","\n","                        fig_fi = px.bar(feature_importance_df,\n","                                       x='Importance', y='Feature',\n","                                       orientation='h',\n","                                       title=f'Top 10 Features - {best_model_name}',\n","                                       color='Importance',\n","                                       color_continuous_scale='Viridis')\n","                        fig_fi.update_layout(height=400, yaxis={'categoryorder':'total ascending'})\n","                        st.plotly_chart(fig_fi, use_container_width=True)\n","\n","                    # Business impact calculation\n","                    st.subheader(\"üí∞ Business Impact Analysis\")\n","\n","                    # Calculate business metrics\n","                    tn, fp, fn, tp = cm.ravel()\n","\n","                    avg_customer_value = df['CLV'].mean()\n","                    retention_campaign_cost = 250\n","                    campaign_success_rate = 0.35\n","\n","                    revenue_saved = tp * avg_customer_value * campaign_success_rate\n","                    campaign_costs = (tp + fp) * retention_campaign_cost\n","                    revenue_lost = fn * avg_customer_value\n","                    net_benefit = revenue_saved - campaign_costs\n","                    roi = (net_benefit / campaign_costs) * 100 if campaign_costs > 0 else 0\n","\n","                    col1, col2, col3, col4 = st.columns(4)\n","\n","                    with col1:\n","                        st.metric(\"Customers to Target\", f\"{tp + fp:,}\")\n","                    with col2:\n","                        st.metric(\"Revenue Saved\", f\"${revenue_saved:,.0f}\")\n","                    with col3:\n","                        st.metric(\"Campaign Costs\", f\"${campaign_costs:,.0f}\")\n","                    with col4:\n","                        st.metric(\"ROI\", f\"{roi:.1f}%\")\n","\n","                    # Store results in session state for use in other parts of the app\n","                    st.session_state['trained_models'] = model_objects\n","                    st.session_state['model_results'] = model_results\n","                    st.session_state['best_model_name'] = best_model_name\n","                    st.session_state['feature_columns'] = feature_columns\n","                    st.session_state['scaler'] = scaler\n","                    st.session_state['label_encoders'] = {\n","                        'gender': le_gender,\n","                        'education_mapping': education_mapping,\n","                        'marital_mapping': marital_mapping,\n","                        'income_mapping': income_mapping,\n","                        'card_mapping': card_mapping\n","                    }\n","\n","                else:\n","                    st.error(\"No models were successfully trained. Please check your data and try again.\")\n","\n","        else:\n","            st.info(\"üëÜ Select models and click 'Train Selected Models' to see live results!\")\n","\n","            # Show sample of what to expect\n","            st.subheader(\"üìã Expected Output\")\n","            st.write(\"After training, you'll see:\")\n","            st.write(\"‚Ä¢ Real-time model performance metrics\")\n","            st.write(\"‚Ä¢ Interactive confusion matrices and ROC curves\")\n","            st.write(\"‚Ä¢ Feature importance analysis\")\n","            st.write(\"‚Ä¢ Business impact calculations\")\n","            st.write(\"‚Ä¢ Model comparison charts\")\n","\n","    with tab4:\n","        st.subheader(\"üö® Real-Time Alert System\")\n","\n","        # Generate some sample alerts\n","        current_time = datetime.now()\n","\n","        # High-risk customer alerts\n","        high_risk_alerts = df[df['Churn_Probability'] >= 0.8].head(5).copy()\n","        high_risk_alerts['Alert_Time'] = [current_time - timedelta(minutes=np.random.randint(1, 60)) for _ in range(len(high_risk_alerts))]\n","        high_risk_alerts['Customer_ID'] = [f'CU{1000+i}' for i in range(len(high_risk_alerts))]\n","\n","        st.subheader(\"üî¥ Critical Risk Alerts (Last Hour)\")\n","\n","        for idx, row in high_risk_alerts.iterrows():\n","            with st.expander(f\"üö® {row['Customer_ID']} - Risk Score: {row['Churn_Probability']:.1%}\"):\n","                col1, col2, col3 = st.columns(3)\n","\n","                with col1:\n","                    st.write(f\"**Customer Profile:**\")\n","                    st.write(f\"Age: {row['Customer_Age']}\")\n","                    st.write(f\"Card: {row['Card_Category']}\")\n","                    st.write(f\"Tenure: {row['Months_on_book']} months\")\n","\n","                with col2:\n","                    st.write(f\"**Risk Factors:**\")\n","                    st.write(f\"Inactive: {row['Months_Inactive_12_mon']} months\")\n","                    st.write(f\"Transactions: {row['Total_Trans_Ct']}\")\n","                    st.write(f\"Products: {row['Total_Relationship_Count']}\")\n","\n","                with col3:\n","                    st.write(f\"**Recommended Actions:**\")\n","                    st.write(\"‚Ä¢ Immediate contact required\")\n","                    st.write(\"‚Ä¢ Offer retention incentives\")\n","                    st.write(\"‚Ä¢ Assign priority support\")\n","\n","        # System health metrics\n","        st.subheader(\"‚öôÔ∏è System Health Dashboard\")\n","\n","        col1, col2, col3, col4 = st.columns(4)\n","\n","        with col1:\n","            st.metric(\"Model Uptime\", \"99.8%\", delta=\"0.1%\")\n","\n","        with col2:\n","            st.metric(\"Predictions/Hour\", \"1,247\", delta=\"+156\")\n","\n","        with col3:\n","            st.metric(\"Alert Response Time\", \"2.3 min\", delta=\"-0.8 min\")\n","\n","        with col4:\n","            st.metric(\"System Accuracy\", \"78.6%\", delta=\"+1.2%\")\n","\n","        # Recent predictions timeline\n","        st.subheader(\"üìä Prediction Activity (Last 24 Hours)\")\n","\n","        hours = list(range(24))\n","        predictions = [np.random.randint(800, 1500) for _ in hours]\n","        high_risk_count = [int(p * 0.12) for p in predictions]\n","\n","        fig_activity = go.Figure()\n","        fig_activity.add_trace(go.Scatter(x=hours, y=predictions, mode='lines+markers',\n","                                        name='Total Predictions', line=dict(color='blue')))\n","        fig_activity.add_trace(go.Scatter(x=hours, y=high_risk_count, mode='lines+markers',\n","                                        name='High Risk Detected', line=dict(color='red')))\n","        fig_activity.update_layout(title='Hourly Prediction Activity',\n","                                 xaxis_title='Hour', yaxis_title='Count', height=300)\n","        st.plotly_chart(fig_activity, use_container_width=True)\n","\n","# Beautiful Footer\n","st.markdown(\"---\")\n","st.markdown(\"\"\"\n","<div style='text-align: center; color: #666; padding: 1rem;'>\n","    <p>üí≥ Credit Card Churn Analytics Dashboard | Built with Streamlit</p>\n","    <p>üè¶ Empowering data-driven retention strategies for banking excellence</p>\n","</div>\n","\"\"\", unsafe_allow_html=True)"],"metadata":{"id":"ljGAo3y4mcyk"},"execution_count":null,"outputs":[]}]}